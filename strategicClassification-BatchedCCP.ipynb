{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranked Strategic Classification ðŸ¦š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our project we tackle the problem of Stategic classification with user dependant responses. \n",
    "\n",
    "We introduce user dependance through competition on $k$ available spots which are given to the top-k rated users.\n",
    "\n",
    "This problem appears in our daily life in multiple places such as acceptance into universities and job interviews, where the users compete for a few available spots.\n",
    "\n",
    "These users are strategic, and can improve their ranking by changing their features. In the example of job interview, can for example create a specialized CV which they know will help them in getting into the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Notations\n",
    "- Time is assumed to be discrete and denoted by $t\\in\\{0,1,...\\}$\n",
    "- Each user $i$ has initial features $x_i(0)\\in \\mathbb{R}^d$\n",
    "- Each user $i$ updates his features at each time $t$ with a function $x_i(t) = \\Delta_h(x_i(0))$.\n",
    "  \n",
    "<ins>Note:</ins> For simplification, the user always updates his features based on his features at $t=0$.\n",
    "- The set of user features at time $t$ is denoted by $X_t$\n",
    "- Each user has a ground truth rating $r(x_i(0))\\in[0,1]$\n",
    "- We train a model to predict the true ratings $r_{\\theta}(x_i(t))$\n",
    "- Denote the sorted indices of the users based on $r_{\\theta}(x_i(t))$ by $s_i(t) = \\text{argsort}(\\{r_\\theta(x(t))\\}_{x(t)\\in X_t})[i]$\n",
    "\n",
    "Given all of the users and their features, the system predicts the rating for each user, $r_\\theta(x_i(t))$, and accepts the top-$k$ rated users $\\{x_{s_1(t)},...,x_{s_k(t)}\\}$. \n",
    "\n",
    "The users are able to change their features with the following function: $\\Delta_h(x)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model the environment based on the environment from the paper: *S. Levanon, N. Rosenfeld: \"Strategic Classification Made Practical\", 2021*\n",
    "\n",
    "We assume a behavioral model for the user strategic response $\\Delta_h(\\cdot)$. Given the ratings at time $t$ we calculate 2 thresholds:\n",
    "\n",
    "$$\n",
    "T_{low} = r_\\theta(x_{s_k(t)}(t)), \\quad T_{high} = r_\\theta(x_{s_{k'}(t)}(t)) \n",
    "$$\n",
    "\n",
    "Where $k'< k$ is a positive integer. These thresholds are used to model the behavioral responses. \n",
    "\n",
    "We hypothesize that users will not be satisfied by sitting on the low threshold and will want acceptance confidence at the higher rating threshold. Using this we design the following reward function: \n",
    "\n",
    "$$h(r) = q\\cdot\\sigma^*_{\\tau}\\left(\\frac{r-T_{low}}{\\tau_{low}}\\right) + (1-q)\\cdot\\sigma^*_{\\tau}\\left(\\frac{r-T_{high}}{\\tau_{high}}\\right)$$\n",
    "\n",
    "Where $q\\in[0,1]$, and $\\sigma(\\cdot)$ is a sigmoid function that will be discussed in the CCP section. This gives small reward $q$ for acceptance and the rest is given for passing the higher threshold.\n",
    "\n",
    "The $\\tau$ parameters express the softness of threshold; small $\\tau$ will give a step like reward and large $\\tau$ will give a smoother reward.\n",
    "\n",
    "The user responses are modeled as \n",
    "\n",
    "$$\\Delta_h(x) = \\underset{x'}{\\text{argmax }} h(r_\\theta(x')) - c(x,x')$$\n",
    "\n",
    "Where $c(\\cdot,\\cdot)$ is a convex cost function for feature change, we use euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T16:15:04.246310800Z",
     "start_time": "2024-04-16T16:14:55.062868900Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "import numpy as np\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import zero_one_loss, confusion_matrix\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os, psutil\n",
    "from datetime import datetime\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "TRAIN_SLOPE = 1\n",
    "EVAL_SLOPE = 5\n",
    "X_LOWER_BOUND = -10\n",
    "X_UPPER_BOUND = 10\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few useful utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T16:15:10.386224200Z",
     "start_time": "2024-04-16T16:15:10.359804100Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(X, Y, percentage):\n",
    "    num_val = int(len(X)*percentage)\n",
    "    return X[num_val:], Y[num_val:], X[:num_val], Y[:num_val]\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    data = torch.cat((Y, X), 1)\n",
    "    data = data[torch.randperm(data.size()[0])]\n",
    "    X = data[:, 1:]\n",
    "    Y = data[:, 0]\n",
    "    return X, Y\n",
    "\n",
    "def conf_mat(Y1, Y2):\n",
    "    num_of_samples = len(Y1)\n",
    "    mat = confusion_matrix(Y1, Y2, labels=[-1, 1])*100/num_of_samples\n",
    "    acc = np.trace(mat)\n",
    "    return mat, acc\n",
    "\n",
    "def calc_accuracy(Y, Ypred):\n",
    "    num = len(Y)\n",
    "    temp = Y - Ypred\n",
    "    acc = len(temp[temp == 0])*1./num\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain & Cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we implement the score functions and based on it, a differientiable sigmoid-like proxy for the sign operation in the $h$ function:\n",
    "\n",
    "$$\\sigma_\\tau^*(z) = 0.5 \\cdot \\sqrt{(\\tau^{-1}(z-T_{low})+1)^2+1} - 0.5 \\cdot \\sqrt{(\\tau^{-1}(z-T_{high})-1)^2+1}$$\n",
    "\n",
    "We denote $0.5 \\cdot \\sqrt{(\\tau^{-1}(z-T_{low})+1)^2+1}$ with $f(x)$ and $0.5 \\cdot \\sqrt{(\\tau^{-1}(z-T_{high})-1)^2+1}$ with $g(x)$.\n",
    "\n",
    "This function can be written as a sum of convex and concave funcitons, a property which is essential to the CCP method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T16:15:15.665231800Z",
     "start_time": "2024-04-16T16:15:15.634356500Z"
    }
   },
   "outputs": [],
   "source": [
    "def score(x, w, b):\n",
    "    return x@w + b\n",
    "\n",
    "def f(x, w, b, slope, T):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*(score(x, w, b)-T) + 1)]), 2)\n",
    "\n",
    "def g(x, w, b, slope, T):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*(score(x, w, b)-T) - 1)]), 2)\n",
    "\n",
    "def c(x, r, x_dim, scale):\n",
    "    return (scale)*cp.sum_squares(x-r)\n",
    "\n",
    "def f_derivative(x, w, b, slope, T):\n",
    "    return cp.multiply(slope*((slope*(score(x, w, b)-T) + 1)/cp.sqrt((slope*(score(x, w, b)-T) + 1)**2 + 1)), w)\n",
    "\n",
    "def f_batch(x, w, b, slope, T):\n",
    "    return cp.norm(cp.vstack([np.ones(x.shape[0]), slope*(score(x, w, b) - T) + 1]), 2, axis=0)\n",
    "\n",
    "def g_batch(x, w, b, slope, T):\n",
    "    return cp.norm(cp.vstack([np.ones((1, x.shape[0])), cp.reshape((slope*(score(x, w, b)-T) - 1), (1, x.shape[0]))]), 2, axis=0)\n",
    "\n",
    "def c_batch(x, r, x_dim, scale):\n",
    "    return (scale)*cp.square(cp.norm(x-r, 2, axis=1))\n",
    "\n",
    "def f_derivative_batch(x, w, b, slope, T):\n",
    "    nablas = 0.5*slope*((slope*(score(x, w, b) - T) + 1)/cp.sqrt((slope*(score(x, w, b) - T) + 1)**2 + 1))\n",
    "    return cp.reshape(nablas, (nablas.shape[0], 1))@cp.reshape(w, (1, x.shape[1]))\n",
    "\n",
    "# ------------------------------------- Our added functions -------------------------------------\n",
    "# summing 2 sigmoids to model initial acceptance and higher acceptance in order to model user behavior of wanting to have some margin from min requirements\n",
    "\n",
    "def f_tot(x, w, b, slope_low, slope_high, Thresh_low, Thresh_high, acp_reward):\n",
    "    return acp_reward * f(x, w, b, slope_low, Thresh_low)+ (1-acp_reward) * f(x, w, b, slope_high, Thresh_high)\n",
    "\n",
    "def g_tot(x, w, b, slope_low, slope_high, Thresh_low, Thresh_high, acp_reward):\n",
    "    return acp_reward * g(x, w, b, slope_low, Thresh_low)+ (1-acp_reward) * g(x, w, b, slope_high, Thresh_high)\n",
    "\n",
    "def f_tot_derivative(x, w, b, slope_low, slope_high, Thresh_low, Thresh_high, acp_reward):\n",
    "    return acp_reward * f_derivative(x, w, b, slope_low, Thresh_low)+ (1-acp_reward) * f_derivative(x, w, b, slope_high, Thresh_high)\n",
    "\n",
    "def f_tot_batch(x, w, b, slope_low, slope_high, Thresh_low, Thresh_high, acp_reward):\n",
    "    return acp_reward * f_batch(x, w, b, slope_low, Thresh_low)+ (1-acp_reward) * f_batch(x, w, b, slope_high, Thresh_high)\n",
    "\n",
    "def g_tot_batch(x, w, b, slope_low, slope_high, Thresh_low, Thresh_high, acp_reward):\n",
    "    return acp_reward * g_batch(x, w, b, slope_low, Thresh_low)+ (1-acp_reward) * g_batch(x, w, b, slope_high, Thresh_high)\n",
    "\n",
    "def f_tot_derivative_batch(x, w, b, slope_low, slope_high, Thresh_low, Thresh_high, acp_reward):\n",
    "    return acp_reward * f_derivative_batch(x, w, b, slope_low, Thresh_low)+ (1-acp_reward) * f_derivative_batch(x, w, b, slope_high, Thresh_high)\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCP classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a CCP class based on the one implemented in the paper referenced above.\n",
    "\n",
    "Convex-Concave Procedure (in short CCP) is an iterative method for solving optimization problems that are expressed as a difference of convex functions, like the one we have in our problem.\n",
    "\n",
    "It does so by iterating through a sequence of concave-relaxed problems (using Taylor series to linearize on of the functions), a process that guarantees convergence to local maxima.\n",
    "\n",
    "We use $h(r)$ which is **a linear combination** of the sigmoid-like proxy described above to model the users behavior.\n",
    "\n",
    "As reminded above, this sigmoid function can be written as a difference of convex functions and is therefore perfect for CCP. \n",
    "\n",
    "Because $h(r)$ is a linear combination of sigmoids, it can also be discribed as a difference of convex functions and we can use CCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCP_rank:\n",
    "    def __init__(self, x_dim, batch_size, funcs, scale):\n",
    "        self.f_derivative = funcs[\"f_derivative\"]\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.x = cp.Variable((batch_size, x_dim))\n",
    "        self.xt = cp.Parameter((batch_size, x_dim))\n",
    "        self.r = cp.Parameter((batch_size, x_dim)) # old reference point (initial x)\n",
    "        self.w = cp.Parameter(x_dim)\n",
    "        self.b = cp.Parameter(1) # bias parameter\n",
    "        # -------------------- adding our function's params --------------------\n",
    "        self.slope_low = cp.Parameter(1) # scale of the low threshold sigmoid\n",
    "        self.slope_high = cp.Parameter(1) # scale of the high threshold sigmoid\n",
    "        self.T_low = cp.Parameter(1) # low threshold of the sigmoid\n",
    "        self.T_high = cp.Parameter(1) # high threshold of the sigmoid\n",
    "        self.acp_reward = cp.Parameter(1) # defines the reward for passing initial threshhold - q in the equations \n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # -------------------- defining our target function --------------------\n",
    "        target = cp.diag(self.x@(self.f_derivative(self.xt, self.w, self.b, \n",
    "                        self.slope_low, self.slope_high, self.T_low, self.T_high, self.acp_reward).T)) - self.g(self.x, self.w,\n",
    "                        self.b, self.slope_low, self.slope_high, self.T_low, self.T_high, self.acp_reward)-self.c(self.x, self.r,\n",
    "                        x_dim, scale)\n",
    "        # ----------------------------------------------------------------------\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        # define the problem's objective\n",
    "        self.prob = cp.Problem(cp.Maximize(cp.sum(target)), constraints)\n",
    "\n",
    "    def ccp(self, r):\n",
    "        \"\"\"\n",
    "        numpy to numpy\n",
    "        \"\"\"\n",
    "        self.xt.value = r\n",
    "        self.r.value = r\n",
    "        result = self.prob.solve()\n",
    "        diff = np.linalg.norm(self.xt.value - self.x.value)\n",
    "        cnt = 0\n",
    "        while diff > 0.001 and cnt < 100:\n",
    "            cnt += 1\n",
    "            self.xt.value = self.x.value\n",
    "            result = self.prob.solve()\n",
    "            diff = np.linalg.norm(self.x.value - self.xt.value)/self.batch_size\n",
    "        return self.x.value\n",
    "\n",
    "    def optimize_X(self, X, w, b, slope_low, slope_high, T_low, T_high, acp_reward):\n",
    "        \"\"\"\n",
    "        tensor to tensor\n",
    "        \"\"\"\n",
    "        w = w.detach().numpy()\n",
    "        b = b.detach().numpy()\n",
    "        \n",
    "        # converting our params into numpy arrays with 1 value\n",
    "        slope_low = np.full(1, slope_low)\n",
    "        slope_high = np.full(1, slope_high)\n",
    "        T_low = np.full(1, T_low)\n",
    "        T_high = np.full(1, T_high)\n",
    "        acp_reward = np.full(1, acp_reward)\n",
    "\n",
    "        X = X.numpy()\n",
    "\n",
    "        self.w.value = w\n",
    "        self.b.value = b\n",
    "        self.slope_low.value = slope_low\n",
    "        self.slope_high.value = slope_high\n",
    "        self.T_low.value = T_low\n",
    "        self.T_high.value = T_high\n",
    "        self.acp_reward.value = acp_reward\n",
    "\n",
    "        return torch.from_numpy(self.ccp(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta \n",
    "This module uses the CCP procedure to compute the user responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DELTA():\n",
    "\n",
    "    def __init__(self, x_dim, funcs, scale, slope_low, slope_high, T_low, T_high, acp_reward):\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "\n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.r = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.w = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.b = cp.Parameter(1, value = np.random.randn(1))\n",
    "        self.f_der = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        # -------------------- adding our function's params --------------------\n",
    "        self.slope_low = slope_high # scale of the low threshold sigmoid\n",
    "        self.slope_high = slope_high # scale of the high threshold sigmoid\n",
    "        self.T_low = T_low # low threshold of the sigmoid\n",
    "        self.T_high = T_high # high threshold of the sigmoid\n",
    "        self.acp_reward = acp_reward # the relation between the 2 slopes - defines the reward for passing initial threshhold\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # -------------------- defining our target function --------------------\n",
    "        target = self.x@self.f_der-self.g(self.x, self.w, self.b, \n",
    "                        self.slope_low, self.slope_high, self.T_low, \n",
    "                        self.T_high, self.acp_reward)-self.c(self.x, self.r, x_dim, scale)\n",
    "        # ----------------------------------------------------------------------\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        objective = cp.Maximize(target)\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        self.layer = CvxpyLayer(problem, parameters=[self.r, self.w, self.b, self.f_der],\n",
    "                                variables=[self.x])\n",
    "\n",
    "    def optimize_X(self, X, w, b, F_DER):\n",
    "        return self.layer(X, w, b, F_DER)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T16:23:14.815264900Z",
     "start_time": "2024-04-16T16:23:14.767441700Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyStrategicModel(torch.nn.Module):\n",
    "    def __init__(self, x_dim, batch_size, funcs, funcs_batch, train_slope, eval_slope, scale, strategic=False):\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        super(MyStrategicModel, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.train_slope, self.eval_slope = train_slope, eval_slope\n",
    "        self.w = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(x_dim, dtype=torch.float64, requires_grad=True)))\n",
    "        self.b = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(1, dtype=torch.float64, requires_grad=True)))\n",
    "        self.strategic = strategic\n",
    "        self.ccp = CCP_rank(x_dim, batch_size, funcs_batch, scale)\n",
    "        self.delta = DELTA(x_dim, funcs, scale)\n",
    "        self.ccp_time = 0\n",
    "        self.total_time = 0\n",
    "\n",
    "    def forward(self, X, evaluation=False):\n",
    "        if self.strategic:\n",
    "            if evaluation:\n",
    "                t1 = time.time()\n",
    "                XT = self.ccp.optimize_X(X, self.w, self.b, self.eval_slope)\n",
    "                self.ccp_time += time.time()-t1\n",
    "                X_opt = XT\n",
    "            else:\n",
    "                t1 = time.time()\n",
    "                XT = self.ccp.optimize_X(X, self.w, self.b, self.train_slope)\n",
    "                self.ccp_time += time.time()-t1\n",
    "                F_DER = self.get_f_ders(XT, self.train_slope)\n",
    "                X_opt = self.delta.optimize_X(X, self.w, self.b, F_DER) # Xopt should be equal to XT but we do it again for the gradients\n",
    "            output = self.score(X_opt)\n",
    "        else:\n",
    "            output = self.score(X)\n",
    "        return output\n",
    "\n",
    "    def optimize_X(self, X, evaluation=False):\n",
    "        slope = self.eval_slope if evaluation else self.train_slope\n",
    "        return self.ccp.optimize_X(X, self.w, self.b, slope)\n",
    "\n",
    "    def normalize_weights(self):\n",
    "        with torch.no_grad():\n",
    "            norm = torch.sqrt(torch.sum(self.w**2) + self.b**2)\n",
    "            self.w /= norm\n",
    "            self.b /= norm\n",
    "\n",
    "    def score(self, x):\n",
    "        return x@self.w + self.b\n",
    "\n",
    "    def get_f_ders(self, XT, slope):\n",
    "        nablas = 0.5*slope*((slope*self.score(XT) + 1)/torch.sqrt((slope*self.score(XT) + 1)**2 + 1))\n",
    "        return torch.reshape(nablas, (len(nablas), 1))@torch.reshape(self.w, (1, len(self.w)))\n",
    "\n",
    "    def calc_accuracy(self, Y, Y_pred):\n",
    "        Y_pred = torch.sign(Y_pred)\n",
    "        num = len(Y)\n",
    "        temp = Y - Y_pred\n",
    "        acc = len(temp[temp == 0])*1./num\n",
    "        return acc\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        return self.calc_accuracy(Y, self.forward(X, evaluation=True))\n",
    "\n",
    "    def loss(self, Y, Y_pred):\n",
    "        return torch.mean(torch.clamp(1 - Y_pred * Y, min=0))\n",
    "\n",
    "    def save_model(self, train_errors, val_errors, train_losses, val_losses, info, path, comment=None):\n",
    "        if comment is not None:\n",
    "            path += \"/\" + comment\n",
    "\n",
    "        filename = path + \"/model.pt\"\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        torch.save(self.state_dict(), filename)\n",
    "\n",
    "        pd.DataFrame(np.array(train_errors)).to_csv(path + '/train_errors.csv')\n",
    "        pd.DataFrame(np.array(val_errors)).to_csv(path + '/val_errors.csv')\n",
    "        pd.DataFrame(np.array(train_losses)).to_csv(path + '/train_losses.csv')\n",
    "        pd.DataFrame(np.array(val_losses)).to_csv(path + '/val_losses.csv')\n",
    "\n",
    "        with open(path + \"/info.txt\", \"w\") as f:\n",
    "            f.write(info)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        self.load_state_dict(torch.load(filename))\n",
    "        self.eval()\n",
    "\n",
    "    def fit(self, path, X, Y, Xval, Yval, opt, opt_kwargs={\"lr\":1e-3}, batch_size=128, epochs=100, verbose=False, callback=None, comment=None):\n",
    "        train_dset = TensorDataset(X, Y)\n",
    "        train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "        test_dset = TensorDataset(Xval, Yval)\n",
    "        test_loader = DataLoader(test_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        opt = opt(self.parameters(), **opt_kwargs)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "\n",
    "        best_val_error = 1\n",
    "        consecutive_no_improvement = 0\n",
    "\n",
    "        total_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            t1 = time.time()\n",
    "            batch = 1\n",
    "            train_losses.append([])\n",
    "            train_errors.append([])\n",
    "            for Xbatch, Ybatch in train_loader:\n",
    "#                 try:\n",
    "                opt.zero_grad()\n",
    "                Ybatch_pred = self.forward(Xbatch)\n",
    "                l = self.loss(Ybatch, Ybatch_pred)\n",
    "                l.backward()\n",
    "                opt.step()\n",
    "                train_losses[-1].append(l.item())\n",
    "                with torch.no_grad():\n",
    "                    e = self.calc_accuracy(Ybatch, Ybatch_pred)\n",
    "                    train_errors[-1].append(1-e)\n",
    "                if verbose:\n",
    "                    print(\"batch %03d / %03d | loss: %3.5f | err: %3.5f\" %\n",
    "                          (batch, len(train_loader), np.mean(train_losses[-1]), np.mean(train_errors[-1])))\n",
    "                batch += 1\n",
    "                if callback is not None:\n",
    "                    callback()\n",
    "#                 except:\n",
    "#                     print(\"failed\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                total_loss = 0\n",
    "                total_error = 0\n",
    "                batch = 0\n",
    "                for Xbatch, Ybatch in test_loader:\n",
    "#                     try:\n",
    "                    Yval_pred = self.forward(Xbatch, evaluation=True)\n",
    "                    val_loss = self.loss(Ybatch, Yval_pred).item()\n",
    "                    total_loss += val_loss\n",
    "                    val_error = 1-self.calc_accuracy(Ybatch, Yval_pred)\n",
    "                    total_error += val_error\n",
    "                    batch += 1\n",
    "#                     except:\n",
    "#                         print(\"failed\")\n",
    "\n",
    "                avg_loss = total_loss/batch\n",
    "                avg_error = total_error/batch\n",
    "                val_losses.append(avg_loss)\n",
    "                val_errors.append(avg_error)\n",
    "                if avg_error < best_val_error:\n",
    "                        consecutive_no_improvement = 0\n",
    "                        best_val_error = avg_error\n",
    "                        info = \"training time in seconds: {}\\nepoch: {}\\nbatch size: {}\\ntrain slope: {}\\neval slope: {}\\nlearning rate: {}\\nvalidation loss: {}\\nvalidation error: {}\\n\".format(\n",
    "                        time.time()-total_time, epoch, batch_size, self.train_slope, self.eval_slope, opt_kwargs[\"lr\"], avg_loss, avg_error)\n",
    "                        self.save_model(train_errors, val_errors, train_losses, val_losses, info, path, comment)\n",
    "                        print(\"model saved!\")\n",
    "\n",
    "                else:\n",
    "                    consecutive_no_improvement += 1\n",
    "                    if consecutive_no_improvement >= 4:\n",
    "                        break\n",
    "\n",
    "            t2 = time.time()\n",
    "            if verbose:\n",
    "                print(\"------------- epoch %03d / %03d | time: %03d sec | loss: %3.5f | err: %3.5f\" % (epoch + 1, epochs, t2-t1, val_losses[-1], val_errors[-1]))\n",
    "\n",
    "        self.total_time = time.time()-total_time\n",
    "        print(\"training time: {} seconds\".format(self.total_time))\n",
    "        return train_errors, val_errors, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "- user features generated randomly\n",
    "- ground truth rating model is $r(x)=u^Tx$ \n",
    "- observed noisy ratings $\\tilde{r}(x)=r(x)+\\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2_\\epsilon)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T16:32:04.618467Z",
     "start_time": "2024-04-16T16:32:04.588158700Z"
    }
   },
   "outputs": [],
   "source": [
    "class RankingEnv():\n",
    "    \"\"\"\n",
    "    Strategic Classification Environment\n",
    "    \"\"\"\n",
    "    def __init__(self, noisyscale, bias, n_features, k, k_tag):\n",
    "        self.noisyscale = noisyscale\n",
    "        self.bias = bias\n",
    "        self.n_features = n_features\n",
    "        self.k = k  # k is number of accepted users\n",
    "        self.k_tag = k_tag  # k' < k sets the threshold of confidant acceptance\n",
    "        self.u_attributes = np.random.normal(loc=0, scale=0.1, size=n_features)\n",
    "\n",
    "    def generate_users(self, n_users):\n",
    "        return np.random.normal(\n",
    "            loc=0,\n",
    "            scale=0.1,\n",
    "            size = (n_users, \n",
    "                    self.n_features,\n",
    "                    ),\n",
    "        )\n",
    "    \n",
    "    def rating(self, users):\n",
    "        # return noisy users rating, when the noise is normally distributed\n",
    "        clean_ratings = self.u_attributes@users\n",
    "        noise = np.random.normal(loc=0, scale=self.noisyscale, size = (len(clean_ratings)))\n",
    "        return clean_ratings + noise\n",
    "    \n",
    "    def get_thresh(self, users):\n",
    "        ranking = self.rating(users)\n",
    "        # reversing the sort into a descending order\n",
    "        sorted_ranking = np.sort(ranking)[::-1]\n",
    "        # calculate thresholds from k and k_tag\n",
    "        return sorted_ranking[self.k-1], sorted_ranking[self.k_tag-1]\n",
    "    \n",
    "    def get_accepted(self, users):\n",
    "        ranking = self.rating(users)\n",
    "        acceptance = np.zeros_like(ranking)\n",
    "        # sort ranking in descending order\n",
    "        sorted_ranking = np.argsort(ranking)[::-1]\n",
    "        acceptance[sorted_ranking[:self.k-1]] = 1\n",
    "        return acceptance\n",
    "    \n",
    "    # TODO: features_update - Update features based on Delta\n",
    "    # Save initial state of users in the environment for update,\n",
    "    # and current state for threshold calculation\n",
    "    # consider including this in the env params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sklearn_data(x_dim, N, informative_frac=1, shift_range=1, scale_range=1, noise_frac=0.01):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    # get ground truth rating model \n",
    "    u_gt = np.random.normal(loc=0, scale=1.0, size=x_dim)\n",
    "    u_gt = u_gt / np.norm(u_gt) # normalize to get ratings [-1,1]\n",
    "    n_informative = int(informative_frac*x_dim)\n",
    "    n_redundant = x_dim - n_informative\n",
    "    shift_arr = shift_range*np.random.randn(x_dim)\n",
    "    scale_arr = scale_range*np.random.randn(x_dim)\n",
    "    X, Y = make_classification(n_samples=N, n_features=x_dim, n_informative=n_informative, n_redundant=n_redundant,\n",
    "                               flip_y=noise_frac, shift=shift_arr, scale=scale_arr, random_state=0)\n",
    "    Y[Y == 0] = -1\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 5])\n",
      "percent of positive samples: 50.78125%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ori Anvar\\anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 7 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\Ori Anvar\\anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 8 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\Ori Anvar\\anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 9 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\Ori Anvar\\anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 10 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\Ori Anvar\\anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 11 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\Ori Anvar\\anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 12 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\Ori Anvar\\anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 13 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\Ori Anvar\\anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 14 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\Ori Anvar\\anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 15 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\Ori Anvar\\anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 16 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required positional arguments: 'slope_low', 'slope_high', 'T_low', 'T_high', and 'acp_reward'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-30-a66ad6de1c0e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[0mccp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mbatch_size\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m9\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m     \u001B[0mstrategic_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMyStrategicModel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_dim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfuncs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfuncs_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mTRAIN_SLOPE\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mEVAL_SLOPE\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscale\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mscale\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstrategic\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     17\u001B[0m     strategic_model.fit(path, X, Y, Xval, Yval,\n\u001B[0;32m     18\u001B[0m                         \u001B[0mopt\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mopt_kwargs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m\"lr\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m1e-1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-28-4b85313febb1>\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, x_dim, batch_size, funcs, funcs_batch, train_slope, eval_slope, scale, strategic)\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrategic\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstrategic\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mccp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCCP_rank\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_dim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfuncs_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscale\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdelta\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDELTA\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_dim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfuncs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscale\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mccp_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtotal_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: __init__() missing 5 required positional arguments: 'slope_low', 'slope_high', 'T_low', 'T_high', and 'acp_reward'"
     ]
    }
   ],
   "source": [
    "path = \"./models/runtime_varying_batch_size\"\n",
    "epochs = 5\n",
    "x_dim = 5\n",
    "scale = 1\n",
    "X, Y = gen_sklearn_data(x_dim, 1024)\n",
    "X, Y, Xval, Yval = split_data(X, Y, 0.25)\n",
    "print(Xval.size())\n",
    "print(\"percent of positive samples: {}%\".format(100 * len(Y[Y == 1]) / len(Y)))\n",
    "\n",
    "funcs = {\"f\": f_tot, \"g\": g_tot, \"f_derivative\": f_tot_derivative, \"c\": c, \"score\": score}\n",
    "funcs_batch = {\"f\": f_tot_batch, \"g\": g_tot_batch, \"f_derivative\": f_tot_derivative_batch, \"c\": c_batch, \"score\": score}\n",
    "\n",
    "total = []\n",
    "ccp = []\n",
    "for batch_size in (2**np.arange(9)).tolist():\n",
    "    strategic_model = MyStrategicModel(x_dim, batch_size, funcs, funcs_batch, TRAIN_SLOPE, EVAL_SLOPE, scale=scale, strategic=True)\n",
    "    strategic_model.fit(path, X, Y, Xval, Yval,\n",
    "                        opt=torch.optim.Adam, opt_kwargs={\"lr\": (1e-1)},\n",
    "                        batch_size=batch_size, epochs=epochs, verbose=True,\n",
    "                       comment=\"batched\")\n",
    "    \n",
    "    total_time = strategic_model.total_time\n",
    "    ccp_time = strategic_model.ccp_time\n",
    "    total.append(total_time)\n",
    "    ccp.append(ccp_time)\n",
    "    pd.DataFrame(np.array(total)).to_csv(path + '/total_timing_results.csv')\n",
    "    pd.DataFrame(np.array(ccp)).to_csv(path + '/ccp_timing_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
