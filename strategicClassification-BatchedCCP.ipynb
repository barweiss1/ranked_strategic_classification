{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranked Strategic Classification ðŸ¦š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our project we tackle the problem of Stategic classification with user dependant responses. \n",
    "\n",
    "We introduce user dependance through competition on $k$ available spots which are given to the top-k rated users.\n",
    "\n",
    "This problem appears in our daily life in multiple places such as acceptance into universities and job interviews, where the users compete for a few available spots.\n",
    "\n",
    "These users are strategic, and can improve their ranking by changing their features. In the example of job interview, can for example create a specialized CV which they know will help them in getting into the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Notations\n",
    "- Time is assumed to be discrete and denoted by $t\\in\\{0,1,...\\}$\n",
    "- Each user $i$ has initial features $x_i(0)\\in \\mathbb{R}^d$\n",
    "- Each user $i$ updates his features at each time $t$ with a function $x_i(t) = \\Delta_h(x_i(0))$.\n",
    "  \n",
    "<ins>Note:</ins> For simplification, the user always updates his features based on his features at $t=0$.\n",
    "- The set of user features at time $t$ is denoted by $X_t$\n",
    "- Each user has a ground truth rating $r(x_i(0))\\in[0,1]$\n",
    "- We train a model to predict the true ratings $r_{\\theta}(x_i(t))$\n",
    "- Denote the sorted indices of the users based on $r_{\\theta}(x_i(t))$ by $s_i(t) = \\text{argsort}(\\{r_\\theta(x(t))\\}_{x(t)\\in X_t})[i]$\n",
    "\n",
    "Given all of the users and their features, the system predicts the rating for each user, $r_\\theta(x_i(t))$, and accepts the top-$k$ rated users $\\{x_{s_1(t)},...,x_{s_k(t)}\\}$. \n",
    "\n",
    "The users are able to change their features with the following function: $\\Delta_h(x)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model the environment based on the environment from the paper: *S. Levanon, N. Rosenfeld: \"Strategic Classification Made Practical\", 2021*\n",
    "\n",
    "We assume a behavioral model for the user strategic response $\\Delta_h(\\cdot)$. Given the ratings at time $t$ we calculate 2 thresholds:\n",
    "\n",
    "$$\n",
    "T_{low} = r_\\theta(x_{s_k(t)}(t)), \\quad T_{high} = r_\\theta(x_{s_{k'}(t)}(t)) \n",
    "$$\n",
    "\n",
    "Where $k'< k$ is a positive integer. These thresholds are used to model the behavioral responses. \n",
    "\n",
    "We hypothesize that users will not be satisfied by sitting on the low threshold and will want acceptance confidence at the higher rating threshold. Using this we design the following reward function: \n",
    "\n",
    "$$h(r) = q\\cdot\\sigma^*_{\\tau}\\left(\\frac{r-T_{low}}{\\tau_{low}}\\right) + (1-q)\\cdot\\sigma^*_{\\tau}\\left(\\frac{r-T_{high}}{\\tau_{high}}\\right)$$\n",
    "\n",
    "Where $q\\in[0,1]$, and $\\sigma(\\cdot)$ is a sigmoid function that will be discussed in the CCP section. This gives small reward $q$ for acceptance and the rest is given for passing the higher threshold.\n",
    "\n",
    "The $\\tau$ parameters express the softness of threshold; small $\\tau$ will give a step like reward and large $\\tau$ will give a smoother reward.\n",
    "\n",
    "The user responses are modeled as \n",
    "\n",
    "$$\\Delta_h(x) = \\underset{x'}{\\text{argmax }} h(r_\\theta(x')) - c(x,x')$$\n",
    "\n",
    "Where $c(\\cdot,\\cdot)$ is a convex cost function for feature change, we use euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T14:20:15.122793200Z",
     "start_time": "2024-04-21T14:19:59.549428700Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "import numpy as np\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import zero_one_loss, confusion_matrix\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os, psutil\n",
    "from datetime import datetime\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "TRAIN_SLOPE = 1\n",
    "EVAL_SLOPE = 5\n",
    "X_LOWER_BOUND = -10\n",
    "X_UPPER_BOUND = 10\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few useful utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T14:20:19.258661300Z",
     "start_time": "2024-04-21T14:20:19.239118300Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(X, Y, percentage):\n",
    "    num_val = int(len(X)*percentage)\n",
    "    return X[num_val:], Y[num_val:], X[:num_val], Y[:num_val]\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    data = torch.cat((Y, X), 1)\n",
    "    data = data[torch.randperm(data.size()[0])]\n",
    "    X = data[:, 1:]\n",
    "    Y = data[:, 0]\n",
    "    return X, Y\n",
    "\n",
    "def conf_mat(Y1, Y2):\n",
    "    num_of_samples = len(Y1)\n",
    "    mat = confusion_matrix(Y1, Y2, labels=[-1, 1])*100/num_of_samples\n",
    "    acc = np.trace(mat)\n",
    "    return mat, acc\n",
    "\n",
    "def calc_accuracy(Y, Ypred):\n",
    "    num = len(Y)\n",
    "    temp = Y - Ypred\n",
    "    acc = len(temp[temp == 0])*1./num\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain & Cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section we implement the score functions and based on it, a differientiable sigmoid-like proxy for the sign operation in the $h$ function:\n",
    "\n",
    "$$\\sigma_\\tau^*(z) = 0.5 \\cdot \\sqrt{(\\tau^{-1}(z-T_{low})+1)^2+1} - 0.5 \\cdot \\sqrt{(\\tau^{-1}(z-T_{high})-1)^2+1}$$\n",
    "\n",
    "We denote $0.5 \\cdot \\sqrt{(\\tau^{-1}(z-T_{low})+1)^2+1}$ with $f(x)$ and $0.5 \\cdot \\sqrt{(\\tau^{-1}(z-T_{high})-1)^2+1}$ with $g(x)$.\n",
    "\n",
    "This function can be written as a sum of convex and concave funcitons, a property which is essential to the CCP method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T14:33:28.105151800Z",
     "start_time": "2024-04-21T14:33:28.084982600Z"
    }
   },
   "outputs": [],
   "source": [
    "def score(x, w, b):\n",
    "    return x@w + b\n",
    "\n",
    "def f(x, w, b, slope, T):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*(score(x, w, b)-T) + 1)]), 2)\n",
    "\n",
    "def g(x, w, b, slope, T):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*(score(x, w, b)-T) - 1)]), 2)\n",
    "\n",
    "def c(x, r, x_dim, scale):\n",
    "    return (scale)*cp.sum_squares(x-r)\n",
    "\n",
    "def f_derivative(x, w, b, slope, T):\n",
    "    return cp.multiply(slope*((slope*(score(x, w, b)-T) + 1)/cp.sqrt((slope*(score(x, w, b)-T) + 1)**2 + 1)), w)\n",
    "\n",
    "def f_batch(x, w, b, slope, T):\n",
    "    return cp.norm(cp.vstack([np.ones(x.shape[0]), slope*(score(x, w, b) - T) + 1]), 2, axis=0)\n",
    "\n",
    "def g_batch(x, w, b, slope, T):\n",
    "    return cp.norm(cp.vstack([np.ones((1, x.shape[0])), cp.reshape((slope*(score(x, w, b)-T) - 1), (1, x.shape[0]))]), 2, axis=0)\n",
    "\n",
    "def c_batch(x, r, x_dim, scale):\n",
    "    return (scale)*cp.square(cp.norm(x-r, 2, axis=1))\n",
    "\n",
    "def f_derivative_batch(x, w, b, slope, T):\n",
    "    nablas = 0.5*slope*((slope*(score(x, w, b) - T) + 1)/cp.sqrt((slope*(score(x, w, b) - T) + 1)**2 + 1))\n",
    "    return cp.reshape(nablas, (nablas.shape[0], 1))@cp.reshape(w, (1, x.shape[1]))\n",
    "\n",
    "# ------------------------------------- Our added functions -------------------------------------\n",
    "# summing 2 sigmoids to model initial acceptance and higher acceptance in order to model user behavior of wanting to have some margin from min requirements\n",
    "\n",
    "def f_tot(x, w, b, slope_low, slope_high, Thresh_low, Thresh_high, acp_reward):\n",
    "    return acp_reward * f(x, w, b, slope_low, Thresh_low)+ (1-acp_reward) * f(x, w, b, slope_high, Thresh_high)\n",
    "\n",
    "def g_tot(x, w, b, slope_low, slope_high, Thresh_low, Thresh_high, acp_reward):\n",
    "    return acp_reward * g(x, w, b, slope_low, Thresh_low)+ (1-acp_reward) * g(x, w, b, slope_high, Thresh_high)\n",
    "\n",
    "def f_tot_derivative(x, w, b, slope_low, slope_high, Thresh_low, Thresh_high, acp_reward):\n",
    "    return acp_reward * f_derivative(x, w, b, slope_low, Thresh_low)+ (1-acp_reward) * f_derivative(x, w, b, slope_high, Thresh_high)\n",
    "\n",
    "def f_tot_batch(x, w, b, slope_low, slope_high, Thresh_low, Thresh_high, acp_reward):\n",
    "    return acp_reward * f_batch(x, w, b, slope_low, Thresh_low)+ (1-acp_reward) * f_batch(x, w, b, slope_high, Thresh_high)\n",
    "\n",
    "def g_tot_batch(x, w, b, slope_low, slope_high, Thresh_low, Thresh_high, acp_reward):\n",
    "    return acp_reward * g_batch(x, w, b, slope_low, Thresh_low)+ (1-acp_reward) * g_batch(x, w, b, slope_high, Thresh_high)\n",
    "\n",
    "def f_tot_derivative_batch(x, w, b, slope_low, slope_high, Thresh_low, Thresh_high, acp_reward):\n",
    "    return acp_reward * f_derivative_batch(x, w, b, slope_low, Thresh_low)+ (1-acp_reward) * f_derivative_batch(x, w, b, slope_high, Thresh_high)\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "funcs = {\n",
    "    ''\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCP classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a CCP class based on the one implemented in the paper referenced above.\n",
    "\n",
    "Convex-Concave Procedure (in short CCP) is an iterative method for solving optimization problems that are expressed as a difference of convex functions, like the one we have in our problem.\n",
    "\n",
    "It does so by iterating through a sequence of concave-relaxed problems (using Taylor series to linearize on of the functions), a process that guarantees convergence to local maxima.\n",
    "\n",
    "We use $h(r)$ which is **a linear combination** of the sigmoid-like proxy described above to model the users behavior.\n",
    "\n",
    "As reminded above, this sigmoid function can be written as a difference of convex functions and is therefore perfect for CCP. \n",
    "\n",
    "Because $h(r)$ is a linear combination of sigmoids, it can also be discribed as a difference of convex functions and we can use CCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T14:20:34.481959700Z",
     "start_time": "2024-04-21T14:20:34.441480500Z"
    }
   },
   "outputs": [],
   "source": [
    "class CCP_rank:\n",
    "    def __init__(self, x_dim, batch_size, funcs, scale):\n",
    "        self.f_derivative = funcs[\"f_derivative\"]\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.x = cp.Variable((batch_size, x_dim))\n",
    "        self.xt = cp.Parameter((batch_size, x_dim))\n",
    "        self.r = cp.Parameter((batch_size, x_dim)) # old reference point (initial x)\n",
    "        self.w = cp.Parameter(x_dim)\n",
    "        self.b = cp.Parameter(1) # bias parameter\n",
    "        # -------------------- adding our function's params --------------------\n",
    "        self.slope_low = cp.Parameter(1) # scale of the low threshold sigmoid\n",
    "        self.slope_high = cp.Parameter(1) # scale of the high threshold sigmoid\n",
    "        self.T_low = cp.Parameter(1) # low threshold of the sigmoid\n",
    "        self.T_high = cp.Parameter(1) # high threshold of the sigmoid\n",
    "        self.acp_reward = cp.Parameter(1) # defines the reward for passing initial threshhold - q in the equations \n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # -------------------- defining our target function --------------------\n",
    "        target = cp.diag(self.x@(self.f_derivative(self.xt, self.w, self.b, \n",
    "                        self.slope_low, self.slope_high, self.T_low, self.T_high, self.acp_reward).T)) - self.g(self.x, self.w,\n",
    "                        self.b, self.slope_low, self.slope_high, self.T_low, self.T_high, self.acp_reward)-self.c(self.x, self.r,\n",
    "                        x_dim, scale)\n",
    "        # ----------------------------------------------------------------------\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        # define the problem's objective\n",
    "        self.prob = cp.Problem(cp.Maximize(cp.sum(target)), constraints)\n",
    "\n",
    "    def ccp(self, r):\n",
    "        \"\"\"\n",
    "        numpy to numpy\n",
    "        \"\"\"\n",
    "        self.xt.value = r\n",
    "        self.r.value = r\n",
    "        result = self.prob.solve()\n",
    "        diff = np.linalg.norm(self.xt.value - self.x.value)\n",
    "        cnt = 0\n",
    "        while diff > 0.001 and cnt < 100:\n",
    "            cnt += 1\n",
    "            self.xt.value = self.x.value\n",
    "            result = self.prob.solve()\n",
    "            diff = np.linalg.norm(self.x.value - self.xt.value)/self.batch_size\n",
    "        return self.x.value\n",
    "\n",
    "    def optimize_X(self, X, w, b, slope_low, slope_high, T_low, T_high, acp_reward):\n",
    "        \"\"\"\n",
    "        tensor to tensor\n",
    "        \"\"\"\n",
    "        w = w.detach().numpy()\n",
    "        b = b.detach().numpy()\n",
    "        \n",
    "        # converting our params into numpy arrays with 1 value\n",
    "        slope_low = np.full(1, slope_low)\n",
    "        slope_high = np.full(1, slope_high)\n",
    "        T_low = np.full(1, T_low)\n",
    "        T_high = np.full(1, T_high)\n",
    "        acp_reward = np.full(1, acp_reward)\n",
    "\n",
    "        X = X.numpy()\n",
    "\n",
    "        self.w.value = w\n",
    "        self.b.value = b\n",
    "        self.slope_low.value = slope_low\n",
    "        self.slope_high.value = slope_high\n",
    "        self.T_low.value = T_low\n",
    "        self.T_high.value = T_high\n",
    "        self.acp_reward.value = acp_reward\n",
    "\n",
    "        return torch.from_numpy(self.ccp(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta \n",
    "This module uses the CCP procedure to compute the user responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T14:34:36.659054700Z",
     "start_time": "2024-04-21T14:34:36.637320100Z"
    }
   },
   "outputs": [],
   "source": [
    "class DELTA():\n",
    "\n",
    "    def __init__(self, x_dim, funcs, scale, slope_low, slope_high, acp_reward):\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "\n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.r = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.w = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.b = cp.Parameter(1, value = np.random.randn(1))\n",
    "        self.f_der = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        # -------------------- adding our function's params --------------------\n",
    "        self.slope_low = slope_low # scale of the low threshold sigmoid\n",
    "        self.slope_high = slope_high # scale of the high threshold sigmoid\n",
    "        self.T_low = cp.Parameter(1) # low threshold of the sigmoid\n",
    "        self.T_high = cp. Parameter(1) # high threshold of the sigmoid\n",
    "        self.acp_reward = acp_reward # the relation between the 2 slopes - defines the reward for passing initial threshhold\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # -------------------- defining our target function --------------------\n",
    "        target = self.x@self.f_der-self.g(self.x, self.w, self.b, \n",
    "                        self.slope_low, self.slope_high, self.T_low, \n",
    "                        self.T_high, self.acp_reward)-self.c(self.x, self.r, x_dim, scale)\n",
    "        # ----------------------------------------------------------------------\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        objective = cp.Maximize(target)\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        self.layer = CvxpyLayer(problem, parameters=[self.r, self.w, self.b, self.f_der, self.T_low, self.T_high], variables=[self.x])\n",
    "\n",
    "    def optimize_X(self, X, w, b, F_DER, T_low, T_high):\n",
    "        return self.layer(X, w, b, F_DER, T_low, T_high)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T14:22:18.666727200Z",
     "start_time": "2024-04-21T14:22:18.638830200Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: after updating the model delete all redundant fields\n",
    "class MyStrategicModel(torch.nn.Module):\n",
    "    def __init__(self, x_dim, batch_size, funcs, train_slope_low, train_slope_high, eval_slope_low, eval_slope_high, scale, acp_reward, env, strategic=False):\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        super(MyStrategicModel, self).__init__()\n",
    "        self.env = env\n",
    "        self.x_dim = x_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.train_slope_low, self.train_slope_high, self.test_slope_low, self.test_slope_high = train_slope_low, train_slope_high, eval_slope_low, eval_slope_high\n",
    "        self.w = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(x_dim, dtype=torch.float64, requires_grad=True)))\n",
    "        self.b = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(1, dtype=torch.float64, requires_grad=True)))\n",
    "        self.acp_reward = acp_reward\n",
    "        self.strategic = strategic\n",
    "        self.ccp = CCP_rank(x_dim, batch_size, funcs, scale)\n",
    "        self.delta = DELTA(x_dim, funcs, scale, train_slope_low, train_slope_high, acp_reward)\n",
    "        self.ccp_time = 0\n",
    "        self.total_time = 0\n",
    "\n",
    "\n",
    "    def forward(self, X, use_delta=False, evaluation=False):\n",
    "        \n",
    "        if self.strategic:\n",
    "            t1 = time.time()\n",
    "            # currently not distinguishing between train and test slopes\n",
    "            Xt, X_opt = self.env.full_dynamic_update(X, self.w, self.b, use_delta)\n",
    "            self.ccp_time += time.time()-t1\n",
    "            X_opt = Xt if evaluation else X_opt\n",
    "            output = self.score(X_opt)\n",
    "        else:\n",
    "            output = self.score(X)\n",
    "        return output\n",
    "\n",
    "    # def optimize_X(self, X, evaluation=False):\n",
    "    #     slope = self.eval_slope if evaluation else self.train_slope\n",
    "    #     return self.ccp.optimize_X(X, self.w, self.b, slope)\n",
    "\n",
    "    def normalize_weights(self):\n",
    "        with torch.no_grad():\n",
    "            norm = torch.sqrt(torch.sum(self.w**2) + self.b**2)\n",
    "            self.w /= norm\n",
    "            self.b /= norm\n",
    "\n",
    "    def score(self, x):\n",
    "        return x@self.w + self.b\n",
    "\n",
    "    def get_f_ders(self, XT, slope):\n",
    "        nablas = 0.5*slope*((slope*self.score(XT) + 1)/torch.sqrt((slope*self.score(XT) + 1)**2 + 1))\n",
    "        return torch.reshape(nablas, (len(nablas), 1))@torch.reshape(self.w, (1, len(self.w)))\n",
    "\n",
    "    def calc_accuracy(self, R, R_pred):\n",
    "        # calculating acceptance label from ratings\n",
    "        Y = self.rating_to_accept(R)\n",
    "        Y_pred = self.rating_to_accept(R_pred)\n",
    "\n",
    "        # calculating accuracy based on Y and Y_pred\n",
    "        temp = Y - Y_pred\n",
    "        acc = len(temp[temp == 0])*1./len(Y)\n",
    "        return acc\n",
    "    \n",
    "    # convert rating vector to acceptance\n",
    "    def rating_to_accept(self, R):\n",
    "        # calculating Y_pred out of score_pred\n",
    "        Y = np.zeros_like(R)\n",
    "        # getting the top-k values\n",
    "        top_k_ranking_indices = np.argsort(R)[-self.env.k:]\n",
    "        Y[top_k_ranking_indices] = 1\n",
    "        return Y\n",
    "        \n",
    "    # def evaluate(self, X, Y):\n",
    "    #     return self.calc_accuracy(Y, self.forward(X, evaluation=True))\n",
    "\n",
    "    # we use mean squared loss between provided ratings\n",
    "    def loss(self, Y, Y_pred):\n",
    "        return torch.mean((Y-Y_pred) ** 2)\n",
    "\n",
    "    def save_model(self, train_errors, val_errors, train_losses, val_losses, info, path, comment=None):\n",
    "        if comment is not None:\n",
    "            path += \"/\" + comment\n",
    "\n",
    "        filename = path + \"/model.pt\"\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        torch.save(self.state_dict(), filename)\n",
    "\n",
    "        pd.DataFrame(np.array(train_errors)).to_csv(path + '/train_errors.csv')\n",
    "        pd.DataFrame(np.array(val_errors)).to_csv(path + '/val_errors.csv')\n",
    "        pd.DataFrame(np.array(train_losses)).to_csv(path + '/train_losses.csv')\n",
    "        pd.DataFrame(np.array(val_losses)).to_csv(path + '/val_losses.csv')\n",
    "\n",
    "        with open(path + \"/info.txt\", \"w\") as f:\n",
    "            f.write(info)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        self.load_state_dict(torch.load(filename))\n",
    "        self.eval()\n",
    "\n",
    "    # we fit according to score as a proxy to the ranking for simplicity reasons\n",
    "    def fit(self, path, X, R, Xval, Rval, opt, opt_kwargs={\"lr\":1e-3}, batch_size=128, epochs=100, verbose=False, callback=None, comment=None):\n",
    "        # labels are the true rating \n",
    "        train_dset = TensorDataset(X, R)\n",
    "        train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "        test_dset = TensorDataset(Xval, Rval)\n",
    "        test_loader = DataLoader(test_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        opt = opt(self.parameters(), **opt_kwargs)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "\n",
    "        best_val_error = 1\n",
    "        consecutive_no_improvement = 0\n",
    "\n",
    "        total_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            t1 = time.time()\n",
    "            batch = 1\n",
    "            train_losses.append([])\n",
    "            train_errors.append([])\n",
    "            for Xbatch, Rbatch in train_loader:\n",
    "#                 try:\n",
    "                opt.zero_grad()\n",
    "                Rbatch_pred = self.forward(Xbatch)\n",
    "                l = self.loss(Rbatch, Rbatch_pred)\n",
    "                l.backward()\n",
    "                opt.step()\n",
    "                train_losses[-1].append(l.item())\n",
    "                with torch.no_grad():\n",
    "                    e = self.calc_accuracy(Rbatch, Rbatch_pred)\n",
    "                    train_errors[-1].append(1-e)\n",
    "                if verbose:\n",
    "                    print(\"batch %03d / %03d | loss: %3.5f | err: %3.5f\" %\n",
    "                          (batch, len(train_loader), np.mean(train_losses[-1]), np.mean(train_errors[-1])))\n",
    "                batch += 1\n",
    "                if callback is not None:\n",
    "                    callback()\n",
    "#                 except:\n",
    "#                     print(\"failed\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                total_loss = 0\n",
    "                total_error = 0\n",
    "                batch = 0\n",
    "                for Xbatch, Rbatch in test_loader:\n",
    "#                     try:\n",
    "                    Rval_pred = self.forward(Xbatch, evaluation=True)\n",
    "                    val_loss = self.loss(Rbatch, Rval_pred).item()\n",
    "                    total_loss += val_loss\n",
    "                    val_error = 1-self.calc_accuracy(Rbatch, Rval_pred)\n",
    "                    total_error += val_error\n",
    "                    batch += 1\n",
    "#                     except:\n",
    "#                         print(\"failed\")\n",
    "\n",
    "                avg_loss = total_loss/batch\n",
    "                avg_error = total_error/batch\n",
    "                val_losses.append(avg_loss)\n",
    "                val_errors.append(avg_error)\n",
    "                if avg_error < best_val_error:\n",
    "                        consecutive_no_improvement = 0\n",
    "                        best_val_error = avg_error\n",
    "                        info = \"training time in seconds: {}\\nepoch: {}\\nbatch size: {}\\nslope low: {}\\nslope high: {}\\nlearning rate: {}\\nvalidation loss: {}\\nvalidation error: {}\\n\".format(\n",
    "                        time.time()-total_time, epoch, batch_size, self.train_slope_low, self.train_slope_high, opt_kwargs[\"lr\"], avg_loss, avg_error)\n",
    "                        self.save_model(train_errors, val_errors, train_losses, val_losses, info, path, comment)\n",
    "                        print(\"model saved!\")\n",
    "\n",
    "                else:\n",
    "                    consecutive_no_improvement += 1\n",
    "                    if consecutive_no_improvement >= 4:\n",
    "                        break\n",
    "\n",
    "            t2 = time.time()\n",
    "            if verbose:\n",
    "                print(\"------------- epoch %03d / %03d | time: %03d sec | loss: %3.5f | err: %3.5f\" % (epoch + 1, epochs, t2-t1, val_losses[-1], val_errors[-1]))\n",
    "\n",
    "        self.total_time = time.time()-total_time\n",
    "        print(\"training time: {} seconds\".format(self.total_time))\n",
    "        return train_errors, val_errors, train_losses, val_losses\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "- user features generated randomly\n",
    "- ground truth rating model is $r(x)=u^Tx$ \n",
    "- observed noisy ratings $\\tilde{r}(x)=r(x)+\\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2_\\epsilon)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training Environment\n",
    "\n",
    "Each training step will include several dynamic iterations (user feature updates) and after that a single update of the system features $\\theta$.\n",
    "\n",
    "Pseudocode of the training:\n",
    "\n",
    "- until model convergence:\n",
    "    - for $t = 0:T$\n",
    "        - Get $T_{low},T_{high}$ from $r_{\\theta}(\\cdot)$ and $x_t$\n",
    "        - $x_{t+1} = \\Delta_h(x_t,T_{low},T_{high})$\n",
    "    - Update model based on $x_T,r$\n",
    "    - $w,b = GD(x_T,r)$\n",
    "\n",
    "In test time we will evaluate features after the dynamic iteration i.e., $x_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T14:30:14.450206900Z",
     "start_time": "2024-04-21T14:30:14.416652500Z"
    }
   },
   "outputs": [],
   "source": [
    "class RankingEnv():\n",
    "    \"\"\"\n",
    "    Strategic Classification Environment\n",
    "    \"\"\"\n",
    "    def __init__(self, noisyscale, n_features, k, k_tag, n_users, slope_low, slope_high, acp_reward, funcs, funcs_batch, scale):\n",
    "        self.noisyscale = noisyscale\n",
    "        self.n_features = n_features\n",
    "        self.k = k  # k is number of accepted users\n",
    "        self.k_tag = k_tag  # k' < k sets the threshold of confidant acceptance\n",
    "        self.system_attributes = np.random.normal(loc=0, scale=0.1, size=n_features)  # u system rating vector\n",
    "        \n",
    "        # system variables \n",
    "        self.w = torch.nn.parameter.Parameter(math.sqrt(1/n_features)*(1-2*torch.rand(n_features, dtype=torch.float64, requires_grad=True)))\n",
    "        self.b = torch.nn.parameter.Parameter(math.sqrt(1/n_features)*(1-2*torch.rand(1, dtype=torch.float64, requires_grad=True)))\n",
    "        \n",
    "        # Delta parameters\n",
    "        self.slope_low = slope_low # scale of the low threshold sigmoid\n",
    "        self.slope_high = slope_high # scale of the high threshold sigmoid\n",
    "        self.acp_reward = acp_reward # the relation between the 2 slopes - defines the reward for passing initial threshhold\n",
    "        self.scale = scale  # cost function scale\n",
    "        self.funcs = funcs\n",
    "        self.ccp = CCP_rank(n_features, batch_size=n_users, funcs=funcs_batch, scale=scale)\n",
    "        self.delta = DELTA(n_features, funcs, scale, slope_low, slope_high, acp_reward)\n",
    "\n",
    "        \n",
    "        # user variables \n",
    "        self.n_users = n_users\n",
    "        self.users_initial = self.generate_users(n_users=n_users)\n",
    "        # self.users_current = self.users_initial\n",
    "        self.t = 0  # time indicator\n",
    "        self.T_max = 5  #  number of dynamic iterations\n",
    "        \n",
    "    def generate_users(self, n_users):\n",
    "        return np.random.normal(\n",
    "            loc=0,\n",
    "            scale=0.1,\n",
    "            size = (n_users, \n",
    "                    self.n_features,\n",
    "                    ),\n",
    "        )\n",
    "    \n",
    "    # def get_users_current(self):\n",
    "    #     return self.users_current\n",
    "    \n",
    "    def get_users_initial(self):\n",
    "        return self.users_initial\n",
    "    \n",
    "    def rating(self, users):\n",
    "        # return noisy users rating, when the noise is normally distributed\n",
    "        clean_ratings = self.system_attributes@users\n",
    "        noise = np.random.normal(loc=0, scale=self.noisyscale, size = (len(clean_ratings)))\n",
    "        return clean_ratings + noise\n",
    "    \n",
    "    def get_thresh(self, X):\n",
    "        ranking = self.score(X)\n",
    "        # reversing the sort into a descending order\n",
    "        sorted_ranking = np.sort(ranking)[::-1]\n",
    "        # calculate thresholds from k and k_tag\n",
    "        return sorted_ranking[self.k-1], sorted_ranking[self.k_tag-1]\n",
    "    \n",
    "    # returns ground truth ratings for users\n",
    "    def get_true_accepted(self):\n",
    "        ranking = self.rating(self.users_initial)\n",
    "        acceptance = np.zeros_like(ranking)\n",
    "        # sort ranking in descending order\n",
    "        sorted_ranking = np.argsort(ranking)[::-1]\n",
    "        acceptance[sorted_ranking[:self.k-1]] = 1\n",
    "        return acceptance\n",
    "    \n",
    "    def user_features_update(self, X, Xt, use_delta=False):\n",
    "        # update features based on the response function\n",
    "        self.t += 1\n",
    "        # update features\n",
    "        T_low, T_high = self.get_thresh(Xt)\n",
    "        XT = self.ccp.optimize_X(X, self.w, self.b, self.slope_low, self.slope_high, T_low,\n",
    "                                 T_high, self.acp_reward)\n",
    "        if use_delta:\n",
    "            F_DER = self.get_f_ders(XT, self.slope_low, self.slope_high, T_low, T_high)\n",
    "            X_opt = self.delta.optimize_X(X, self.w, self.b, F_DER, T_low, T_high) # Xopt should be equal to XT but repeat for gradients\n",
    "            return X_opt # this includes the whole dynamic process in the derivatives\n",
    "        else:\n",
    "            return XT \n",
    "        \n",
    "    def update_wb(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.t = 0  # reset time as new paramemters show \n",
    "    \n",
    "    def full_dynamic_update(self, X, w, b, use_delta=False):\n",
    "        self.update_wb(w, b)\n",
    "        Xt = X\n",
    "        for _ in range(self.T_max):\n",
    "            Xt = self.user_features_update(X, Xt, use_delta=use_delta)\n",
    "        # if delta wasn't used it must be used in the last step\n",
    "        X_opt = Xt\n",
    "        if not use_delta:\n",
    "            T_low, T_high = self.get_thresh(Xt)\n",
    "            F_DER = self.get_f_ders(Xt, self.slope_low, self.slope_high, T_low, T_high)\n",
    "            X_opt = self.delta.optimize_X(X, self.w, self.b, F_DER, T_low, T_high)\n",
    "        # returning both in order to support evaluation condition in forward\n",
    "        return Xt, X_opt\n",
    "    \n",
    "    def get_f_ders_helper(self, XT, slope, T):\n",
    "        nablas = 0.5*slope*((slope*(self.score(XT) - T) + 1)/torch.sqrt((slope*(self.score(XT) - T) + 1)**2 + 1))\n",
    "        return torch.reshape(nablas, (len(nablas), 1))@torch.reshape(self.w, (1, len(self.w)))\n",
    "    \n",
    "    def get_f_ders(self, XT, slope_low, slope_high, T_low, T_high):\n",
    "        return (self.acp_reward*self.get_f_ders_helper(XT, slope_low, T_low) + \n",
    "                (1-self.acp_reward)*self.get_f_ders_helper(XT, slope_high, T_high))\n",
    "    \n",
    "    def score(self, x):\n",
    "        return x@self.w + self.b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 5)\n",
      "(200, 5)\n",
      "(5,)\n",
      "(5,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_problem_data() got an unexpected keyword argument 'solver_opts'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-18-9c22c19f8fc7>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m               'scale' : 1}\n\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m \u001B[0mrank_env\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mRankingEnv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0menv_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-11-c41571d0e0ec>\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, noisyscale, n_features, k, k_tag, n_users, slope_low, slope_high, acp_reward, funcs, funcs_batch, scale)\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfuncs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfuncs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mccp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCCP_rank\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_features\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mn_users\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfuncs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfuncs_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscale\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mscale\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 23\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdelta\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDELTA\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_features\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfuncs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscale\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mslope_low\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mslope_high\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0macp_reward\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-17-0280e36d84f4>\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, x_dim, funcs, scale, slope_low, slope_high, acp_reward)\u001B[0m\n\u001B[0;32m     27\u001B[0m         \u001B[0mobjective\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMaximize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m         \u001B[0mproblem\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mProblem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobjective\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconstraints\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCvxpyLayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mproblem\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparameters\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_der\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mT_low\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mT_high\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvariables\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0moptimize_X\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mF_DER\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mT_low\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mT_high\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpylayers\\torch\\cvxpylayer.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, problem, parameters, variables, gp)\u001B[0m\n\u001B[0;32m    113\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    114\u001B[0m             data, _, _ = problem.get_problem_data(\n\u001B[1;32m--> 115\u001B[1;33m                 solver=cp.SCS, solver_opts={'use_quad_obj': False})\n\u001B[0m\u001B[0;32m    116\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompiler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msettings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPARAM_PROB\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    117\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparam_ids\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mid\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparam_order\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: get_problem_data() got an unexpected keyword argument 'solver_opts'"
     ]
    }
   ],
   "source": [
    "# initialize env\n",
    "funcs = {\"f\": f_tot, \"g\": g_tot, \"f_derivative\": f_tot_derivative, \"c\": c, \"score\": score}\n",
    "funcs_batch = {\"f\": f_tot_batch, \"g\": g_tot_batch, \"f_derivative\": f_tot_derivative_batch, \"c\": c_batch, \"score\": score}\n",
    "env_params = {'noisyscale' : 0.1,\n",
    "              'n_features' : 5,\n",
    "              'k' : 40,\n",
    "              'k_tag' : 10,\n",
    "              'n_users': 200,\n",
    "              'slope_low' : 40,\n",
    "              'slope_high' : 10,\n",
    "              'acp_reward' : 0.6,\n",
    "              'funcs' : funcs,\n",
    "              'funcs_batch' : funcs_batch,\n",
    "              'scale' : 1}\n",
    "\n",
    "rank_env = RankingEnv(**env_params)\n",
    "\n",
    "# TODO: debug, try creating an environment, generate train and test \n",
    "# train and generate plots"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T14:34:45.396368600Z",
     "start_time": "2024-04-21T14:34:45.335307600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_sklearn_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-a66ad6de1c0e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mx_dim\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mscale\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mY\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgen_sklearn_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_dim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1024\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mY\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mXval\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mYval\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msplit_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mY\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.25\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mXval\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'gen_sklearn_data' is not defined"
     ]
    }
   ],
   "source": [
    "path = \"./models/first_test\"\n",
    "epochs = 5\n",
    "x_dim = 5\n",
    "scale = 1\n",
    "X, Y = gen_sklearn_data(x_dim, 1024)\n",
    "X, Y, Xval, Yval = split_data(X, Y, 0.25)\n",
    "print(Xval.size())\n",
    "print(\"percent of positive samples: {}%\".format(100 * len(Y[Y == 1]) / len(Y)))\n",
    "\n",
    "\n",
    "total = []\n",
    "ccp = []\n",
    "for batch_size in (2**np.arange(9)).tolist():\n",
    "    strategic_model = MyStrategicModel(x_dim, batch_size, funcs, funcs_batch, TRAIN_SLOPE, EVAL_SLOPE, scale=scale, strategic=True)\n",
    "    strategic_model.fit(path, X, Y, Xval, Yval,\n",
    "                        opt=torch.optim.Adam, opt_kwargs={\"lr\": (1e-1)},\n",
    "                        batch_size=batch_size, epochs=epochs, verbose=True,\n",
    "                       comment=\"batched\")\n",
    "    \n",
    "    total_time = strategic_model.total_time\n",
    "    ccp_time = strategic_model.ccp_time\n",
    "    total.append(total_time)\n",
    "    ccp.append(ccp_time)\n",
    "    pd.DataFrame(np.array(total)).to_csv(path + '/total_timing_results.csv')\n",
    "    pd.DataFrame(np.array(ccp)).to_csv(path + '/ccp_timing_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
