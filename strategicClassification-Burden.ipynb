{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T08:13:55.623597700Z",
     "start_time": "2024-05-11T08:13:53.403820700Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "import numpy as np\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import zero_one_loss, confusion_matrix\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.patches as mpatches\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os, psutil\n",
    "from datetime import datetime\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "XDIM = 11\n",
    "COST = 1/XDIM\n",
    "TRAIN_SLOPE = 2\n",
    "EVAL_SLOPE = 5\n",
    "X_LOWER_BOUND = -10\n",
    "X_UPPER_BOUND = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T08:13:55.643601300Z",
     "start_time": "2024-05-11T08:13:55.629136100Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(X, Y, percentage):\n",
    "    num_val = int(len(X)*percentage)\n",
    "    return X[num_val:], Y[num_val:], X[:num_val], Y[:num_val]\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    data = torch.cat((X, Y), 1)\n",
    "    data = data[torch.randperm(data.size()[0])]\n",
    "    X = data[:, :2]\n",
    "    Y = data[:, 2]\n",
    "    return X, Y\n",
    "\n",
    "def conf_mat(Y1, Y2):\n",
    "    num_of_samples = len(Y1)\n",
    "    mat = confusion_matrix(Y1, Y2, labels=[-1, 1])*100/num_of_samples\n",
    "    acc = np.trace(mat)\n",
    "    return mat, acc\n",
    "\n",
    "def calc_accuracy(Y, Ypred):\n",
    "    num = len(Y)\n",
    "    temp = Y - Ypred\n",
    "    acc = len(temp[temp == 0])*1./num\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T08:13:55.664861900Z",
     "start_time": "2024-05-11T08:13:55.644598100Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_credit_default_data():\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    url = 'https://raw.githubusercontent.com/ustunb/actionable-recourse/master/examples/paper/data/credit_processed.csv'\n",
    "    df = pd.read_csv(url)\n",
    "    df[\"NoDefaultNextMonth\"].replace({0: -1}, inplace=True)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    df = df.drop(['Married', 'Single', 'Age_lt_25', 'Age_in_25_to_40', 'Age_in_40_to_59', 'Age_geq_60'], axis = 1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df.loc[:, df.columns != \"NoDefaultNextMonth\"] = scaler.fit_transform(df.drop(\"NoDefaultNextMonth\", axis=1))\n",
    "\n",
    "    fraud_df = df.loc[df[\"NoDefaultNextMonth\"] == -1]\n",
    "    non_fraud_df = df.loc[df[\"NoDefaultNextMonth\"] == 1][:6636]\n",
    "\n",
    "    normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "    # Shuffle dataframe rows\n",
    "    df = normal_distributed_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    Y, X = df.iloc[:, 0].values, df.iloc[:, 1:].values\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCP classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T08:13:55.680297600Z",
     "start_time": "2024-05-11T08:13:55.663865800Z"
    }
   },
   "outputs": [],
   "source": [
    "class CCP:\n",
    "    def __init__(self, x_dim, funcs):\n",
    "        self.f_derivative = funcs[\"f_derivative\"]\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.xt = cp.Parameter(x_dim)\n",
    "        self.r = cp.Parameter(x_dim)\n",
    "        self.w = cp.Parameter(x_dim)\n",
    "        self.b = cp.Parameter(1)\n",
    "        self.slope = cp.Parameter(1)\n",
    "\n",
    "        target = self.x@self.f_derivative(self.xt, self.w, self.b, self.slope)-self.g(self.x, self.w, self.b, self.slope)-self.c(self.x, self.r)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        self.prob = cp.Problem(cp.Maximize(target), constraints)\n",
    "        \n",
    "    def ccp(self, r):\n",
    "        \"\"\"\n",
    "        numpy to numpy\n",
    "        \"\"\"\n",
    "        self.xt.value = r\n",
    "        self.r.value = r\n",
    "        result = self.prob.solve()\n",
    "        diff = np.linalg.norm(self.xt.value - self.x.value)\n",
    "        cnt = 0\n",
    "        while diff > 0.0001 and cnt < 10:\n",
    "            cnt += 1\n",
    "            self.xt.value = self.x.value\n",
    "            result = self.prob.solve()\n",
    "            diff = np.linalg.norm(self.x.value - self.xt.value)\n",
    "        return self.x.value\n",
    "    \n",
    "    def optimize_X(self, X, w, b, slope):\n",
    "        \"\"\"\n",
    "        tensor to tensor\n",
    "        \"\"\"\n",
    "        w = w.detach().numpy()\n",
    "        b = b.detach().numpy()\n",
    "        slope = np.full(1, slope)\n",
    "        X = X.numpy()\n",
    "        \n",
    "        self.w.value = w\n",
    "        self.b.value = b\n",
    "        self.slope.value = slope\n",
    "        \n",
    "        return torch.stack([torch.from_numpy(self.ccp(x)) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T08:13:55.711874Z",
     "start_time": "2024-05-11T08:13:55.673808900Z"
    }
   },
   "outputs": [],
   "source": [
    "class DELTA():\n",
    "    \n",
    "    def __init__(self, x_dim, funcs):\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.r = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.w = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.b = cp.Parameter(1, value = np.random.randn(1))\n",
    "        self.f_der = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "\n",
    "        target = self.x@self.f_der-self.g(self.x, self.w, self.b, TRAIN_SLOPE)-self.c(self.x, self.r)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        objective = cp.Maximize(target)\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        self.layer = CvxpyLayer(problem, parameters=[self.r, self.w, self.b, self.f_der],\n",
    "                                variables=[self.x])\n",
    "        \n",
    "        \n",
    "    def optimize_X(self, X, w, b, F_DER):\n",
    "        return self.layer(X, w, b, F_DER)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T08:13:55.711874Z",
     "start_time": "2024-05-11T08:13:55.695199700Z"
    }
   },
   "outputs": [],
   "source": [
    "class BURDEN():\n",
    "    \n",
    "    def __init__(self, x_dim, funcs):\n",
    "        self.c = funcs[\"c\"]\n",
    "        self.score = funcs[\"score\"]\n",
    "\n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.r = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.w = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.b = cp.Parameter(1, value = np.random.randn(1))\n",
    "\n",
    "        target = self.c(self.x, self.r)\n",
    "        constraints = [self.score(self.x, self.w, self.b) >= 0,\n",
    "                       self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "\n",
    "        objective = cp.Minimize(target)\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        self.layer = CvxpyLayer(problem, parameters=[self.r, self.w, self.b], variables=[self.x])\n",
    "        \n",
    "        \n",
    "    def calc_burden(self, X, Y, w, b):\n",
    "        \"\"\"\n",
    "        tensor to tensor\n",
    "        \"\"\"\n",
    "        Xpos = X[Y==1]\n",
    "        if len(Xpos) == 0:\n",
    "            return 0\n",
    "        Xmin = self.layer(Xpos, w, b)[0]\n",
    "        return torch.mean(torch.sum((Xpos-Xmin)**2, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain & Cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T08:13:55.723572600Z",
     "start_time": "2024-05-11T08:13:55.706850500Z"
    }
   },
   "outputs": [],
   "source": [
    "def score(x, w, b):\n",
    "    return x@w + b\n",
    "\n",
    "def f(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*score(x, w, b) + 1)]), 2)\n",
    "\n",
    "def g(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*score(x, w, b) - 1)]), 2)\n",
    "\n",
    "def c(x, r):\n",
    "    return COST*cp.sum_squares(x-r)\n",
    "\n",
    "def f_derivative(x, w, b, slope):\n",
    "    return 0.5*cp.multiply(slope*((slope*score(x, w, b) + 1)/cp.sqrt((slope*score(x, w, b) + 1)**2 + 1)), w)\n",
    "\n",
    "funcs = {\"f\": f, \"g\": g, \"f_derivative\": f_derivative, \"c\": c, \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-05-11T08:13:57.152682900Z",
     "start_time": "2024-05-11T08:13:55.722568400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of positive samples: 49.266666666666666%\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_credit_default_data()\n",
    "X, Y = X[:3000], Y[:3000]\n",
    "\n",
    "assert(len(X[0]) == XDIM)\n",
    "X, Y, Xval, Yval = split_data(X, Y, 0.5)\n",
    "Xval, Yval, Xtest, Ytest = split_data(Xval, Yval, 0.5)\n",
    "\n",
    "print(\"percent of positive samples: {}%\".format(100 * len(Y[Y == 1]) / len(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T08:13:57.219326700Z",
     "start_time": "2024-05-11T08:13:57.165671Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyStrategicModel(torch.nn.Module):\n",
    "    def __init__(self, x_dim, funcs, train_slope, eval_slope, strategic=False, lamb=0):\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        super(MyStrategicModel, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.train_slope, self.eval_slope = train_slope, eval_slope\n",
    "        self.w = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(x_dim, dtype=torch.float64, requires_grad=True)))\n",
    "        self.b = torch.nn.parameter.Parameter(torch.rand(1, dtype=torch.float64, requires_grad=True))\n",
    "        self.strategic = strategic\n",
    "        self.lamb = lamb\n",
    "        self.ccp = CCP(x_dim, funcs)\n",
    "        self.delta = DELTA(x_dim, funcs)\n",
    "        self.burden = BURDEN(x_dim, funcs)\n",
    "\n",
    "    def forward(self, X, evaluation=False):\n",
    "        slope = self.eval_slope if evaluation else self.train_slope\n",
    "        \n",
    "        if self.strategic:\n",
    "            XT = self.ccp.optimize_X(X, self.w, self.b, slope)\n",
    "            F_DER = self.get_f_ders(XT, slope)\n",
    "            X_opt = self.delta.optimize_X(X, self.w, self.b, F_DER) # Xopt should equal to XT but we do it again for the gradients\n",
    "\n",
    "            output = self.score(X_opt)\n",
    "        else:\n",
    "            output = self.score(X)        \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def optimize_X(self, X, evaluation=False):\n",
    "        slope = self.eval_slope if evaluation else self.train_slope\n",
    "        return self.ccp.optimize_X(X, self.w, self.b, slope)\n",
    "    \n",
    "    def score(self, x):\n",
    "        return x@self.w + self.b\n",
    "    \n",
    "    def get_f_ders(self, XT, slope):\n",
    "        return torch.stack([0.5*slope*((slope*self.score(xt) + 1)/torch.sqrt((slope*self.score(xt) + 1)**2 + 1))*self.w for xt in XT])\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        Y_pred = torch.sign(self.forward(X, evaluation=True))\n",
    "        num = len(Y)\n",
    "        temp = Y - Y_pred\n",
    "        acc = len(temp[temp == 0])*1./num        \n",
    "        return acc\n",
    "    \n",
    "    def calc_burden(self, X, Y):\n",
    "        return self.burden.calc_burden(X, Y, self.w, self.b)\n",
    "    \n",
    "    def loss(self, X, Y, Y_pred):\n",
    "        if self.lamb > 0:\n",
    "            return torch.mean(torch.clamp(1 - Y_pred * Y, min=0)) + self.lamb*self.calc_burden(X, Y)\n",
    "        else:\n",
    "            return torch.mean(torch.clamp(1 - Y_pred * Y, min=0))\n",
    "        \n",
    "    \n",
    "    def save_model(self, X, Y, Xval, Yval, Xtest, Ytest, train_errors, val_errors, train_losses, val_losses, val_burdens, info, path, comment=None):\n",
    "        if comment is not None:\n",
    "            path += \"_____\" + comment\n",
    "            \n",
    "        filename = path + \"/model.pt\"\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        torch.save(self.state_dict(), filename)\n",
    "        \n",
    "        pd.DataFrame(X.numpy()).to_csv(path + '/X.csv')\n",
    "        pd.DataFrame(Y.numpy()).to_csv(path + '/Y.csv')\n",
    "        pd.DataFrame(Xval.numpy()).to_csv(path + '/Xval.csv')\n",
    "        pd.DataFrame(Yval.numpy()).to_csv(path + '/Yval.csv')\n",
    "        pd.DataFrame(Xval.numpy()).to_csv(path + '/Xtest.csv')\n",
    "        pd.DataFrame(Yval.numpy()).to_csv(path + '/Ytest.csv')\n",
    "        \n",
    "        pd.DataFrame(np.array(train_errors)).to_csv(path + '/train_errors.csv')\n",
    "        pd.DataFrame(np.array(val_errors)).to_csv(path + '/val_errors.csv')\n",
    "        pd.DataFrame(np.array(train_losses)).to_csv(path + '/train_losses.csv')\n",
    "        pd.DataFrame(np.array(val_losses)).to_csv(path + '/val_losses.csv')\n",
    "        pd.DataFrame(np.array(val_burdens)).to_csv(path + '/val_burdens.csv')\n",
    "        \n",
    "        with open(path + \"/info.txt\", \"w\") as f:\n",
    "            f.write(info)\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        self.load_state_dict(torch.load(filename))\n",
    "        self.eval()\n",
    "    \n",
    "    def fit(self, X, Y, Xval, Yval, Xtest, Ytest, opt, opt_kwargs={\"lr\":1e-3}, batch_size=128, epochs=100, verbose=False, callback=None, calc_train_errors=False, comment=None):\n",
    "        train_dset = TensorDataset(X, Y)\n",
    "        train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "        opt = opt(self.parameters(), **opt_kwargs)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "        val_burdens = []\n",
    "        \n",
    "        best_val_error = 1\n",
    "        consecutive_no_improvement = 0\n",
    "        now = datetime.now()\n",
    "        path = \"./models/burden/\" + now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "\n",
    "        total_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            t1 = time.time()\n",
    "            batch = 1\n",
    "            train_losses.append([])\n",
    "            train_errors.append([])\n",
    "            for Xbatch, Ybatch in train_loader:\n",
    "                opt.zero_grad()\n",
    "                Ybatch_pred = self.forward(Xbatch)\n",
    "                l = self.loss(Xbatch, Ybatch, Ybatch_pred)\n",
    "                l.backward()\n",
    "                opt.step()\n",
    "                train_losses[-1].append(l.item())\n",
    "                if calc_train_errors:\n",
    "                    with torch.no_grad():\n",
    "                        e = self.evaluate(Xbatch, Ybatch)\n",
    "                        train_errors[-1].append(1-e)\n",
    "                if verbose:\n",
    "                    print(\"batch %03d / %03d | loss: %3.5f\" %\n",
    "                          (batch, len(train_loader), np.mean(train_losses[-1])))\n",
    "                batch += 1\n",
    "                if callback is not None:\n",
    "                    callback()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                Yval_pred = self.forward(Xval, evaluation=True)\n",
    "                val_loss = self.loss(Xval, Yval, Yval_pred).item()\n",
    "                val_losses.append(val_loss)\n",
    "                val_error = 1-self.evaluate(Xval, Yval)\n",
    "                val_errors.append(val_error)\n",
    "                val_burden = self.calc_burden(Xval, Yval).item()\n",
    "                val_burdens.append(val_burden)\n",
    "                if val_error < best_val_error:\n",
    "                    consecutive_no_improvement = 0\n",
    "                    best_val_error = val_error\n",
    "                    if self.strategic:\n",
    "                        info = \"training time in seconds: {}\\nepoch: {}\\nbatch size: {}\\ntrain slope: {}\\neval slope: {}\\nlearning rate: {}\\nvalidation loss: {}\\nvalidation error: {}\\nburden: {}\".format(\n",
    "                        time.time()-total_time, epoch, batch_size, self.train_slope, self.eval_slope, opt_kwargs[\"lr\"], val_loss, val_error, val_burden)\n",
    "                        self.save_model(X, Y, Xval, Yval, Xtest, Ytest, train_errors, val_errors, train_losses, val_losses, val_burdens, info, path, comment)\n",
    "                        print(\"model saved!\")\n",
    "                else:\n",
    "                    consecutive_no_improvement += 1\n",
    "                    if consecutive_no_improvement >= 5:\n",
    "                        break\n",
    "                \n",
    "            t2 = time.time()\n",
    "            if verbose:\n",
    "                print(\"----- epoch %03d / %03d | time: %03d sec | loss: %3.5f | err: %3.5f\" % (epoch + 1, epochs, t2-t1, val_losses[-1], val_errors[-1]))\n",
    "        print(\"training time: {} seconds\".format(time.time()-total_time)) \n",
    "        return train_errors, val_errors, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-05-11T17:13:09.537884800Z",
     "start_time": "2024-05-11T08:13:57.184172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- training non-strategically----------\n",
      "batch 001 / 024 | loss: 1.02473\n",
      "batch 002 / 024 | loss: 0.98493\n",
      "batch 003 / 024 | loss: 0.94925\n",
      "batch 004 / 024 | loss: 0.91794\n",
      "batch 005 / 024 | loss: 0.87623\n",
      "batch 006 / 024 | loss: 0.85815\n",
      "batch 007 / 024 | loss: 0.83093\n",
      "batch 008 / 024 | loss: 0.79413\n",
      "batch 009 / 024 | loss: 0.79697\n",
      "batch 010 / 024 | loss: 0.78610\n",
      "batch 011 / 024 | loss: 0.77937\n",
      "batch 012 / 024 | loss: 0.77649\n",
      "batch 013 / 024 | loss: 0.78361\n",
      "batch 014 / 024 | loss: 0.76675\n",
      "batch 015 / 024 | loss: 0.75696\n",
      "batch 016 / 024 | loss: 0.75645\n",
      "batch 017 / 024 | loss: 0.74582\n",
      "batch 018 / 024 | loss: 0.75056\n",
      "batch 019 / 024 | loss: 0.74364\n",
      "batch 020 / 024 | loss: 0.74202\n",
      "batch 021 / 024 | loss: 0.73184\n",
      "batch 022 / 024 | loss: 0.72664\n",
      "batch 023 / 024 | loss: 0.72057\n",
      "batch 024 / 024 | loss: 0.71777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 1 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 2 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 3 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- epoch 001 / 010 | time: 001 sec | loss: 0.67070 | err: 0.30667\n",
      "batch 001 / 024 | loss: 0.63756\n",
      "batch 002 / 024 | loss: 0.74415\n",
      "batch 003 / 024 | loss: 0.65346\n",
      "batch 004 / 024 | loss: 0.64884\n",
      "batch 005 / 024 | loss: 0.66767\n",
      "batch 006 / 024 | loss: 0.69588\n",
      "batch 007 / 024 | loss: 0.68155\n",
      "batch 008 / 024 | loss: 0.68499\n",
      "batch 009 / 024 | loss: 0.65439\n",
      "batch 010 / 024 | loss: 0.64327\n",
      "batch 011 / 024 | loss: 0.63350\n",
      "batch 012 / 024 | loss: 0.62305\n",
      "batch 013 / 024 | loss: 0.62653\n",
      "batch 014 / 024 | loss: 0.62231\n",
      "batch 015 / 024 | loss: 0.61317\n",
      "batch 016 / 024 | loss: 0.60766\n",
      "batch 017 / 024 | loss: 0.61433\n",
      "batch 018 / 024 | loss: 0.61720\n",
      "batch 019 / 024 | loss: 0.61618\n",
      "batch 020 / 024 | loss: 0.61311\n",
      "batch 021 / 024 | loss: 0.61944\n",
      "batch 022 / 024 | loss: 0.62171\n",
      "batch 023 / 024 | loss: 0.61818\n",
      "batch 024 / 024 | loss: 0.62587\n",
      "----- epoch 002 / 010 | time: 001 sec | loss: 0.64266 | err: 0.30667\n",
      "batch 001 / 024 | loss: 0.62060\n",
      "batch 002 / 024 | loss: 0.71489\n",
      "batch 003 / 024 | loss: 0.72164\n",
      "batch 004 / 024 | loss: 0.68468\n",
      "batch 005 / 024 | loss: 0.69994\n",
      "batch 006 / 024 | loss: 0.67327\n",
      "batch 007 / 024 | loss: 0.67349\n",
      "batch 008 / 024 | loss: 0.66118\n",
      "batch 009 / 024 | loss: 0.66065\n",
      "batch 010 / 024 | loss: 0.65403\n",
      "batch 011 / 024 | loss: 0.63992\n",
      "batch 012 / 024 | loss: 0.64438\n",
      "batch 013 / 024 | loss: 0.63067\n",
      "batch 014 / 024 | loss: 0.62576\n",
      "batch 015 / 024 | loss: 0.61634\n",
      "batch 016 / 024 | loss: 0.61890\n",
      "batch 017 / 024 | loss: 0.61267\n",
      "batch 018 / 024 | loss: 0.61603\n",
      "batch 019 / 024 | loss: 0.60595\n",
      "batch 020 / 024 | loss: 0.61311\n",
      "batch 021 / 024 | loss: 0.60613\n",
      "batch 022 / 024 | loss: 0.61076\n",
      "batch 023 / 024 | loss: 0.60608\n",
      "batch 024 / 024 | loss: 0.60762\n",
      "----- epoch 003 / 010 | time: 001 sec | loss: 0.65323 | err: 0.30667\n",
      "batch 001 / 024 | loss: 0.55966\n",
      "batch 002 / 024 | loss: 0.62755\n",
      "batch 003 / 024 | loss: 0.58902\n",
      "batch 004 / 024 | loss: 0.63154\n",
      "batch 005 / 024 | loss: 0.61220\n",
      "batch 006 / 024 | loss: 0.59015\n",
      "batch 007 / 024 | loss: 0.58870\n",
      "batch 008 / 024 | loss: 0.58726\n",
      "batch 009 / 024 | loss: 0.57987\n",
      "batch 010 / 024 | loss: 0.56619\n",
      "batch 011 / 024 | loss: 0.57681\n",
      "batch 012 / 024 | loss: 0.58516\n",
      "batch 013 / 024 | loss: 0.58731\n",
      "batch 014 / 024 | loss: 0.60516\n",
      "batch 015 / 024 | loss: 0.59701\n",
      "batch 016 / 024 | loss: 0.60178\n",
      "batch 017 / 024 | loss: 0.59157\n",
      "batch 018 / 024 | loss: 0.59214\n",
      "batch 019 / 024 | loss: 0.58607\n",
      "batch 020 / 024 | loss: 0.59040\n",
      "batch 021 / 024 | loss: 0.60007\n",
      "batch 022 / 024 | loss: 0.59567\n",
      "batch 023 / 024 | loss: 0.59165\n",
      "batch 024 / 024 | loss: 0.60366\n",
      "----- epoch 004 / 010 | time: 001 sec | loss: 0.62349 | err: 0.30533\n",
      "batch 001 / 024 | loss: 0.67006\n",
      "batch 002 / 024 | loss: 0.65754\n",
      "batch 003 / 024 | loss: 0.65455\n",
      "batch 004 / 024 | loss: 0.61744\n",
      "batch 005 / 024 | loss: 0.59971\n",
      "batch 006 / 024 | loss: 0.60106\n",
      "batch 007 / 024 | loss: 0.59036\n",
      "batch 008 / 024 | loss: 0.58967\n",
      "batch 009 / 024 | loss: 0.61068\n",
      "batch 010 / 024 | loss: 0.58192\n",
      "batch 011 / 024 | loss: 0.57644\n",
      "batch 012 / 024 | loss: 0.58821\n",
      "batch 013 / 024 | loss: 0.57784\n",
      "batch 014 / 024 | loss: 0.57392\n",
      "batch 015 / 024 | loss: 0.57213\n",
      "batch 016 / 024 | loss: 0.57629\n",
      "batch 017 / 024 | loss: 0.57372\n",
      "batch 018 / 024 | loss: 0.57549\n",
      "batch 019 / 024 | loss: 0.58230\n",
      "batch 020 / 024 | loss: 0.58828\n",
      "batch 021 / 024 | loss: 0.59293\n",
      "batch 022 / 024 | loss: 0.59464\n",
      "batch 023 / 024 | loss: 0.59506\n",
      "batch 024 / 024 | loss: 0.59793\n",
      "----- epoch 005 / 010 | time: 001 sec | loss: 0.63300 | err: 0.30533\n",
      "batch 001 / 024 | loss: 0.43385\n",
      "batch 002 / 024 | loss: 0.50224\n",
      "batch 003 / 024 | loss: 0.51183\n",
      "batch 004 / 024 | loss: 0.56091\n",
      "batch 005 / 024 | loss: 0.57535\n",
      "batch 006 / 024 | loss: 0.58680\n",
      "batch 007 / 024 | loss: 0.58580\n",
      "batch 008 / 024 | loss: 0.61365\n",
      "batch 009 / 024 | loss: 0.61411\n",
      "batch 010 / 024 | loss: 0.60075\n",
      "batch 011 / 024 | loss: 0.59367\n",
      "batch 012 / 024 | loss: 0.58045\n",
      "batch 013 / 024 | loss: 0.58594\n",
      "batch 014 / 024 | loss: 0.58652\n",
      "batch 015 / 024 | loss: 0.59315\n",
      "batch 016 / 024 | loss: 0.61089\n",
      "batch 017 / 024 | loss: 0.60575\n",
      "batch 018 / 024 | loss: 0.60546\n",
      "batch 019 / 024 | loss: 0.61155\n",
      "batch 020 / 024 | loss: 0.60787\n",
      "batch 021 / 024 | loss: 0.60468\n",
      "batch 022 / 024 | loss: 0.60740\n",
      "batch 023 / 024 | loss: 0.59986\n",
      "batch 024 / 024 | loss: 0.59363\n",
      "----- epoch 006 / 010 | time: 001 sec | loss: 0.63407 | err: 0.30533\n",
      "batch 001 / 024 | loss: 0.42694\n",
      "batch 002 / 024 | loss: 0.58234\n",
      "batch 003 / 024 | loss: 0.57802\n",
      "batch 004 / 024 | loss: 0.57818\n",
      "batch 005 / 024 | loss: 0.57953\n",
      "batch 006 / 024 | loss: 0.60111\n",
      "batch 007 / 024 | loss: 0.60089\n",
      "batch 008 / 024 | loss: 0.59689\n",
      "batch 009 / 024 | loss: 0.60115\n",
      "batch 010 / 024 | loss: 0.58859\n",
      "batch 011 / 024 | loss: 0.57994\n",
      "batch 012 / 024 | loss: 0.58739\n",
      "batch 013 / 024 | loss: 0.59587\n",
      "batch 014 / 024 | loss: 0.58917\n",
      "batch 015 / 024 | loss: 0.58675\n",
      "batch 016 / 024 | loss: 0.58951\n",
      "batch 017 / 024 | loss: 0.58489\n",
      "batch 018 / 024 | loss: 0.59286\n",
      "batch 019 / 024 | loss: 0.59934\n",
      "batch 020 / 024 | loss: 0.59528\n",
      "batch 021 / 024 | loss: 0.59741\n",
      "batch 022 / 024 | loss: 0.59212\n",
      "batch 023 / 024 | loss: 0.58884\n",
      "batch 024 / 024 | loss: 0.59177\n",
      "----- epoch 007 / 010 | time: 001 sec | loss: 0.62184 | err: 0.30533\n",
      "batch 001 / 024 | loss: 0.57394\n",
      "batch 002 / 024 | loss: 0.49330\n",
      "batch 003 / 024 | loss: 0.53094\n",
      "batch 004 / 024 | loss: 0.61420\n",
      "batch 005 / 024 | loss: 0.61134\n",
      "batch 006 / 024 | loss: 0.59565\n",
      "batch 007 / 024 | loss: 0.60573\n",
      "batch 008 / 024 | loss: 0.57440\n",
      "batch 009 / 024 | loss: 0.58416\n",
      "batch 010 / 024 | loss: 0.57738\n",
      "batch 011 / 024 | loss: 0.55514\n",
      "batch 012 / 024 | loss: 0.54090\n",
      "batch 013 / 024 | loss: 0.54360\n",
      "batch 014 / 024 | loss: 0.55738\n",
      "batch 015 / 024 | loss: 0.56478\n",
      "batch 016 / 024 | loss: 0.56492\n",
      "batch 017 / 024 | loss: 0.55810\n",
      "batch 018 / 024 | loss: 0.55542\n",
      "batch 019 / 024 | loss: 0.55941\n",
      "batch 020 / 024 | loss: 0.56672\n",
      "batch 021 / 024 | loss: 0.58140\n",
      "batch 022 / 024 | loss: 0.58562\n",
      "batch 023 / 024 | loss: 0.58564\n",
      "batch 024 / 024 | loss: 0.58555\n",
      "----- epoch 008 / 010 | time: 001 sec | loss: 0.63274 | err: 0.30667\n",
      "batch 001 / 024 | loss: 0.50497\n",
      "batch 002 / 024 | loss: 0.47403\n",
      "batch 003 / 024 | loss: 0.48082\n",
      "batch 004 / 024 | loss: 0.51765\n",
      "batch 005 / 024 | loss: 0.55781\n",
      "batch 006 / 024 | loss: 0.55354\n",
      "batch 007 / 024 | loss: 0.52615\n",
      "batch 008 / 024 | loss: 0.56577\n",
      "batch 009 / 024 | loss: 0.57246\n",
      "batch 010 / 024 | loss: 0.57499\n",
      "batch 011 / 024 | loss: 0.58184\n",
      "batch 012 / 024 | loss: 0.57669\n",
      "batch 013 / 024 | loss: 0.59426\n",
      "batch 014 / 024 | loss: 0.59574\n",
      "batch 015 / 024 | loss: 0.59070\n",
      "batch 016 / 024 | loss: 0.58518\n",
      "batch 017 / 024 | loss: 0.58598\n",
      "batch 018 / 024 | loss: 0.58459\n",
      "batch 019 / 024 | loss: 0.58636\n",
      "batch 020 / 024 | loss: 0.59518\n",
      "batch 021 / 024 | loss: 0.59897\n",
      "batch 022 / 024 | loss: 0.60037\n",
      "batch 023 / 024 | loss: 0.60450\n",
      "batch 024 / 024 | loss: 0.59551\n",
      "training time: 13.614269733428955 seconds\n",
      "tensor([0.0100, 0.0118, 0.0140, 0.0165, 0.0195, 0.0230, 0.0272, 0.0321, 0.0380,\n",
      "        0.0448, 0.0530, 0.0626, 0.0740, 0.0874, 0.1032, 0.1220, 0.1441, 0.1702,\n",
      "        0.2011, 0.2376, 0.2807, 0.3317, 0.3918, 0.4629, 0.5469, 0.6462, 0.7634,\n",
      "        0.9019, 1.0656, 1.2589])\n",
      "---------- training strategically----------\n",
      "lambda:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 4 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 5 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 6 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.78666\n",
      "batch 002 / 024 | loss: 0.88988\n",
      "batch 003 / 024 | loss: 0.88344\n",
      "batch 004 / 024 | loss: 0.89186\n",
      "batch 005 / 024 | loss: 0.92062\n",
      "batch 006 / 024 | loss: 0.92066\n",
      "batch 007 / 024 | loss: 0.91981\n",
      "batch 008 / 024 | loss: 0.90910\n",
      "batch 009 / 024 | loss: 0.90979\n",
      "batch 010 / 024 | loss: 0.90177\n",
      "batch 011 / 024 | loss: 0.87971\n",
      "batch 012 / 024 | loss: 0.87538\n",
      "batch 013 / 024 | loss: 0.86987\n",
      "batch 014 / 024 | loss: 0.86459\n",
      "batch 015 / 024 | loss: 0.85140\n",
      "batch 016 / 024 | loss: 0.84926\n",
      "batch 017 / 024 | loss: 0.84182\n",
      "batch 018 / 024 | loss: 0.84685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 019 / 024 | loss: 0.84885\n",
      "batch 020 / 024 | loss: 0.84202\n",
      "batch 021 / 024 | loss: 0.83783\n",
      "batch 022 / 024 | loss: 0.83886\n",
      "batch 023 / 024 | loss: 0.84018\n",
      "batch 024 / 024 | loss: 0.83781\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 195 sec | loss: 0.98128 | err: 0.33333\n",
      "batch 001 / 024 | loss: 0.78325\n",
      "batch 002 / 024 | loss: 0.82067\n",
      "batch 003 / 024 | loss: 0.80761\n",
      "batch 004 / 024 | loss: 0.79776\n",
      "batch 005 / 024 | loss: 0.81731\n",
      "batch 006 / 024 | loss: 0.83521\n",
      "batch 007 / 024 | loss: 0.81535\n",
      "batch 008 / 024 | loss: 0.79450\n",
      "batch 009 / 024 | loss: 0.79462\n",
      "batch 010 / 024 | loss: 0.79268\n",
      "batch 011 / 024 | loss: 0.79742\n",
      "batch 012 / 024 | loss: 0.79368\n",
      "batch 013 / 024 | loss: 0.79364\n",
      "batch 014 / 024 | loss: 0.79131\n",
      "batch 015 / 024 | loss: 0.78978\n",
      "batch 016 / 024 | loss: 0.79218\n",
      "batch 017 / 024 | loss: 0.79500\n",
      "batch 018 / 024 | loss: 0.79193\n",
      "batch 019 / 024 | loss: 0.79767\n",
      "batch 020 / 024 | loss: 0.79916\n",
      "batch 021 / 024 | loss: 0.80267\n",
      "batch 022 / 024 | loss: 0.80695\n",
      "batch 023 / 024 | loss: 0.80441\n",
      "batch 024 / 024 | loss: 0.80394\n",
      "----- epoch 002 / 010 | time: 202 sec | loss: 0.97971 | err: 0.35067\n",
      "batch 001 / 024 | loss: 0.78145\n",
      "batch 002 / 024 | loss: 0.74112\n",
      "batch 003 / 024 | loss: 0.73958\n",
      "batch 004 / 024 | loss: 0.74920\n",
      "batch 005 / 024 | loss: 0.76741\n",
      "batch 006 / 024 | loss: 0.77635\n",
      "batch 007 / 024 | loss: 0.79048\n",
      "batch 008 / 024 | loss: 0.77887\n",
      "batch 009 / 024 | loss: 0.78049\n",
      "batch 010 / 024 | loss: 0.78846\n",
      "batch 011 / 024 | loss: 0.78471\n",
      "batch 012 / 024 | loss: 0.78450\n",
      "batch 013 / 024 | loss: 0.78484\n",
      "batch 014 / 024 | loss: 0.77434\n",
      "batch 015 / 024 | loss: 0.75948\n",
      "batch 016 / 024 | loss: 0.76339\n",
      "batch 017 / 024 | loss: 0.76356\n",
      "batch 018 / 024 | loss: 0.77073\n",
      "batch 019 / 024 | loss: 0.76565\n",
      "batch 020 / 024 | loss: 0.77055\n",
      "batch 021 / 024 | loss: 0.77193\n",
      "batch 022 / 024 | loss: 0.78082\n",
      "batch 023 / 024 | loss: 0.77924\n",
      "batch 024 / 024 | loss: 0.77969\n",
      "----- epoch 003 / 010 | time: 194 sec | loss: 1.08041 | err: 0.33333\n",
      "batch 001 / 024 | loss: 0.66860\n",
      "batch 002 / 024 | loss: 0.76912\n",
      "batch 003 / 024 | loss: 0.75063\n",
      "batch 004 / 024 | loss: 0.79921\n",
      "batch 005 / 024 | loss: 0.79543\n",
      "batch 006 / 024 | loss: 0.78134\n",
      "batch 007 / 024 | loss: 0.78455\n",
      "batch 008 / 024 | loss: 0.77958\n",
      "batch 009 / 024 | loss: 0.76148\n",
      "batch 010 / 024 | loss: 0.77787\n",
      "batch 011 / 024 | loss: 0.78518\n",
      "batch 012 / 024 | loss: 0.78901\n",
      "batch 013 / 024 | loss: 0.78001\n",
      "batch 014 / 024 | loss: 0.78800\n",
      "batch 015 / 024 | loss: 0.79422\n",
      "batch 016 / 024 | loss: 0.80242\n",
      "batch 017 / 024 | loss: 0.80299\n",
      "batch 018 / 024 | loss: 0.80641\n",
      "batch 019 / 024 | loss: 0.80474\n",
      "batch 020 / 024 | loss: 0.80290\n",
      "batch 021 / 024 | loss: 0.81277\n",
      "batch 022 / 024 | loss: 0.80937\n",
      "batch 023 / 024 | loss: 0.80762\n",
      "batch 024 / 024 | loss: 0.81112\n",
      "model saved!\n",
      "----- epoch 004 / 010 | time: 202 sec | loss: 0.89937 | err: 0.32533\n",
      "batch 001 / 024 | loss: 0.90597\n",
      "batch 002 / 024 | loss: 0.83938\n",
      "batch 003 / 024 | loss: 0.85317\n",
      "batch 004 / 024 | loss: 0.77894\n",
      "batch 005 / 024 | loss: 0.78998\n",
      "batch 006 / 024 | loss: 0.77387\n",
      "batch 007 / 024 | loss: 0.78198\n",
      "batch 008 / 024 | loss: 0.78043\n",
      "batch 009 / 024 | loss: 0.79150\n",
      "batch 010 / 024 | loss: 0.77404\n",
      "batch 011 / 024 | loss: 0.77390\n",
      "batch 012 / 024 | loss: 0.78347\n",
      "batch 013 / 024 | loss: 0.76854\n",
      "batch 014 / 024 | loss: 0.76668\n",
      "batch 015 / 024 | loss: 0.77058\n",
      "batch 016 / 024 | loss: 0.77704\n",
      "batch 017 / 024 | loss: 0.77285\n",
      "batch 018 / 024 | loss: 0.77436\n",
      "batch 019 / 024 | loss: 0.77603\n",
      "batch 020 / 024 | loss: 0.78390\n",
      "batch 021 / 024 | loss: 0.78797\n",
      "batch 022 / 024 | loss: 0.79177\n",
      "batch 023 / 024 | loss: 0.79061\n",
      "batch 024 / 024 | loss: 0.78771\n",
      "----- epoch 005 / 010 | time: 197 sec | loss: 0.99967 | err: 0.35333\n",
      "batch 001 / 024 | loss: 0.81504\n",
      "batch 002 / 024 | loss: 0.76965\n",
      "batch 003 / 024 | loss: 0.78960\n",
      "batch 004 / 024 | loss: 0.79494\n",
      "batch 005 / 024 | loss: 0.80898\n",
      "batch 006 / 024 | loss: 0.83396\n",
      "batch 007 / 024 | loss: 0.83429\n",
      "batch 008 / 024 | loss: 0.85579\n",
      "batch 009 / 024 | loss: 0.85261\n",
      "batch 010 / 024 | loss: 0.84380\n",
      "batch 011 / 024 | loss: 0.84354\n",
      "batch 012 / 024 | loss: 0.84513\n",
      "batch 013 / 024 | loss: 0.85337\n",
      "batch 014 / 024 | loss: 0.85562\n",
      "batch 015 / 024 | loss: 0.85258\n",
      "batch 016 / 024 | loss: 0.85063\n",
      "batch 017 / 024 | loss: 0.84360\n",
      "batch 018 / 024 | loss: 0.84560\n",
      "batch 019 / 024 | loss: 0.84482\n",
      "batch 020 / 024 | loss: 0.82786\n",
      "batch 021 / 024 | loss: 0.82365\n",
      "batch 022 / 024 | loss: 0.82633\n",
      "batch 023 / 024 | loss: 0.81902\n",
      "batch 024 / 024 | loss: 0.81300\n",
      "----- epoch 006 / 010 | time: 188 sec | loss: 1.04164 | err: 0.32667\n",
      "batch 001 / 024 | loss: 0.60967\n",
      "batch 002 / 024 | loss: 0.67686\n",
      "batch 003 / 024 | loss: 0.70164\n",
      "batch 004 / 024 | loss: 0.70177\n",
      "batch 005 / 024 | loss: 0.73183\n",
      "batch 006 / 024 | loss: 0.77094\n",
      "batch 007 / 024 | loss: 0.77330\n",
      "batch 008 / 024 | loss: 0.77143\n",
      "batch 009 / 024 | loss: 0.78154\n",
      "batch 010 / 024 | loss: 0.77264\n",
      "batch 011 / 024 | loss: 0.75944\n",
      "batch 012 / 024 | loss: 0.77710\n",
      "batch 013 / 024 | loss: 0.78427\n",
      "batch 014 / 024 | loss: 0.77581\n",
      "batch 015 / 024 | loss: 0.77352\n",
      "batch 016 / 024 | loss: 0.78068\n",
      "batch 017 / 024 | loss: 0.77539\n",
      "batch 018 / 024 | loss: 0.77918\n",
      "batch 019 / 024 | loss: 0.78407\n",
      "batch 020 / 024 | loss: 0.78696\n",
      "batch 021 / 024 | loss: 0.78501\n",
      "batch 022 / 024 | loss: 0.78580\n",
      "batch 023 / 024 | loss: 0.78500\n",
      "batch 024 / 024 | loss: 0.78112\n",
      "----- epoch 007 / 010 | time: 190 sec | loss: 1.02163 | err: 0.33200\n",
      "batch 001 / 024 | loss: 0.83893\n",
      "batch 002 / 024 | loss: 0.76824\n",
      "batch 003 / 024 | loss: 0.81045\n",
      "batch 004 / 024 | loss: 0.81601\n",
      "batch 005 / 024 | loss: 0.78523\n",
      "batch 006 / 024 | loss: 0.78506\n",
      "batch 007 / 024 | loss: 0.77081\n",
      "batch 008 / 024 | loss: 0.77380\n",
      "batch 009 / 024 | loss: 0.78934\n",
      "batch 010 / 024 | loss: 0.78161\n",
      "batch 011 / 024 | loss: 0.76624\n",
      "batch 012 / 024 | loss: 0.75549\n",
      "batch 013 / 024 | loss: 0.75354\n",
      "batch 014 / 024 | loss: 0.76579\n",
      "batch 015 / 024 | loss: 0.76322\n",
      "batch 016 / 024 | loss: 0.76070\n",
      "batch 017 / 024 | loss: 0.75572\n",
      "batch 018 / 024 | loss: 0.76082\n",
      "batch 019 / 024 | loss: 0.76893\n",
      "batch 020 / 024 | loss: 0.77178\n",
      "batch 021 / 024 | loss: 0.77735\n",
      "batch 022 / 024 | loss: 0.78116\n",
      "batch 023 / 024 | loss: 0.77966\n",
      "batch 024 / 024 | loss: 0.78660\n",
      "----- epoch 008 / 010 | time: 186 sec | loss: 0.98837 | err: 0.33467\n",
      "batch 001 / 024 | loss: 0.82078\n",
      "batch 002 / 024 | loss: 0.80801\n",
      "batch 003 / 024 | loss: 0.80746\n",
      "batch 004 / 024 | loss: 0.81532\n",
      "batch 005 / 024 | loss: 0.81031\n",
      "batch 006 / 024 | loss: 0.81249\n",
      "batch 007 / 024 | loss: 0.78425\n",
      "batch 008 / 024 | loss: 0.78218\n",
      "batch 009 / 024 | loss: 0.80566\n",
      "batch 010 / 024 | loss: 0.79880\n",
      "batch 011 / 024 | loss: 0.79847\n",
      "batch 012 / 024 | loss: 0.79379\n",
      "batch 013 / 024 | loss: 0.79600\n",
      "batch 014 / 024 | loss: 0.79495\n",
      "batch 015 / 024 | loss: 0.79065\n",
      "batch 016 / 024 | loss: 0.78914\n",
      "batch 017 / 024 | loss: 0.78569\n",
      "batch 018 / 024 | loss: 0.78597\n",
      "batch 019 / 024 | loss: 0.78547\n",
      "batch 020 / 024 | loss: 0.78710\n",
      "batch 021 / 024 | loss: 0.78890\n",
      "batch 022 / 024 | loss: 0.79523\n",
      "batch 023 / 024 | loss: 0.79799\n",
      "batch 024 / 024 | loss: 0.79508\n",
      "training time: 1738.8089091777802 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.01181445773799895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 7 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 8 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 9 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.78750\n",
      "batch 002 / 024 | loss: 0.89100\n",
      "batch 003 / 024 | loss: 0.88427\n",
      "batch 004 / 024 | loss: 0.89250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 005 / 024 | loss: 0.92110\n",
      "batch 006 / 024 | loss: 0.92109\n",
      "batch 007 / 024 | loss: 0.92013\n",
      "batch 008 / 024 | loss: 0.90930\n",
      "batch 009 / 024 | loss: 0.91030\n",
      "batch 010 / 024 | loss: 0.90234\n",
      "batch 011 / 024 | loss: 0.87952\n",
      "batch 012 / 024 | loss: 0.87522\n",
      "batch 013 / 024 | loss: 0.86971\n",
      "batch 014 / 024 | loss: 0.86272\n",
      "batch 015 / 024 | loss: 0.85145\n",
      "batch 016 / 024 | loss: 0.84864\n",
      "batch 017 / 024 | loss: 0.84138\n",
      "batch 018 / 024 | loss: 0.84669\n",
      "batch 019 / 024 | loss: 0.84885\n",
      "batch 020 / 024 | loss: 0.84240\n",
      "batch 021 / 024 | loss: 0.83701\n",
      "batch 022 / 024 | loss: 0.83545\n",
      "batch 023 / 024 | loss: 0.83696\n",
      "batch 024 / 024 | loss: 0.83436\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 183 sec | loss: 0.99205 | err: 0.32133\n",
      "batch 001 / 024 | loss: 0.73918\n",
      "batch 002 / 024 | loss: 0.79287\n",
      "batch 003 / 024 | loss: 0.79510\n",
      "batch 004 / 024 | loss: 0.78464\n",
      "batch 005 / 024 | loss: 0.80677\n",
      "batch 006 / 024 | loss: 0.82847\n",
      "batch 007 / 024 | loss: 0.80988\n",
      "batch 008 / 024 | loss: 0.78683\n",
      "batch 009 / 024 | loss: 0.78505\n",
      "batch 010 / 024 | loss: 0.78226\n",
      "batch 011 / 024 | loss: 0.78706\n",
      "batch 012 / 024 | loss: 0.78110\n",
      "batch 013 / 024 | loss: 0.77610\n",
      "batch 014 / 024 | loss: 0.77231\n",
      "batch 015 / 024 | loss: 0.76560\n",
      "batch 016 / 024 | loss: 0.76932\n",
      "batch 017 / 024 | loss: 0.77062\n",
      "batch 018 / 024 | loss: 0.76794\n",
      "batch 019 / 024 | loss: 0.77508\n",
      "batch 020 / 024 | loss: 0.77799\n",
      "batch 021 / 024 | loss: 0.78359\n",
      "batch 022 / 024 | loss: 0.78792\n",
      "batch 023 / 024 | loss: 0.78760\n",
      "batch 024 / 024 | loss: 0.78831\n",
      "----- epoch 002 / 010 | time: 169 sec | loss: 0.97425 | err: 0.33467\n",
      "batch 001 / 024 | loss: 0.78158\n",
      "batch 002 / 024 | loss: 0.75309\n",
      "batch 003 / 024 | loss: 0.75003\n",
      "batch 004 / 024 | loss: 0.76172\n",
      "batch 005 / 024 | loss: 0.78767\n",
      "batch 006 / 024 | loss: 0.79302\n",
      "batch 007 / 024 | loss: 0.80472\n",
      "batch 008 / 024 | loss: 0.79247\n",
      "batch 009 / 024 | loss: 0.79465\n",
      "batch 010 / 024 | loss: 0.80609\n",
      "batch 011 / 024 | loss: 0.80684\n",
      "batch 012 / 024 | loss: 0.81253\n",
      "batch 013 / 024 | loss: 0.81834\n",
      "batch 014 / 024 | loss: 0.81638\n",
      "batch 015 / 024 | loss: 0.80461\n",
      "batch 016 / 024 | loss: 0.80575\n",
      "batch 017 / 024 | loss: 0.79898\n",
      "batch 018 / 024 | loss: 0.80012\n",
      "batch 019 / 024 | loss: 0.79234\n",
      "batch 020 / 024 | loss: 0.79462\n",
      "batch 021 / 024 | loss: 0.79021\n",
      "batch 022 / 024 | loss: 0.79861\n",
      "batch 023 / 024 | loss: 0.79516\n",
      "batch 024 / 024 | loss: 0.79403\n",
      "model saved!\n",
      "----- epoch 003 / 010 | time: 211 sec | loss: 1.32774 | err: 0.31200\n",
      "batch 001 / 024 | loss: 0.66356\n",
      "batch 002 / 024 | loss: 0.80724\n",
      "batch 003 / 024 | loss: 0.73746\n",
      "batch 004 / 024 | loss: 0.78002\n",
      "batch 005 / 024 | loss: 0.76563\n",
      "batch 006 / 024 | loss: 0.74867\n",
      "batch 007 / 024 | loss: 0.74586\n",
      "batch 008 / 024 | loss: 0.74100\n",
      "batch 009 / 024 | loss: 0.72506\n",
      "batch 010 / 024 | loss: 0.72687\n",
      "batch 011 / 024 | loss: 0.73936\n",
      "batch 012 / 024 | loss: 0.74364\n",
      "batch 013 / 024 | loss: 0.73065\n",
      "batch 014 / 024 | loss: 0.74183\n",
      "batch 015 / 024 | loss: 0.74577\n",
      "batch 016 / 024 | loss: 0.75376\n",
      "batch 017 / 024 | loss: 0.75777\n",
      "batch 018 / 024 | loss: 0.76215\n",
      "batch 019 / 024 | loss: 0.76273\n",
      "batch 020 / 024 | loss: 0.76489\n",
      "batch 021 / 024 | loss: 0.77642\n",
      "batch 022 / 024 | loss: 0.77321\n",
      "batch 023 / 024 | loss: 0.77284\n",
      "batch 024 / 024 | loss: 0.77754\n",
      "----- epoch 004 / 010 | time: 162 sec | loss: 1.04673 | err: 0.33200\n",
      "batch 001 / 024 | loss: 0.91569\n",
      "batch 002 / 024 | loss: 0.85101\n",
      "batch 003 / 024 | loss: 0.87226\n",
      "batch 004 / 024 | loss: 0.78956\n",
      "batch 005 / 024 | loss: 0.79735\n",
      "batch 006 / 024 | loss: 0.78487\n",
      "batch 007 / 024 | loss: 0.78626\n",
      "batch 008 / 024 | loss: 0.78220\n",
      "batch 009 / 024 | loss: 0.79306\n",
      "batch 010 / 024 | loss: 0.77545\n",
      "batch 011 / 024 | loss: 0.77160\n",
      "batch 012 / 024 | loss: 0.78226\n",
      "batch 013 / 024 | loss: 0.76799\n",
      "batch 014 / 024 | loss: 0.76632\n",
      "batch 015 / 024 | loss: 0.76350\n",
      "batch 016 / 024 | loss: 0.76469\n",
      "batch 017 / 024 | loss: 0.76165\n",
      "batch 018 / 024 | loss: 0.76190\n",
      "batch 019 / 024 | loss: 0.76597\n",
      "batch 020 / 024 | loss: 0.77474\n",
      "batch 021 / 024 | loss: 0.78088\n",
      "batch 022 / 024 | loss: 0.78797\n",
      "batch 023 / 024 | loss: 0.78762\n",
      "batch 024 / 024 | loss: 0.78604\n",
      "----- epoch 005 / 010 | time: 137 sec | loss: 0.87451 | err: 0.35200\n",
      "batch 001 / 024 | loss: 0.81862\n",
      "batch 002 / 024 | loss: 0.77068\n",
      "batch 003 / 024 | loss: 0.78585\n",
      "batch 004 / 024 | loss: 0.79002\n",
      "batch 005 / 024 | loss: 0.79075\n",
      "batch 006 / 024 | loss: 0.81164\n",
      "batch 007 / 024 | loss: 0.81639\n",
      "batch 008 / 024 | loss: 0.83329\n",
      "batch 009 / 024 | loss: 0.81736\n",
      "batch 010 / 024 | loss: 0.80170\n",
      "batch 011 / 024 | loss: 0.79808\n",
      "batch 012 / 024 | loss: 0.79579\n",
      "batch 013 / 024 | loss: 0.80599\n",
      "batch 014 / 024 | loss: 0.81330\n",
      "batch 015 / 024 | loss: 0.81204\n",
      "batch 016 / 024 | loss: 0.81351\n",
      "batch 017 / 024 | loss: 0.80967\n",
      "batch 018 / 024 | loss: 0.81957\n",
      "batch 019 / 024 | loss: 0.82088\n",
      "batch 020 / 024 | loss: 0.80496\n",
      "batch 021 / 024 | loss: 0.80483\n",
      "batch 022 / 024 | loss: 0.80650\n",
      "batch 023 / 024 | loss: 0.79896\n",
      "batch 024 / 024 | loss: 0.79834\n",
      "----- epoch 006 / 010 | time: 163 sec | loss: 1.08726 | err: 0.32800\n",
      "batch 001 / 024 | loss: 0.61478\n",
      "batch 002 / 024 | loss: 0.65421\n",
      "batch 003 / 024 | loss: 0.67772\n",
      "batch 004 / 024 | loss: 0.66865\n",
      "batch 005 / 024 | loss: 0.71318\n",
      "batch 006 / 024 | loss: 0.74590\n",
      "batch 007 / 024 | loss: 0.75175\n",
      "batch 008 / 024 | loss: 0.75649\n",
      "batch 009 / 024 | loss: 0.77215\n",
      "batch 010 / 024 | loss: 0.76370\n",
      "batch 011 / 024 | loss: 0.75277\n",
      "batch 012 / 024 | loss: 0.77089\n",
      "batch 013 / 024 | loss: 0.78180\n",
      "batch 014 / 024 | loss: 0.77369\n",
      "batch 015 / 024 | loss: 0.77509\n",
      "batch 016 / 024 | loss: 0.78300\n",
      "batch 017 / 024 | loss: 0.77749\n",
      "batch 018 / 024 | loss: 0.78147\n",
      "batch 019 / 024 | loss: 0.78757\n",
      "batch 020 / 024 | loss: 0.78983\n",
      "batch 021 / 024 | loss: 0.78750\n",
      "batch 022 / 024 | loss: 0.78928\n",
      "batch 023 / 024 | loss: 0.78731\n",
      "batch 024 / 024 | loss: 0.78426\n",
      "----- epoch 007 / 010 | time: 146 sec | loss: 1.00764 | err: 0.33600\n",
      "batch 001 / 024 | loss: 0.84231\n",
      "batch 002 / 024 | loss: 0.78725\n",
      "batch 003 / 024 | loss: 0.82905\n",
      "batch 004 / 024 | loss: 0.84935\n",
      "batch 005 / 024 | loss: 0.84182\n",
      "batch 006 / 024 | loss: 0.83977\n",
      "batch 007 / 024 | loss: 0.82429\n",
      "batch 008 / 024 | loss: 0.82222\n",
      "batch 009 / 024 | loss: 0.83397\n",
      "batch 010 / 024 | loss: 0.82336\n",
      "batch 011 / 024 | loss: 0.80580\n",
      "batch 012 / 024 | loss: 0.79308\n",
      "batch 013 / 024 | loss: 0.78651\n",
      "batch 014 / 024 | loss: 0.79671\n",
      "batch 015 / 024 | loss: 0.79244\n",
      "batch 016 / 024 | loss: 0.78815\n",
      "batch 017 / 024 | loss: 0.78149\n",
      "batch 018 / 024 | loss: 0.78624\n",
      "batch 019 / 024 | loss: 0.79465\n",
      "batch 020 / 024 | loss: 0.79666\n",
      "batch 021 / 024 | loss: 0.80192\n",
      "batch 022 / 024 | loss: 0.80531\n",
      "batch 023 / 024 | loss: 0.80349\n",
      "batch 024 / 024 | loss: 0.80904\n",
      "training time: 1362.4485805034637 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.01395814116429633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 10 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 11 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 12 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.78848\n",
      "batch 002 / 024 | loss: 0.89233\n",
      "batch 003 / 024 | loss: 0.88525\n",
      "batch 004 / 024 | loss: 0.89327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 005 / 024 | loss: 0.92181\n",
      "batch 006 / 024 | loss: 0.92184\n",
      "batch 007 / 024 | loss: 0.92091\n",
      "batch 008 / 024 | loss: 0.91017\n",
      "batch 009 / 024 | loss: 0.91141\n",
      "batch 010 / 024 | loss: 0.90356\n",
      "batch 011 / 024 | loss: 0.88143\n",
      "batch 012 / 024 | loss: 0.87713\n",
      "batch 013 / 024 | loss: 0.87162\n",
      "batch 014 / 024 | loss: 0.86557\n",
      "batch 015 / 024 | loss: 0.85409\n",
      "batch 016 / 024 | loss: 0.85215\n",
      "batch 017 / 024 | loss: 0.84636\n",
      "batch 018 / 024 | loss: 0.85126\n",
      "batch 019 / 024 | loss: 0.85317\n",
      "batch 020 / 024 | loss: 0.84575\n",
      "batch 021 / 024 | loss: 0.84286\n",
      "batch 022 / 024 | loss: 0.84406\n",
      "batch 023 / 024 | loss: 0.84683\n",
      "batch 024 / 024 | loss: 0.84419\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 186 sec | loss: 0.96218 | err: 0.34000\n",
      "batch 001 / 024 | loss: 0.80971\n",
      "batch 002 / 024 | loss: 0.83694\n",
      "batch 003 / 024 | loss: 0.81156\n",
      "batch 004 / 024 | loss: 0.80165\n",
      "batch 005 / 024 | loss: 0.82054\n",
      "batch 006 / 024 | loss: 0.83962\n",
      "batch 007 / 024 | loss: 0.82258\n",
      "batch 008 / 024 | loss: 0.79820\n",
      "batch 009 / 024 | loss: 0.79790\n",
      "batch 010 / 024 | loss: 0.79596\n",
      "batch 011 / 024 | loss: 0.80067\n",
      "batch 012 / 024 | loss: 0.80071\n",
      "batch 013 / 024 | loss: 0.80072\n",
      "batch 014 / 024 | loss: 0.79798\n",
      "batch 015 / 024 | loss: 0.79751\n",
      "batch 016 / 024 | loss: 0.80075\n",
      "batch 017 / 024 | loss: 0.80208\n",
      "batch 018 / 024 | loss: 0.79898\n",
      "batch 019 / 024 | loss: 0.80515\n",
      "batch 020 / 024 | loss: 0.80559\n",
      "batch 021 / 024 | loss: 0.81020\n",
      "batch 022 / 024 | loss: 0.81370\n",
      "batch 023 / 024 | loss: 0.81256\n",
      "batch 024 / 024 | loss: 0.81170\n",
      "----- epoch 002 / 010 | time: 195 sec | loss: 1.01081 | err: 0.35467\n",
      "batch 001 / 024 | loss: 0.78603\n",
      "batch 002 / 024 | loss: 0.76203\n",
      "batch 003 / 024 | loss: 0.77555\n",
      "batch 004 / 024 | loss: 0.78464\n",
      "batch 005 / 024 | loss: 0.81338\n",
      "batch 006 / 024 | loss: 0.81598\n",
      "batch 007 / 024 | loss: 0.83039\n",
      "batch 008 / 024 | loss: 0.81697\n",
      "batch 009 / 024 | loss: 0.81600\n",
      "batch 010 / 024 | loss: 0.82118\n",
      "batch 011 / 024 | loss: 0.81593\n",
      "batch 012 / 024 | loss: 0.81474\n",
      "batch 013 / 024 | loss: 0.81176\n",
      "batch 014 / 024 | loss: 0.79938\n",
      "batch 015 / 024 | loss: 0.78318\n",
      "batch 016 / 024 | loss: 0.78592\n",
      "batch 017 / 024 | loss: 0.78487\n",
      "batch 018 / 024 | loss: 0.79084\n",
      "batch 019 / 024 | loss: 0.78445\n",
      "batch 020 / 024 | loss: 0.78657\n",
      "batch 021 / 024 | loss: 0.78656\n",
      "batch 022 / 024 | loss: 0.79570\n",
      "batch 023 / 024 | loss: 0.79409\n",
      "batch 024 / 024 | loss: 0.79377\n",
      "model saved!\n",
      "----- epoch 003 / 010 | time: 185 sec | loss: 1.03391 | err: 0.33600\n",
      "batch 001 / 024 | loss: 0.66507\n",
      "batch 002 / 024 | loss: 0.78921\n",
      "batch 003 / 024 | loss: 0.77141\n",
      "batch 004 / 024 | loss: 0.83273\n",
      "batch 005 / 024 | loss: 0.82240\n",
      "batch 006 / 024 | loss: 0.80349\n",
      "batch 007 / 024 | loss: 0.80451\n",
      "batch 008 / 024 | loss: 0.80088\n",
      "batch 009 / 024 | loss: 0.78229\n",
      "batch 010 / 024 | loss: 0.79703\n",
      "batch 011 / 024 | loss: 0.80620\n",
      "batch 012 / 024 | loss: 0.81024\n",
      "batch 013 / 024 | loss: 0.79944\n",
      "batch 014 / 024 | loss: 0.80611\n",
      "batch 015 / 024 | loss: 0.80781\n",
      "batch 016 / 024 | loss: 0.81307\n",
      "batch 017 / 024 | loss: 0.81126\n",
      "batch 018 / 024 | loss: 0.81339\n",
      "batch 019 / 024 | loss: 0.81161\n",
      "batch 020 / 024 | loss: 0.80965\n",
      "batch 021 / 024 | loss: 0.81936\n",
      "batch 022 / 024 | loss: 0.81422\n",
      "batch 023 / 024 | loss: 0.81054\n",
      "batch 024 / 024 | loss: 0.81402\n",
      "model saved!\n",
      "----- epoch 004 / 010 | time: 178 sec | loss: 1.06948 | err: 0.33200\n",
      "batch 001 / 024 | loss: 0.92897\n",
      "batch 002 / 024 | loss: 0.84616\n",
      "batch 003 / 024 | loss: 0.86770\n",
      "batch 004 / 024 | loss: 0.78744\n",
      "batch 005 / 024 | loss: 0.79430\n",
      "batch 006 / 024 | loss: 0.77840\n",
      "batch 007 / 024 | loss: 0.78013\n",
      "batch 008 / 024 | loss: 0.77682\n",
      "batch 009 / 024 | loss: 0.78831\n",
      "batch 010 / 024 | loss: 0.77087\n",
      "batch 011 / 024 | loss: 0.77105\n",
      "batch 012 / 024 | loss: 0.78357\n",
      "batch 013 / 024 | loss: 0.76924\n",
      "batch 014 / 024 | loss: 0.77044\n",
      "batch 015 / 024 | loss: 0.77400\n",
      "batch 016 / 024 | loss: 0.77959\n",
      "batch 017 / 024 | loss: 0.77839\n",
      "batch 018 / 024 | loss: 0.78171\n",
      "batch 019 / 024 | loss: 0.78527\n",
      "batch 020 / 024 | loss: 0.79329\n",
      "batch 021 / 024 | loss: 0.79930\n",
      "batch 022 / 024 | loss: 0.80479\n",
      "batch 023 / 024 | loss: 0.80280\n",
      "batch 024 / 024 | loss: 0.79983\n",
      "----- epoch 005 / 010 | time: 179 sec | loss: 0.96277 | err: 0.35333\n",
      "batch 001 / 024 | loss: 0.80688\n",
      "batch 002 / 024 | loss: 0.76979\n",
      "batch 003 / 024 | loss: 0.79038\n",
      "batch 004 / 024 | loss: 0.80116\n",
      "batch 005 / 024 | loss: 0.81150\n",
      "batch 006 / 024 | loss: 0.83602\n",
      "batch 007 / 024 | loss: 0.83817\n",
      "batch 008 / 024 | loss: 0.85877\n",
      "batch 009 / 024 | loss: 0.85342\n",
      "batch 010 / 024 | loss: 0.84588\n",
      "batch 011 / 024 | loss: 0.84387\n",
      "batch 012 / 024 | loss: 0.84582\n",
      "batch 013 / 024 | loss: 0.85400\n",
      "batch 014 / 024 | loss: 0.85710\n",
      "batch 015 / 024 | loss: 0.85337\n",
      "batch 016 / 024 | loss: 0.85165\n",
      "batch 017 / 024 | loss: 0.84651\n",
      "batch 018 / 024 | loss: 0.85049\n",
      "batch 019 / 024 | loss: 0.84983\n",
      "batch 020 / 024 | loss: 0.83274\n",
      "batch 021 / 024 | loss: 0.82868\n",
      "batch 022 / 024 | loss: 0.83041\n",
      "batch 023 / 024 | loss: 0.82238\n",
      "batch 024 / 024 | loss: 0.82006\n",
      "model saved!\n",
      "----- epoch 006 / 010 | time: 163 sec | loss: 1.10039 | err: 0.32400\n",
      "batch 001 / 024 | loss: 0.64021\n",
      "batch 002 / 024 | loss: 0.67584\n",
      "batch 003 / 024 | loss: 0.69985\n",
      "batch 004 / 024 | loss: 0.68525\n",
      "batch 005 / 024 | loss: 0.73516\n",
      "batch 006 / 024 | loss: 0.77012\n",
      "batch 007 / 024 | loss: 0.77244\n",
      "batch 008 / 024 | loss: 0.77668\n",
      "batch 009 / 024 | loss: 0.78728\n",
      "batch 010 / 024 | loss: 0.77650\n",
      "batch 011 / 024 | loss: 0.76468\n",
      "batch 012 / 024 | loss: 0.78017\n",
      "batch 013 / 024 | loss: 0.78621\n",
      "batch 014 / 024 | loss: 0.77725\n",
      "batch 015 / 024 | loss: 0.77400\n",
      "batch 016 / 024 | loss: 0.78155\n",
      "batch 017 / 024 | loss: 0.77600\n",
      "batch 018 / 024 | loss: 0.78051\n",
      "batch 019 / 024 | loss: 0.78745\n",
      "batch 020 / 024 | loss: 0.78973\n",
      "batch 021 / 024 | loss: 0.78629\n",
      "batch 022 / 024 | loss: 0.78660\n",
      "batch 023 / 024 | loss: 0.78455\n",
      "batch 024 / 024 | loss: 0.77994\n",
      "----- epoch 007 / 010 | time: 151 sec | loss: 1.11827 | err: 0.33067\n",
      "batch 001 / 024 | loss: 0.82376\n",
      "batch 002 / 024 | loss: 0.76198\n",
      "batch 003 / 024 | loss: 0.80756\n",
      "batch 004 / 024 | loss: 0.82606\n",
      "batch 005 / 024 | loss: 0.80960\n",
      "batch 006 / 024 | loss: 0.81329\n",
      "batch 007 / 024 | loss: 0.79862\n",
      "batch 008 / 024 | loss: 0.80028\n",
      "batch 009 / 024 | loss: 0.81333\n",
      "batch 010 / 024 | loss: 0.80648\n",
      "batch 011 / 024 | loss: 0.79161\n",
      "batch 012 / 024 | loss: 0.77937\n",
      "batch 013 / 024 | loss: 0.77261\n",
      "batch 014 / 024 | loss: 0.78261\n",
      "batch 015 / 024 | loss: 0.77982\n",
      "batch 016 / 024 | loss: 0.77686\n",
      "batch 017 / 024 | loss: 0.77166\n",
      "batch 018 / 024 | loss: 0.77611\n",
      "batch 019 / 024 | loss: 0.78363\n",
      "batch 020 / 024 | loss: 0.78429\n",
      "batch 021 / 024 | loss: 0.78872\n",
      "batch 022 / 024 | loss: 0.79252\n",
      "batch 023 / 024 | loss: 0.79007\n",
      "batch 024 / 024 | loss: 0.79634\n",
      "----- epoch 008 / 010 | time: 142 sec | loss: 0.98218 | err: 0.33600\n",
      "batch 001 / 024 | loss: 0.82673\n",
      "batch 002 / 024 | loss: 0.81751\n",
      "batch 003 / 024 | loss: 0.80358\n",
      "batch 004 / 024 | loss: 0.82271\n",
      "batch 005 / 024 | loss: 0.81450\n",
      "batch 006 / 024 | loss: 0.82262\n",
      "batch 007 / 024 | loss: 0.79459\n",
      "batch 008 / 024 | loss: 0.79167\n",
      "batch 009 / 024 | loss: 0.81543\n",
      "batch 010 / 024 | loss: 0.80760\n",
      "batch 011 / 024 | loss: 0.80783\n",
      "batch 012 / 024 | loss: 0.80332\n",
      "batch 013 / 024 | loss: 0.80376\n",
      "batch 014 / 024 | loss: 0.80110\n",
      "batch 015 / 024 | loss: 0.79540\n",
      "batch 016 / 024 | loss: 0.79375\n",
      "batch 017 / 024 | loss: 0.78765\n",
      "batch 018 / 024 | loss: 0.78805\n",
      "batch 019 / 024 | loss: 0.78672\n",
      "batch 020 / 024 | loss: 0.78812\n",
      "batch 021 / 024 | loss: 0.79087\n",
      "batch 022 / 024 | loss: 0.79785\n",
      "batch 023 / 024 | loss: 0.79985\n",
      "batch 024 / 024 | loss: 0.79684\n",
      "----- epoch 009 / 010 | time: 138 sec | loss: 0.93598 | err: 0.34400\n",
      "batch 001 / 024 | loss: 0.69162\n",
      "batch 002 / 024 | loss: 0.80296\n",
      "batch 003 / 024 | loss: 0.82490\n",
      "batch 004 / 024 | loss: 0.82698\n",
      "batch 005 / 024 | loss: 0.81810\n",
      "batch 006 / 024 | loss: 0.82252\n",
      "batch 007 / 024 | loss: 0.80445\n",
      "batch 008 / 024 | loss: 0.81527\n",
      "batch 009 / 024 | loss: 0.81037\n",
      "batch 010 / 024 | loss: 0.81437\n",
      "batch 011 / 024 | loss: 0.81565\n",
      "batch 012 / 024 | loss: 0.80132\n",
      "batch 013 / 024 | loss: 0.79331\n",
      "batch 014 / 024 | loss: 0.79218\n",
      "batch 015 / 024 | loss: 0.80080\n",
      "batch 016 / 024 | loss: 0.79798\n",
      "batch 017 / 024 | loss: 0.79293\n",
      "batch 018 / 024 | loss: 0.78297\n",
      "batch 019 / 024 | loss: 0.78168\n",
      "batch 020 / 024 | loss: 0.78291\n",
      "batch 021 / 024 | loss: 0.78891\n",
      "batch 022 / 024 | loss: 0.78663\n",
      "batch 023 / 024 | loss: 0.78974\n",
      "batch 024 / 024 | loss: 0.79804\n",
      "----- epoch 010 / 010 | time: 136 sec | loss: 0.92226 | err: 0.33600\n",
      "training time: 1658.451768398285 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.016490786888660246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 13 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 14 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 15 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.78965\n",
      "batch 002 / 024 | loss: 0.89391\n",
      "batch 003 / 024 | loss: 0.88640\n",
      "batch 004 / 024 | loss: 0.89417\n",
      "batch 005 / 024 | loss: 0.92263\n",
      "batch 006 / 024 | loss: 0.92271\n",
      "batch 007 / 024 | loss: 0.92182\n",
      "batch 008 / 024 | loss: 0.91119\n",
      "batch 009 / 024 | loss: 0.91275\n",
      "batch 010 / 024 | loss: 0.90503\n",
      "batch 011 / 024 | loss: 0.88310\n",
      "batch 012 / 024 | loss: 0.87920\n",
      "batch 013 / 024 | loss: 0.87368\n",
      "batch 014 / 024 | loss: 0.86688\n",
      "batch 015 / 024 | loss: 0.85510\n",
      "batch 016 / 024 | loss: 0.85327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 017 / 024 | loss: 0.84644\n",
      "batch 018 / 024 | loss: 0.85160\n",
      "batch 019 / 024 | loss: 0.85330\n",
      "batch 020 / 024 | loss: 0.84633\n",
      "batch 021 / 024 | loss: 0.84283\n",
      "batch 022 / 024 | loss: 0.84415\n",
      "batch 023 / 024 | loss: 0.84600\n",
      "batch 024 / 024 | loss: 0.84326\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 141 sec | loss: 1.06693 | err: 0.33733\n",
      "batch 001 / 024 | loss: 0.76471\n",
      "batch 002 / 024 | loss: 0.81451\n",
      "batch 003 / 024 | loss: 0.81256\n",
      "batch 004 / 024 | loss: 0.80304\n",
      "batch 005 / 024 | loss: 0.82483\n",
      "batch 006 / 024 | loss: 0.84538\n",
      "batch 007 / 024 | loss: 0.82755\n",
      "batch 008 / 024 | loss: 0.80267\n",
      "batch 009 / 024 | loss: 0.80346\n",
      "batch 010 / 024 | loss: 0.80151\n",
      "batch 011 / 024 | loss: 0.80531\n",
      "batch 012 / 024 | loss: 0.80106\n",
      "batch 013 / 024 | loss: 0.80040\n",
      "batch 014 / 024 | loss: 0.79659\n",
      "batch 015 / 024 | loss: 0.79210\n",
      "batch 016 / 024 | loss: 0.79502\n",
      "batch 017 / 024 | loss: 0.79498\n",
      "batch 018 / 024 | loss: 0.79116\n",
      "batch 019 / 024 | loss: 0.79755\n",
      "batch 020 / 024 | loss: 0.79831\n",
      "batch 021 / 024 | loss: 0.80206\n",
      "batch 022 / 024 | loss: 0.80580\n",
      "batch 023 / 024 | loss: 0.80032\n",
      "batch 024 / 024 | loss: 0.79994\n",
      "----- epoch 002 / 010 | time: 146 sec | loss: 1.03809 | err: 0.34000\n",
      "batch 001 / 024 | loss: 0.77361\n",
      "batch 002 / 024 | loss: 0.74483\n",
      "batch 003 / 024 | loss: 0.73828\n",
      "batch 004 / 024 | loss: 0.74837\n",
      "batch 005 / 024 | loss: 0.76339\n",
      "batch 006 / 024 | loss: 0.77286\n",
      "batch 007 / 024 | loss: 0.79032\n",
      "batch 008 / 024 | loss: 0.78671\n",
      "batch 009 / 024 | loss: 0.78962\n",
      "batch 010 / 024 | loss: 0.80287\n",
      "batch 011 / 024 | loss: 0.80356\n",
      "batch 012 / 024 | loss: 0.81098\n",
      "batch 013 / 024 | loss: 0.81350\n",
      "batch 014 / 024 | loss: 0.80715\n",
      "batch 015 / 024 | loss: 0.79341\n",
      "batch 016 / 024 | loss: 0.79744\n",
      "batch 017 / 024 | loss: 0.79246\n",
      "batch 018 / 024 | loss: 0.79855\n",
      "batch 019 / 024 | loss: 0.79201\n",
      "batch 020 / 024 | loss: 0.79391\n",
      "batch 021 / 024 | loss: 0.79431\n",
      "batch 022 / 024 | loss: 0.80380\n",
      "batch 023 / 024 | loss: 0.80299\n",
      "batch 024 / 024 | loss: 0.80243\n",
      "----- epoch 003 / 010 | time: 192 sec | loss: 0.90905 | err: 0.35200\n",
      "batch 001 / 024 | loss: 0.69544\n",
      "batch 002 / 024 | loss: 0.83455\n",
      "batch 003 / 024 | loss: 0.80483\n",
      "batch 004 / 024 | loss: 0.82083\n",
      "batch 005 / 024 | loss: 0.80917\n",
      "batch 006 / 024 | loss: 0.78900\n",
      "batch 007 / 024 | loss: 0.77611\n",
      "batch 008 / 024 | loss: 0.77315\n",
      "batch 009 / 024 | loss: 0.74907\n",
      "batch 010 / 024 | loss: 0.75712\n",
      "batch 011 / 024 | loss: 0.76838\n",
      "batch 012 / 024 | loss: 0.77098\n",
      "batch 013 / 024 | loss: 0.76403\n",
      "batch 014 / 024 | loss: 0.77287\n",
      "batch 015 / 024 | loss: 0.77815\n",
      "batch 016 / 024 | loss: 0.78886\n",
      "batch 017 / 024 | loss: 0.79066\n",
      "batch 018 / 024 | loss: 0.79514\n",
      "batch 019 / 024 | loss: 0.79490\n",
      "batch 020 / 024 | loss: 0.79617\n",
      "batch 021 / 024 | loss: 0.80632\n",
      "batch 022 / 024 | loss: 0.80319\n",
      "batch 023 / 024 | loss: 0.80048\n",
      "batch 024 / 024 | loss: 0.80452\n",
      "model saved!\n",
      "----- epoch 004 / 010 | time: 153 sec | loss: 0.99962 | err: 0.32533\n",
      "batch 001 / 024 | loss: 0.91398\n",
      "batch 002 / 024 | loss: 0.86875\n",
      "batch 003 / 024 | loss: 0.90539\n",
      "batch 004 / 024 | loss: 0.82437\n",
      "batch 005 / 024 | loss: 0.81070\n",
      "batch 006 / 024 | loss: 0.78472\n",
      "batch 007 / 024 | loss: 0.78977\n",
      "batch 008 / 024 | loss: 0.78302\n",
      "batch 009 / 024 | loss: 0.79428\n",
      "batch 010 / 024 | loss: 0.77675\n",
      "batch 011 / 024 | loss: 0.77537\n",
      "batch 012 / 024 | loss: 0.78557\n",
      "batch 013 / 024 | loss: 0.77102\n",
      "batch 014 / 024 | loss: 0.76815\n",
      "batch 015 / 024 | loss: 0.76857\n",
      "batch 016 / 024 | loss: 0.77465\n",
      "batch 017 / 024 | loss: 0.77197\n",
      "batch 018 / 024 | loss: 0.77457\n",
      "batch 019 / 024 | loss: 0.77765\n",
      "batch 020 / 024 | loss: 0.78606\n",
      "batch 021 / 024 | loss: 0.79103\n",
      "batch 022 / 024 | loss: 0.79536\n",
      "batch 023 / 024 | loss: 0.79379\n",
      "batch 024 / 024 | loss: 0.79053\n",
      "----- epoch 005 / 010 | time: 197 sec | loss: 1.02451 | err: 0.35200\n",
      "batch 001 / 024 | loss: 0.83077\n",
      "batch 002 / 024 | loss: 0.78186\n",
      "batch 003 / 024 | loss: 0.81000\n",
      "batch 004 / 024 | loss: 0.80790\n",
      "batch 005 / 024 | loss: 0.82391\n",
      "batch 006 / 024 | loss: 0.84806\n",
      "batch 007 / 024 | loss: 0.84554\n",
      "batch 008 / 024 | loss: 0.86472\n",
      "batch 009 / 024 | loss: 0.86050\n",
      "batch 010 / 024 | loss: 0.85217\n",
      "batch 011 / 024 | loss: 0.84978\n",
      "batch 012 / 024 | loss: 0.85225\n",
      "batch 013 / 024 | loss: 0.86067\n",
      "batch 014 / 024 | loss: 0.86218\n",
      "batch 015 / 024 | loss: 0.85915\n",
      "batch 016 / 024 | loss: 0.85711\n",
      "batch 017 / 024 | loss: 0.85029\n",
      "batch 018 / 024 | loss: 0.85317\n",
      "batch 019 / 024 | loss: 0.85252\n",
      "batch 020 / 024 | loss: 0.83490\n",
      "batch 021 / 024 | loss: 0.83074\n",
      "batch 022 / 024 | loss: 0.83181\n",
      "batch 023 / 024 | loss: 0.82338\n",
      "batch 024 / 024 | loss: 0.82117\n",
      "----- epoch 006 / 010 | time: 146 sec | loss: 1.07165 | err: 0.33067\n",
      "batch 001 / 024 | loss: 0.63973\n",
      "batch 002 / 024 | loss: 0.67214\n",
      "batch 003 / 024 | loss: 0.71012\n",
      "batch 004 / 024 | loss: 0.69855\n",
      "batch 005 / 024 | loss: 0.74749\n",
      "batch 006 / 024 | loss: 0.77453\n",
      "batch 007 / 024 | loss: 0.77623\n",
      "batch 008 / 024 | loss: 0.77989\n",
      "batch 009 / 024 | loss: 0.79039\n",
      "batch 010 / 024 | loss: 0.77960\n",
      "batch 011 / 024 | loss: 0.76438\n",
      "batch 012 / 024 | loss: 0.78179\n",
      "batch 013 / 024 | loss: 0.78789\n",
      "batch 014 / 024 | loss: 0.77994\n",
      "batch 015 / 024 | loss: 0.77646\n",
      "batch 016 / 024 | loss: 0.78376\n",
      "batch 017 / 024 | loss: 0.77796\n",
      "batch 018 / 024 | loss: 0.78269\n",
      "batch 019 / 024 | loss: 0.78979\n",
      "batch 020 / 024 | loss: 0.79237\n",
      "batch 021 / 024 | loss: 0.78910\n",
      "batch 022 / 024 | loss: 0.78911\n",
      "batch 023 / 024 | loss: 0.78713\n",
      "batch 024 / 024 | loss: 0.78235\n",
      "----- epoch 007 / 010 | time: 151 sec | loss: 1.07419 | err: 0.33200\n",
      "batch 001 / 024 | loss: 0.82363\n",
      "batch 002 / 024 | loss: 0.76969\n",
      "batch 003 / 024 | loss: 0.81237\n",
      "batch 004 / 024 | loss: 0.82480\n",
      "batch 005 / 024 | loss: 0.80531\n",
      "batch 006 / 024 | loss: 0.80204\n",
      "batch 007 / 024 | loss: 0.78603\n",
      "batch 008 / 024 | loss: 0.78898\n",
      "batch 009 / 024 | loss: 0.80267\n",
      "batch 010 / 024 | loss: 0.79675\n",
      "batch 011 / 024 | loss: 0.78067\n",
      "batch 012 / 024 | loss: 0.76938\n",
      "batch 013 / 024 | loss: 0.76618\n",
      "batch 014 / 024 | loss: 0.77773\n",
      "batch 015 / 024 | loss: 0.77584\n",
      "batch 016 / 024 | loss: 0.77153\n",
      "batch 017 / 024 | loss: 0.76614\n",
      "batch 018 / 024 | loss: 0.77213\n",
      "batch 019 / 024 | loss: 0.78034\n",
      "batch 020 / 024 | loss: 0.78244\n",
      "batch 021 / 024 | loss: 0.78881\n",
      "batch 022 / 024 | loss: 0.79313\n",
      "batch 023 / 024 | loss: 0.79261\n",
      "batch 024 / 024 | loss: 0.79915\n",
      "----- epoch 008 / 010 | time: 147 sec | loss: 0.94209 | err: 0.34400\n",
      "batch 001 / 024 | loss: 0.88195\n",
      "batch 002 / 024 | loss: 0.86288\n",
      "batch 003 / 024 | loss: 0.85722\n",
      "batch 004 / 024 | loss: 0.86100\n",
      "batch 005 / 024 | loss: 0.84887\n",
      "batch 006 / 024 | loss: 0.84676\n",
      "batch 007 / 024 | loss: 0.81150\n",
      "batch 008 / 024 | loss: 0.80674\n",
      "batch 009 / 024 | loss: 0.83207\n",
      "batch 010 / 024 | loss: 0.82373\n",
      "batch 011 / 024 | loss: 0.81762\n",
      "batch 012 / 024 | loss: 0.81247\n",
      "batch 013 / 024 | loss: 0.81388\n",
      "batch 014 / 024 | loss: 0.81324\n",
      "batch 015 / 024 | loss: 0.80688\n",
      "batch 016 / 024 | loss: 0.79902\n",
      "batch 017 / 024 | loss: 0.79276\n",
      "batch 018 / 024 | loss: 0.79045\n",
      "batch 019 / 024 | loss: 0.79092\n",
      "batch 020 / 024 | loss: 0.79440\n",
      "batch 021 / 024 | loss: 0.79590\n",
      "batch 022 / 024 | loss: 0.80263\n",
      "batch 023 / 024 | loss: 0.80422\n",
      "batch 024 / 024 | loss: 0.80189\n",
      "training time: 1453.6507940292358 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.01948297047624237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 16 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 17 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 18 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.79102\n",
      "batch 002 / 024 | loss: 0.89576\n",
      "batch 003 / 024 | loss: 0.88776\n",
      "batch 004 / 024 | loss: 0.89522\n",
      "batch 005 / 024 | loss: 0.92360\n",
      "batch 006 / 024 | loss: 0.92374\n",
      "batch 007 / 024 | loss: 0.92288\n",
      "batch 008 / 024 | loss: 0.91237\n",
      "batch 009 / 024 | loss: 0.91443\n",
      "batch 010 / 024 | loss: 0.90683\n",
      "batch 011 / 024 | loss: 0.88485\n",
      "batch 012 / 024 | loss: 0.88252\n",
      "batch 013 / 024 | loss: 0.87711\n",
      "batch 014 / 024 | loss: 0.87174\n",
      "batch 015 / 024 | loss: 0.85855\n",
      "batch 016 / 024 | loss: 0.85670\n",
      "batch 017 / 024 | loss: 0.84919\n",
      "batch 018 / 024 | loss: 0.85471\n",
      "batch 019 / 024 | loss: 0.85676\n",
      "batch 020 / 024 | loss: 0.84939\n",
      "batch 021 / 024 | loss: 0.84440\n",
      "batch 022 / 024 | loss: 0.84582\n",
      "batch 023 / 024 | loss: 0.84895\n",
      "batch 024 / 024 | loss: 0.84622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved!\n",
      "----- epoch 001 / 010 | time: 205 sec | loss: 1.02498 | err: 0.34667\n",
      "batch 001 / 024 | loss: 0.79283\n",
      "batch 002 / 024 | loss: 0.83463\n",
      "batch 003 / 024 | loss: 0.82502\n",
      "batch 004 / 024 | loss: 0.81229\n",
      "batch 005 / 024 | loss: 0.83502\n",
      "batch 006 / 024 | loss: 0.85468\n",
      "batch 007 / 024 | loss: 0.83543\n",
      "batch 008 / 024 | loss: 0.81002\n",
      "batch 009 / 024 | loss: 0.80589\n",
      "batch 010 / 024 | loss: 0.80021\n",
      "batch 011 / 024 | loss: 0.80073\n",
      "batch 012 / 024 | loss: 0.79371\n",
      "batch 013 / 024 | loss: 0.79116\n",
      "batch 014 / 024 | loss: 0.78806\n",
      "batch 015 / 024 | loss: 0.78234\n",
      "batch 016 / 024 | loss: 0.78481\n",
      "batch 017 / 024 | loss: 0.78587\n",
      "batch 018 / 024 | loss: 0.78479\n",
      "batch 019 / 024 | loss: 0.79011\n",
      "batch 020 / 024 | loss: 0.79024\n",
      "batch 021 / 024 | loss: 0.79330\n",
      "batch 022 / 024 | loss: 0.79650\n",
      "batch 023 / 024 | loss: 0.79131\n",
      "batch 024 / 024 | loss: 0.79469\n",
      "model saved!\n",
      "----- epoch 002 / 010 | time: 203 sec | loss: 0.97470 | err: 0.33733\n",
      "batch 001 / 024 | loss: 0.78651\n",
      "batch 002 / 024 | loss: 0.74549\n",
      "batch 003 / 024 | loss: 0.75086\n",
      "batch 004 / 024 | loss: 0.76097\n",
      "batch 005 / 024 | loss: 0.78109\n",
      "batch 006 / 024 | loss: 0.78776\n",
      "batch 007 / 024 | loss: 0.79873\n",
      "batch 008 / 024 | loss: 0.78609\n",
      "batch 009 / 024 | loss: 0.78299\n",
      "batch 010 / 024 | loss: 0.79065\n",
      "batch 011 / 024 | loss: 0.78778\n",
      "batch 012 / 024 | loss: 0.78954\n",
      "batch 013 / 024 | loss: 0.79170\n",
      "batch 014 / 024 | loss: 0.78103\n",
      "batch 015 / 024 | loss: 0.76605\n",
      "batch 016 / 024 | loss: 0.77033\n",
      "batch 017 / 024 | loss: 0.76702\n",
      "batch 018 / 024 | loss: 0.77292\n",
      "batch 019 / 024 | loss: 0.76762\n",
      "batch 020 / 024 | loss: 0.77069\n",
      "batch 021 / 024 | loss: 0.77151\n",
      "batch 022 / 024 | loss: 0.78089\n",
      "batch 023 / 024 | loss: 0.78033\n",
      "batch 024 / 024 | loss: 0.78127\n",
      "model saved!\n",
      "----- epoch 003 / 010 | time: 198 sec | loss: 1.02135 | err: 0.33467\n",
      "batch 001 / 024 | loss: 0.67295\n",
      "batch 002 / 024 | loss: 0.79657\n",
      "batch 003 / 024 | loss: 0.77633\n",
      "batch 004 / 024 | loss: 0.83118\n",
      "batch 005 / 024 | loss: 0.82146\n",
      "batch 006 / 024 | loss: 0.80263\n",
      "batch 007 / 024 | loss: 0.80399\n",
      "batch 008 / 024 | loss: 0.80488\n",
      "batch 009 / 024 | loss: 0.78566\n",
      "batch 010 / 024 | loss: 0.79942\n",
      "batch 011 / 024 | loss: 0.81034\n",
      "batch 012 / 024 | loss: 0.81628\n",
      "batch 013 / 024 | loss: 0.80985\n",
      "batch 014 / 024 | loss: 0.81691\n",
      "batch 015 / 024 | loss: 0.82361\n",
      "batch 016 / 024 | loss: 0.83171\n",
      "batch 017 / 024 | loss: 0.83280\n",
      "batch 018 / 024 | loss: 0.83734\n",
      "batch 019 / 024 | loss: 0.83814\n",
      "batch 020 / 024 | loss: 0.84148\n",
      "batch 021 / 024 | loss: 0.85227\n",
      "batch 022 / 024 | loss: 0.84757\n",
      "batch 023 / 024 | loss: 0.84583\n",
      "batch 024 / 024 | loss: 0.84844\n",
      "----- epoch 004 / 010 | time: 178 sec | loss: 0.91161 | err: 0.35200\n",
      "batch 001 / 024 | loss: 0.94084\n",
      "batch 002 / 024 | loss: 0.87390\n",
      "batch 003 / 024 | loss: 0.89237\n",
      "batch 004 / 024 | loss: 0.82088\n",
      "batch 005 / 024 | loss: 0.82974\n",
      "batch 006 / 024 | loss: 0.81160\n",
      "batch 007 / 024 | loss: 0.81515\n",
      "batch 008 / 024 | loss: 0.80791\n",
      "batch 009 / 024 | loss: 0.81907\n",
      "batch 010 / 024 | loss: 0.79731\n",
      "batch 011 / 024 | loss: 0.79639\n",
      "batch 012 / 024 | loss: 0.80760\n",
      "batch 013 / 024 | loss: 0.79071\n",
      "batch 014 / 024 | loss: 0.78718\n",
      "batch 015 / 024 | loss: 0.78326\n",
      "batch 016 / 024 | loss: 0.78264\n",
      "batch 017 / 024 | loss: 0.77893\n",
      "batch 018 / 024 | loss: 0.77697\n",
      "batch 019 / 024 | loss: 0.77838\n",
      "batch 020 / 024 | loss: 0.78534\n",
      "batch 021 / 024 | loss: 0.78826\n",
      "batch 022 / 024 | loss: 0.79127\n",
      "batch 023 / 024 | loss: 0.79033\n",
      "batch 024 / 024 | loss: 0.78729\n",
      "----- epoch 005 / 010 | time: 163 sec | loss: 1.12591 | err: 0.34000\n",
      "batch 001 / 024 | loss: 0.78754\n",
      "batch 002 / 024 | loss: 0.73383\n",
      "batch 003 / 024 | loss: 0.76348\n",
      "batch 004 / 024 | loss: 0.77189\n",
      "batch 005 / 024 | loss: 0.77835\n",
      "batch 006 / 024 | loss: 0.80542\n",
      "batch 007 / 024 | loss: 0.81218\n",
      "batch 008 / 024 | loss: 0.83676\n",
      "batch 009 / 024 | loss: 0.82725\n",
      "batch 010 / 024 | loss: 0.81928\n",
      "batch 011 / 024 | loss: 0.81351\n",
      "batch 012 / 024 | loss: 0.81300\n",
      "batch 013 / 024 | loss: 0.82077\n",
      "batch 014 / 024 | loss: 0.83224\n",
      "batch 015 / 024 | loss: 0.82985\n",
      "batch 016 / 024 | loss: 0.82859\n",
      "batch 017 / 024 | loss: 0.82326\n",
      "batch 018 / 024 | loss: 0.82677\n",
      "batch 019 / 024 | loss: 0.82704\n",
      "batch 020 / 024 | loss: 0.81212\n",
      "batch 021 / 024 | loss: 0.80891\n",
      "batch 022 / 024 | loss: 0.81009\n",
      "batch 023 / 024 | loss: 0.80404\n",
      "batch 024 / 024 | loss: 0.80449\n",
      "model saved!\n",
      "----- epoch 006 / 010 | time: 156 sec | loss: 1.03081 | err: 0.33333\n",
      "batch 001 / 024 | loss: 0.62652\n",
      "batch 002 / 024 | loss: 0.70568\n",
      "batch 003 / 024 | loss: 0.74812\n",
      "batch 004 / 024 | loss: 0.73981\n",
      "batch 005 / 024 | loss: 0.78178\n",
      "batch 006 / 024 | loss: 0.80762\n",
      "batch 007 / 024 | loss: 0.80507\n",
      "batch 008 / 024 | loss: 0.80712\n",
      "batch 009 / 024 | loss: 0.81727\n",
      "batch 010 / 024 | loss: 0.80395\n",
      "batch 011 / 024 | loss: 0.79196\n",
      "batch 012 / 024 | loss: 0.80770\n",
      "batch 013 / 024 | loss: 0.81529\n",
      "batch 014 / 024 | loss: 0.80668\n",
      "batch 015 / 024 | loss: 0.80416\n",
      "batch 016 / 024 | loss: 0.80957\n",
      "batch 017 / 024 | loss: 0.80387\n",
      "batch 018 / 024 | loss: 0.80617\n",
      "batch 019 / 024 | loss: 0.81142\n",
      "batch 020 / 024 | loss: 0.81229\n",
      "batch 021 / 024 | loss: 0.80851\n",
      "batch 022 / 024 | loss: 0.80697\n",
      "batch 023 / 024 | loss: 0.80516\n",
      "batch 024 / 024 | loss: 0.80203\n",
      "model saved!\n",
      "----- epoch 007 / 010 | time: 155 sec | loss: 1.10784 | err: 0.33200\n",
      "batch 001 / 024 | loss: 0.88213\n",
      "batch 002 / 024 | loss: 0.80228\n",
      "batch 003 / 024 | loss: 0.84224\n",
      "batch 004 / 024 | loss: 0.85220\n",
      "batch 005 / 024 | loss: 0.84411\n",
      "batch 006 / 024 | loss: 0.84390\n",
      "batch 007 / 024 | loss: 0.82837\n",
      "batch 008 / 024 | loss: 0.82903\n",
      "batch 009 / 024 | loss: 0.84014\n",
      "batch 010 / 024 | loss: 0.82701\n",
      "batch 011 / 024 | loss: 0.80992\n",
      "batch 012 / 024 | loss: 0.79791\n",
      "batch 013 / 024 | loss: 0.78998\n",
      "batch 014 / 024 | loss: 0.80090\n",
      "batch 015 / 024 | loss: 0.79695\n",
      "batch 016 / 024 | loss: 0.79285\n",
      "batch 017 / 024 | loss: 0.78621\n",
      "batch 018 / 024 | loss: 0.78846\n",
      "batch 019 / 024 | loss: 0.79653\n",
      "batch 020 / 024 | loss: 0.79904\n",
      "batch 021 / 024 | loss: 0.80316\n",
      "batch 022 / 024 | loss: 0.80755\n",
      "batch 023 / 024 | loss: 0.80584\n",
      "batch 024 / 024 | loss: 0.81141\n",
      "----- epoch 008 / 010 | time: 150 sec | loss: 0.91667 | err: 0.34267\n",
      "batch 001 / 024 | loss: 0.85671\n",
      "batch 002 / 024 | loss: 0.84019\n",
      "batch 003 / 024 | loss: 0.83121\n",
      "batch 004 / 024 | loss: 0.84270\n",
      "batch 005 / 024 | loss: 0.83355\n",
      "batch 006 / 024 | loss: 0.83533\n",
      "batch 007 / 024 | loss: 0.80841\n",
      "batch 008 / 024 | loss: 0.80487\n",
      "batch 009 / 024 | loss: 0.82584\n",
      "batch 010 / 024 | loss: 0.81984\n",
      "batch 011 / 024 | loss: 0.81702\n",
      "batch 012 / 024 | loss: 0.81245\n",
      "batch 013 / 024 | loss: 0.81252\n",
      "batch 014 / 024 | loss: 0.80972\n",
      "batch 015 / 024 | loss: 0.80308\n",
      "batch 016 / 024 | loss: 0.80126\n",
      "batch 017 / 024 | loss: 0.79509\n",
      "batch 018 / 024 | loss: 0.79571\n",
      "batch 019 / 024 | loss: 0.79474\n",
      "batch 020 / 024 | loss: 0.79593\n",
      "batch 021 / 024 | loss: 0.79896\n",
      "batch 022 / 024 | loss: 0.80513\n",
      "batch 023 / 024 | loss: 0.80683\n",
      "batch 024 / 024 | loss: 0.80495\n",
      "----- epoch 009 / 010 | time: 146 sec | loss: 0.86965 | err: 0.35600\n",
      "batch 001 / 024 | loss: 0.71893\n",
      "batch 002 / 024 | loss: 0.84260\n",
      "batch 003 / 024 | loss: 0.86699\n",
      "batch 004 / 024 | loss: 0.86264\n",
      "batch 005 / 024 | loss: 0.85735\n",
      "batch 006 / 024 | loss: 0.85299\n",
      "batch 007 / 024 | loss: 0.83319\n",
      "batch 008 / 024 | loss: 0.83899\n",
      "batch 009 / 024 | loss: 0.83158\n",
      "batch 010 / 024 | loss: 0.83280\n",
      "batch 011 / 024 | loss: 0.83223\n",
      "batch 012 / 024 | loss: 0.81435\n",
      "batch 013 / 024 | loss: 0.80375\n",
      "batch 014 / 024 | loss: 0.80095\n",
      "batch 015 / 024 | loss: 0.80855\n",
      "batch 016 / 024 | loss: 0.80581\n",
      "batch 017 / 024 | loss: 0.80261\n",
      "batch 018 / 024 | loss: 0.79533\n",
      "batch 019 / 024 | loss: 0.79645\n",
      "batch 020 / 024 | loss: 0.79783\n",
      "batch 021 / 024 | loss: 0.80221\n",
      "batch 022 / 024 | loss: 0.80074\n",
      "batch 023 / 024 | loss: 0.80301\n",
      "batch 024 / 024 | loss: 0.80738\n",
      "----- epoch 010 / 010 | time: 146 sec | loss: 1.02384 | err: 0.34400\n",
      "training time: 1704.4506378173828 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.02301807313022468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 19 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 20 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 21 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.79265\n",
      "batch 002 / 024 | loss: 0.88752\n",
      "batch 003 / 024 | loss: 0.88051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 004 / 024 | loss: 0.88782\n",
      "batch 005 / 024 | loss: 0.91565\n",
      "batch 006 / 024 | loss: 0.91596\n",
      "batch 007 / 024 | loss: 0.91515\n",
      "batch 008 / 024 | loss: 0.90483\n",
      "batch 009 / 024 | loss: 0.90851\n",
      "batch 010 / 024 | loss: 0.90102\n",
      "batch 011 / 024 | loss: 0.87832\n",
      "batch 012 / 024 | loss: 0.87790\n",
      "batch 013 / 024 | loss: 0.87603\n",
      "batch 014 / 024 | loss: 0.87349\n",
      "batch 015 / 024 | loss: 0.86588\n",
      "batch 016 / 024 | loss: 0.86506\n",
      "batch 017 / 024 | loss: 0.86046\n",
      "batch 018 / 024 | loss: 0.86874\n",
      "batch 019 / 024 | loss: 0.86996\n",
      "batch 020 / 024 | loss: 0.86370\n",
      "batch 021 / 024 | loss: 0.85843\n",
      "batch 022 / 024 | loss: 0.85857\n",
      "batch 023 / 024 | loss: 0.85943\n",
      "batch 024 / 024 | loss: 0.85679\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 190 sec | loss: 0.88681 | err: 0.33467\n",
      "batch 001 / 024 | loss: 0.77286\n",
      "batch 002 / 024 | loss: 0.81948\n",
      "batch 003 / 024 | loss: 0.80311\n",
      "batch 004 / 024 | loss: 0.79194\n",
      "batch 005 / 024 | loss: 0.81380\n",
      "batch 006 / 024 | loss: 0.83980\n",
      "batch 007 / 024 | loss: 0.82054\n",
      "batch 008 / 024 | loss: 0.80075\n",
      "batch 009 / 024 | loss: 0.80468\n",
      "batch 010 / 024 | loss: 0.80495\n",
      "batch 011 / 024 | loss: 0.81223\n",
      "batch 012 / 024 | loss: 0.81409\n",
      "batch 013 / 024 | loss: 0.81422\n",
      "batch 014 / 024 | loss: 0.81234\n",
      "batch 015 / 024 | loss: 0.81174\n",
      "batch 016 / 024 | loss: 0.81384\n",
      "batch 017 / 024 | loss: 0.81457\n",
      "batch 018 / 024 | loss: 0.81142\n",
      "batch 019 / 024 | loss: 0.81536\n",
      "batch 020 / 024 | loss: 0.81441\n",
      "batch 021 / 024 | loss: 0.81801\n",
      "batch 022 / 024 | loss: 0.82070\n",
      "batch 023 / 024 | loss: 0.81463\n",
      "batch 024 / 024 | loss: 0.81237\n",
      "model saved!\n",
      "----- epoch 002 / 010 | time: 192 sec | loss: 1.05798 | err: 0.33333\n",
      "batch 001 / 024 | loss: 0.77034\n",
      "batch 002 / 024 | loss: 0.74067\n",
      "batch 003 / 024 | loss: 0.73154\n",
      "batch 004 / 024 | loss: 0.74377\n",
      "batch 005 / 024 | loss: 0.75704\n",
      "batch 006 / 024 | loss: 0.76527\n",
      "batch 007 / 024 | loss: 0.78116\n",
      "batch 008 / 024 | loss: 0.77513\n",
      "batch 009 / 024 | loss: 0.77382\n",
      "batch 010 / 024 | loss: 0.78448\n",
      "batch 011 / 024 | loss: 0.78161\n",
      "batch 012 / 024 | loss: 0.78402\n",
      "batch 013 / 024 | loss: 0.78542\n",
      "batch 014 / 024 | loss: 0.77528\n",
      "batch 015 / 024 | loss: 0.76065\n",
      "batch 016 / 024 | loss: 0.76540\n",
      "batch 017 / 024 | loss: 0.76453\n",
      "batch 018 / 024 | loss: 0.77197\n",
      "batch 019 / 024 | loss: 0.76671\n",
      "batch 020 / 024 | loss: 0.77213\n",
      "batch 021 / 024 | loss: 0.77405\n",
      "batch 022 / 024 | loss: 0.78469\n",
      "batch 023 / 024 | loss: 0.78458\n",
      "batch 024 / 024 | loss: 0.78519\n",
      "----- epoch 003 / 010 | time: 182 sec | loss: 0.97985 | err: 0.33733\n",
      "batch 001 / 024 | loss: 0.68187\n",
      "batch 002 / 024 | loss: 0.78970\n",
      "batch 003 / 024 | loss: 0.76765\n",
      "batch 004 / 024 | loss: 0.82461\n",
      "batch 005 / 024 | loss: 0.81693\n",
      "batch 006 / 024 | loss: 0.79842\n",
      "batch 007 / 024 | loss: 0.79738\n",
      "batch 008 / 024 | loss: 0.79398\n",
      "batch 009 / 024 | loss: 0.77103\n",
      "batch 010 / 024 | loss: 0.78631\n",
      "batch 011 / 024 | loss: 0.79867\n",
      "batch 012 / 024 | loss: 0.80199\n",
      "batch 013 / 024 | loss: 0.79404\n",
      "batch 014 / 024 | loss: 0.80159\n",
      "batch 015 / 024 | loss: 0.80683\n",
      "batch 016 / 024 | loss: 0.81488\n",
      "batch 017 / 024 | loss: 0.81585\n",
      "batch 018 / 024 | loss: 0.81914\n",
      "batch 019 / 024 | loss: 0.81727\n",
      "batch 020 / 024 | loss: 0.81659\n",
      "batch 021 / 024 | loss: 0.82626\n",
      "batch 022 / 024 | loss: 0.82281\n",
      "batch 023 / 024 | loss: 0.82095\n",
      "batch 024 / 024 | loss: 0.82422\n",
      "----- epoch 004 / 010 | time: 188 sec | loss: 0.92027 | err: 0.33333\n",
      "batch 001 / 024 | loss: 0.92608\n",
      "batch 002 / 024 | loss: 0.85476\n",
      "batch 003 / 024 | loss: 0.87253\n",
      "batch 004 / 024 | loss: 0.79902\n",
      "batch 005 / 024 | loss: 0.80906\n",
      "batch 006 / 024 | loss: 0.79616\n",
      "batch 007 / 024 | loss: 0.80182\n",
      "batch 008 / 024 | loss: 0.79592\n",
      "batch 009 / 024 | loss: 0.80687\n",
      "batch 010 / 024 | loss: 0.78788\n",
      "batch 011 / 024 | loss: 0.78644\n",
      "batch 012 / 024 | loss: 0.79900\n",
      "batch 013 / 024 | loss: 0.78468\n",
      "batch 014 / 024 | loss: 0.78339\n",
      "batch 015 / 024 | loss: 0.78600\n",
      "batch 016 / 024 | loss: 0.78883\n",
      "batch 017 / 024 | loss: 0.78589\n",
      "batch 018 / 024 | loss: 0.78789\n",
      "batch 019 / 024 | loss: 0.79161\n",
      "batch 020 / 024 | loss: 0.79985\n",
      "batch 021 / 024 | loss: 0.80588\n",
      "batch 022 / 024 | loss: 0.81134\n",
      "batch 023 / 024 | loss: 0.80923\n",
      "batch 024 / 024 | loss: 0.80603\n",
      "----- epoch 005 / 010 | time: 166 sec | loss: 0.96966 | err: 0.36000\n",
      "batch 001 / 024 | loss: 0.83045\n",
      "batch 002 / 024 | loss: 0.78361\n",
      "batch 003 / 024 | loss: 0.81417\n",
      "batch 004 / 024 | loss: 0.81509\n",
      "batch 005 / 024 | loss: 0.83253\n",
      "batch 006 / 024 | loss: 0.85694\n",
      "batch 007 / 024 | loss: 0.85873\n",
      "batch 008 / 024 | loss: 0.87695\n",
      "batch 009 / 024 | loss: 0.87136\n",
      "batch 010 / 024 | loss: 0.86201\n",
      "batch 011 / 024 | loss: 0.85941\n",
      "batch 012 / 024 | loss: 0.86299\n",
      "batch 013 / 024 | loss: 0.87066\n",
      "batch 014 / 024 | loss: 0.87382\n",
      "batch 015 / 024 | loss: 0.86926\n",
      "batch 016 / 024 | loss: 0.86677\n",
      "batch 017 / 024 | loss: 0.86084\n",
      "batch 018 / 024 | loss: 0.86463\n",
      "batch 019 / 024 | loss: 0.86313\n",
      "batch 020 / 024 | loss: 0.84483\n",
      "batch 021 / 024 | loss: 0.84091\n",
      "batch 022 / 024 | loss: 0.84232\n",
      "batch 023 / 024 | loss: 0.83378\n",
      "batch 024 / 024 | loss: 0.83163\n",
      "model saved!\n",
      "----- epoch 006 / 010 | time: 193 sec | loss: 1.11088 | err: 0.31733\n",
      "batch 001 / 024 | loss: 0.64977\n",
      "batch 002 / 024 | loss: 0.67285\n",
      "batch 003 / 024 | loss: 0.70609\n",
      "batch 004 / 024 | loss: 0.69376\n",
      "batch 005 / 024 | loss: 0.74526\n",
      "batch 006 / 024 | loss: 0.78155\n",
      "batch 007 / 024 | loss: 0.78355\n",
      "batch 008 / 024 | loss: 0.78659\n",
      "batch 009 / 024 | loss: 0.79748\n",
      "batch 010 / 024 | loss: 0.78624\n",
      "batch 011 / 024 | loss: 0.77218\n",
      "batch 012 / 024 | loss: 0.78920\n",
      "batch 013 / 024 | loss: 0.79313\n",
      "batch 014 / 024 | loss: 0.78408\n",
      "batch 015 / 024 | loss: 0.78243\n",
      "batch 016 / 024 | loss: 0.78975\n",
      "batch 017 / 024 | loss: 0.78417\n",
      "batch 018 / 024 | loss: 0.78834\n",
      "batch 019 / 024 | loss: 0.79438\n",
      "batch 020 / 024 | loss: 0.79707\n",
      "batch 021 / 024 | loss: 0.79332\n",
      "batch 022 / 024 | loss: 0.79528\n",
      "batch 023 / 024 | loss: 0.79372\n",
      "batch 024 / 024 | loss: 0.79068\n",
      "----- epoch 007 / 010 | time: 186 sec | loss: 1.07992 | err: 0.33333\n",
      "batch 001 / 024 | loss: 0.83524\n",
      "batch 002 / 024 | loss: 0.77863\n",
      "batch 003 / 024 | loss: 0.83494\n",
      "batch 004 / 024 | loss: 0.84339\n",
      "batch 005 / 024 | loss: 0.83194\n",
      "batch 006 / 024 | loss: 0.83367\n",
      "batch 007 / 024 | loss: 0.81740\n",
      "batch 008 / 024 | loss: 0.81629\n",
      "batch 009 / 024 | loss: 0.82724\n",
      "batch 010 / 024 | loss: 0.81889\n",
      "batch 011 / 024 | loss: 0.80198\n",
      "batch 012 / 024 | loss: 0.79068\n",
      "batch 013 / 024 | loss: 0.78428\n",
      "batch 014 / 024 | loss: 0.79508\n",
      "batch 015 / 024 | loss: 0.79248\n",
      "batch 016 / 024 | loss: 0.78879\n",
      "batch 017 / 024 | loss: 0.78241\n",
      "batch 018 / 024 | loss: 0.78915\n",
      "batch 019 / 024 | loss: 0.79815\n",
      "batch 020 / 024 | loss: 0.79907\n",
      "batch 021 / 024 | loss: 0.80599\n",
      "batch 022 / 024 | loss: 0.80930\n",
      "batch 023 / 024 | loss: 0.80813\n",
      "batch 024 / 024 | loss: 0.81528\n",
      "----- epoch 008 / 010 | time: 190 sec | loss: 0.91166 | err: 0.34133\n",
      "batch 001 / 024 | loss: 0.88263\n",
      "batch 002 / 024 | loss: 0.87586\n",
      "batch 003 / 024 | loss: 0.86955\n",
      "batch 004 / 024 | loss: 0.87400\n",
      "batch 005 / 024 | loss: 0.86292\n",
      "batch 006 / 024 | loss: 0.85864\n",
      "batch 007 / 024 | loss: 0.82625\n",
      "batch 008 / 024 | loss: 0.82094\n",
      "batch 009 / 024 | loss: 0.84226\n",
      "batch 010 / 024 | loss: 0.83209\n",
      "batch 011 / 024 | loss: 0.82768\n",
      "batch 012 / 024 | loss: 0.82055\n",
      "batch 013 / 024 | loss: 0.82131\n",
      "batch 014 / 024 | loss: 0.81998\n",
      "batch 015 / 024 | loss: 0.81056\n",
      "batch 016 / 024 | loss: 0.80304\n",
      "batch 017 / 024 | loss: 0.79556\n",
      "batch 018 / 024 | loss: 0.79294\n",
      "batch 019 / 024 | loss: 0.79178\n",
      "batch 020 / 024 | loss: 0.79314\n",
      "batch 021 / 024 | loss: 0.79520\n",
      "batch 022 / 024 | loss: 0.80055\n",
      "batch 023 / 024 | loss: 0.80259\n",
      "batch 024 / 024 | loss: 0.80097\n",
      "----- epoch 009 / 010 | time: 192 sec | loss: 0.93553 | err: 0.34533\n",
      "batch 001 / 024 | loss: 0.69103\n",
      "batch 002 / 024 | loss: 0.83084\n",
      "batch 003 / 024 | loss: 0.87416\n",
      "batch 004 / 024 | loss: 0.86846\n",
      "batch 005 / 024 | loss: 0.86449\n",
      "batch 006 / 024 | loss: 0.86438\n",
      "batch 007 / 024 | loss: 0.85251\n",
      "batch 008 / 024 | loss: 0.86750\n",
      "batch 009 / 024 | loss: 0.86590\n",
      "batch 010 / 024 | loss: 0.86395\n",
      "batch 011 / 024 | loss: 0.86727\n",
      "batch 012 / 024 | loss: 0.85210\n",
      "batch 013 / 024 | loss: 0.84023\n",
      "batch 014 / 024 | loss: 0.83401\n",
      "batch 015 / 024 | loss: 0.83922\n",
      "batch 016 / 024 | loss: 0.83355\n",
      "batch 017 / 024 | loss: 0.82739\n",
      "batch 018 / 024 | loss: 0.81617\n",
      "batch 019 / 024 | loss: 0.81526\n",
      "batch 020 / 024 | loss: 0.81594\n",
      "batch 021 / 024 | loss: 0.81789\n",
      "batch 022 / 024 | loss: 0.81389\n",
      "batch 023 / 024 | loss: 0.81669\n",
      "batch 024 / 024 | loss: 0.82132\n",
      "----- epoch 010 / 010 | time: 143 sec | loss: 1.00382 | err: 0.33333\n",
      "training time: 1827.2009060382843 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.027194605220720855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 22 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 23 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 24 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.79457\n",
      "batch 002 / 024 | loss: 0.88999\n",
      "batch 003 / 024 | loss: 0.88228\n",
      "batch 004 / 024 | loss: 0.88920\n",
      "batch 005 / 024 | loss: 0.91691\n",
      "batch 006 / 024 | loss: 0.91734\n",
      "batch 007 / 024 | loss: 0.91658\n",
      "batch 008 / 024 | loss: 0.90640\n",
      "batch 009 / 024 | loss: 0.91020\n",
      "batch 010 / 024 | loss: 0.90287\n",
      "batch 011 / 024 | loss: 0.88051\n",
      "batch 012 / 024 | loss: 0.88066\n",
      "batch 013 / 024 | loss: 0.87884\n",
      "batch 014 / 024 | loss: 0.87735\n",
      "batch 015 / 024 | loss: 0.87250\n",
      "batch 016 / 024 | loss: 0.87153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 017 / 024 | loss: 0.87038\n",
      "batch 018 / 024 | loss: 0.87763\n",
      "batch 019 / 024 | loss: 0.87814\n",
      "batch 020 / 024 | loss: 0.87024\n",
      "batch 021 / 024 | loss: 0.86555\n",
      "batch 022 / 024 | loss: 0.86392\n",
      "batch 023 / 024 | loss: 0.86505\n",
      "batch 024 / 024 | loss: 0.85990\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 142 sec | loss: 0.99013 | err: 0.32933\n",
      "batch 001 / 024 | loss: 0.73189\n",
      "batch 002 / 024 | loss: 0.78507\n",
      "batch 003 / 024 | loss: 0.78343\n",
      "batch 004 / 024 | loss: 0.77993\n",
      "batch 005 / 024 | loss: 0.80585\n",
      "batch 006 / 024 | loss: 0.83076\n",
      "batch 007 / 024 | loss: 0.81724\n",
      "batch 008 / 024 | loss: 0.79816\n",
      "batch 009 / 024 | loss: 0.79932\n",
      "batch 010 / 024 | loss: 0.79801\n",
      "batch 011 / 024 | loss: 0.80157\n",
      "batch 012 / 024 | loss: 0.79667\n",
      "batch 013 / 024 | loss: 0.79291\n",
      "batch 014 / 024 | loss: 0.78691\n",
      "batch 015 / 024 | loss: 0.78214\n",
      "batch 016 / 024 | loss: 0.78758\n",
      "batch 017 / 024 | loss: 0.78960\n",
      "batch 018 / 024 | loss: 0.78703\n",
      "batch 019 / 024 | loss: 0.79035\n",
      "batch 020 / 024 | loss: 0.79180\n",
      "batch 021 / 024 | loss: 0.79909\n",
      "batch 022 / 024 | loss: 0.80395\n",
      "batch 023 / 024 | loss: 0.80375\n",
      "batch 024 / 024 | loss: 0.80717\n",
      "----- epoch 002 / 010 | time: 150 sec | loss: 0.93304 | err: 0.36533\n",
      "batch 001 / 024 | loss: 0.78129\n",
      "batch 002 / 024 | loss: 0.75343\n",
      "batch 003 / 024 | loss: 0.78191\n",
      "batch 004 / 024 | loss: 0.80270\n",
      "batch 005 / 024 | loss: 0.84154\n",
      "batch 006 / 024 | loss: 0.84452\n",
      "batch 007 / 024 | loss: 0.86452\n",
      "batch 008 / 024 | loss: 0.85417\n",
      "batch 009 / 024 | loss: 0.85657\n",
      "batch 010 / 024 | loss: 0.86396\n",
      "batch 011 / 024 | loss: 0.86369\n",
      "batch 012 / 024 | loss: 0.86555\n",
      "batch 013 / 024 | loss: 0.87089\n",
      "batch 014 / 024 | loss: 0.87005\n",
      "batch 015 / 024 | loss: 0.86337\n",
      "batch 016 / 024 | loss: 0.86711\n",
      "batch 017 / 024 | loss: 0.86506\n",
      "batch 018 / 024 | loss: 0.86866\n",
      "batch 019 / 024 | loss: 0.85963\n",
      "batch 020 / 024 | loss: 0.86107\n",
      "batch 021 / 024 | loss: 0.85937\n",
      "batch 022 / 024 | loss: 0.86718\n",
      "batch 023 / 024 | loss: 0.86385\n",
      "batch 024 / 024 | loss: 0.86099\n",
      "----- epoch 003 / 010 | time: 143 sec | loss: 0.90488 | err: 0.34267\n",
      "batch 001 / 024 | loss: 0.68998\n",
      "batch 002 / 024 | loss: 0.79771\n",
      "batch 003 / 024 | loss: 0.76795\n",
      "batch 004 / 024 | loss: 0.80324\n",
      "batch 005 / 024 | loss: 0.79759\n",
      "batch 006 / 024 | loss: 0.77707\n",
      "batch 007 / 024 | loss: 0.76757\n",
      "batch 008 / 024 | loss: 0.76557\n",
      "batch 009 / 024 | loss: 0.74430\n",
      "batch 010 / 024 | loss: 0.75141\n",
      "batch 011 / 024 | loss: 0.76274\n",
      "batch 012 / 024 | loss: 0.77029\n",
      "batch 013 / 024 | loss: 0.76503\n",
      "batch 014 / 024 | loss: 0.77474\n",
      "batch 015 / 024 | loss: 0.78227\n",
      "batch 016 / 024 | loss: 0.79009\n",
      "batch 017 / 024 | loss: 0.79434\n",
      "batch 018 / 024 | loss: 0.79913\n",
      "batch 019 / 024 | loss: 0.79846\n",
      "batch 020 / 024 | loss: 0.79882\n",
      "batch 021 / 024 | loss: 0.80896\n",
      "batch 022 / 024 | loss: 0.80666\n",
      "batch 023 / 024 | loss: 0.80578\n",
      "batch 024 / 024 | loss: 0.80931\n",
      "----- epoch 004 / 010 | time: 140 sec | loss: 1.03344 | err: 0.33867\n",
      "batch 001 / 024 | loss: 0.93970\n",
      "batch 002 / 024 | loss: 0.86702\n",
      "batch 003 / 024 | loss: 0.89045\n",
      "batch 004 / 024 | loss: 0.80985\n",
      "batch 005 / 024 | loss: 0.82252\n",
      "batch 006 / 024 | loss: 0.80774\n",
      "batch 007 / 024 | loss: 0.81131\n",
      "batch 008 / 024 | loss: 0.80730\n",
      "batch 009 / 024 | loss: 0.81642\n",
      "batch 010 / 024 | loss: 0.79861\n",
      "batch 011 / 024 | loss: 0.79738\n",
      "batch 012 / 024 | loss: 0.80960\n",
      "batch 013 / 024 | loss: 0.79485\n",
      "batch 014 / 024 | loss: 0.79269\n",
      "batch 015 / 024 | loss: 0.79514\n",
      "batch 016 / 024 | loss: 0.80170\n",
      "batch 017 / 024 | loss: 0.79759\n",
      "batch 018 / 024 | loss: 0.79853\n",
      "batch 019 / 024 | loss: 0.80231\n",
      "batch 020 / 024 | loss: 0.81001\n",
      "batch 021 / 024 | loss: 0.81520\n",
      "batch 022 / 024 | loss: 0.82049\n",
      "batch 023 / 024 | loss: 0.81825\n",
      "batch 024 / 024 | loss: 0.81481\n",
      "----- epoch 005 / 010 | time: 134 sec | loss: 0.97828 | err: 0.35867\n",
      "batch 001 / 024 | loss: 0.82972\n",
      "batch 002 / 024 | loss: 0.77650\n",
      "batch 003 / 024 | loss: 0.80216\n",
      "batch 004 / 024 | loss: 0.80852\n",
      "batch 005 / 024 | loss: 0.82455\n",
      "batch 006 / 024 | loss: 0.84755\n",
      "batch 007 / 024 | loss: 0.84777\n",
      "batch 008 / 024 | loss: 0.86776\n",
      "batch 009 / 024 | loss: 0.85637\n",
      "batch 010 / 024 | loss: 0.84309\n",
      "batch 011 / 024 | loss: 0.84062\n",
      "batch 012 / 024 | loss: 0.84094\n",
      "batch 013 / 024 | loss: 0.85096\n",
      "batch 014 / 024 | loss: 0.85646\n",
      "batch 015 / 024 | loss: 0.85259\n",
      "batch 016 / 024 | loss: 0.85253\n",
      "batch 017 / 024 | loss: 0.84749\n",
      "batch 018 / 024 | loss: 0.85453\n",
      "batch 019 / 024 | loss: 0.85452\n",
      "batch 020 / 024 | loss: 0.83711\n",
      "batch 021 / 024 | loss: 0.83370\n",
      "batch 022 / 024 | loss: 0.83456\n",
      "batch 023 / 024 | loss: 0.82457\n",
      "batch 024 / 024 | loss: 0.82286\n",
      "model saved!\n",
      "----- epoch 006 / 010 | time: 164 sec | loss: 1.12066 | err: 0.32267\n",
      "batch 001 / 024 | loss: 0.64914\n",
      "batch 002 / 024 | loss: 0.67151\n",
      "batch 003 / 024 | loss: 0.71113\n",
      "batch 004 / 024 | loss: 0.70181\n",
      "batch 005 / 024 | loss: 0.75363\n",
      "batch 006 / 024 | loss: 0.78283\n",
      "batch 007 / 024 | loss: 0.78729\n",
      "batch 008 / 024 | loss: 0.79453\n",
      "batch 009 / 024 | loss: 0.80763\n",
      "batch 010 / 024 | loss: 0.79595\n",
      "batch 011 / 024 | loss: 0.78456\n",
      "batch 012 / 024 | loss: 0.79986\n",
      "batch 013 / 024 | loss: 0.80601\n",
      "batch 014 / 024 | loss: 0.79932\n",
      "batch 015 / 024 | loss: 0.79966\n",
      "batch 016 / 024 | loss: 0.80799\n",
      "batch 017 / 024 | loss: 0.80367\n",
      "batch 018 / 024 | loss: 0.80802\n",
      "batch 019 / 024 | loss: 0.81627\n",
      "batch 020 / 024 | loss: 0.81979\n",
      "batch 021 / 024 | loss: 0.82076\n",
      "batch 022 / 024 | loss: 0.82440\n",
      "batch 023 / 024 | loss: 0.82310\n",
      "batch 024 / 024 | loss: 0.82197\n",
      "----- epoch 007 / 010 | time: 153 sec | loss: 0.92614 | err: 0.35200\n",
      "batch 001 / 024 | loss: 0.86216\n",
      "batch 002 / 024 | loss: 0.82861\n",
      "batch 003 / 024 | loss: 0.87255\n",
      "batch 004 / 024 | loss: 0.88601\n",
      "batch 005 / 024 | loss: 0.88725\n",
      "batch 006 / 024 | loss: 0.88616\n",
      "batch 007 / 024 | loss: 0.87468\n",
      "batch 008 / 024 | loss: 0.87937\n",
      "batch 009 / 024 | loss: 0.89343\n",
      "batch 010 / 024 | loss: 0.88329\n",
      "batch 011 / 024 | loss: 0.86806\n",
      "batch 012 / 024 | loss: 0.85548\n",
      "batch 013 / 024 | loss: 0.84643\n",
      "batch 014 / 024 | loss: 0.85296\n",
      "batch 015 / 024 | loss: 0.84733\n",
      "batch 016 / 024 | loss: 0.84179\n",
      "batch 017 / 024 | loss: 0.83417\n",
      "batch 018 / 024 | loss: 0.83346\n",
      "batch 019 / 024 | loss: 0.83757\n",
      "batch 020 / 024 | loss: 0.83422\n",
      "batch 021 / 024 | loss: 0.83407\n",
      "batch 022 / 024 | loss: 0.83758\n",
      "batch 023 / 024 | loss: 0.83539\n",
      "batch 024 / 024 | loss: 0.84201\n",
      "----- epoch 008 / 010 | time: 153 sec | loss: 0.91802 | err: 0.35733\n",
      "batch 001 / 024 | loss: 0.91157\n",
      "batch 002 / 024 | loss: 0.87743\n",
      "batch 003 / 024 | loss: 0.88079\n",
      "batch 004 / 024 | loss: 0.88152\n",
      "batch 005 / 024 | loss: 0.87310\n",
      "batch 006 / 024 | loss: 0.86603\n",
      "batch 007 / 024 | loss: 0.83542\n",
      "batch 008 / 024 | loss: 0.83144\n",
      "batch 009 / 024 | loss: 0.84629\n",
      "batch 010 / 024 | loss: 0.83915\n",
      "batch 011 / 024 | loss: 0.82931\n",
      "batch 012 / 024 | loss: 0.82693\n",
      "batch 013 / 024 | loss: 0.82994\n",
      "batch 014 / 024 | loss: 0.82509\n",
      "batch 015 / 024 | loss: 0.81298\n",
      "batch 016 / 024 | loss: 0.80123\n",
      "batch 017 / 024 | loss: 0.79454\n",
      "batch 018 / 024 | loss: 0.79529\n",
      "batch 019 / 024 | loss: 0.79642\n",
      "batch 020 / 024 | loss: 0.79631\n",
      "batch 021 / 024 | loss: 0.79912\n",
      "batch 022 / 024 | loss: 0.80054\n",
      "batch 023 / 024 | loss: 0.80332\n",
      "batch 024 / 024 | loss: 0.79972\n",
      "----- epoch 009 / 010 | time: 157 sec | loss: 0.90230 | err: 0.33600\n",
      "batch 001 / 024 | loss: 0.67843\n",
      "batch 002 / 024 | loss: 0.80711\n",
      "batch 003 / 024 | loss: 0.85980\n",
      "batch 004 / 024 | loss: 0.85918\n",
      "batch 005 / 024 | loss: 0.86260\n",
      "batch 006 / 024 | loss: 0.85990\n",
      "batch 007 / 024 | loss: 0.85062\n",
      "batch 008 / 024 | loss: 0.86863\n",
      "batch 009 / 024 | loss: 0.86342\n",
      "batch 010 / 024 | loss: 0.85931\n",
      "batch 011 / 024 | loss: 0.86300\n",
      "batch 012 / 024 | loss: 0.84520\n",
      "batch 013 / 024 | loss: 0.83533\n",
      "batch 014 / 024 | loss: 0.82985\n",
      "batch 015 / 024 | loss: 0.83580\n",
      "batch 016 / 024 | loss: 0.83432\n",
      "batch 017 / 024 | loss: 0.82863\n",
      "batch 018 / 024 | loss: 0.81823\n",
      "batch 019 / 024 | loss: 0.81672\n",
      "batch 020 / 024 | loss: 0.81752\n",
      "batch 021 / 024 | loss: 0.82034\n",
      "batch 022 / 024 | loss: 0.81663\n",
      "batch 023 / 024 | loss: 0.81984\n",
      "batch 024 / 024 | loss: 0.82238\n",
      "----- epoch 010 / 010 | time: 160 sec | loss: 0.98704 | err: 0.34667\n",
      "training time: 1501.9202852249146 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.03212895140817722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 25 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 26 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 27 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.79684\n",
      "batch 002 / 024 | loss: 0.89291\n",
      "batch 003 / 024 | loss: 0.88437\n",
      "batch 004 / 024 | loss: 0.89082\n",
      "batch 005 / 024 | loss: 0.91838\n",
      "batch 006 / 024 | loss: 0.91894\n",
      "batch 007 / 024 | loss: 0.91824\n",
      "batch 008 / 024 | loss: 0.90821\n",
      "batch 009 / 024 | loss: 0.91219\n",
      "batch 010 / 024 | loss: 0.90564\n",
      "batch 011 / 024 | loss: 0.88331\n",
      "batch 012 / 024 | loss: 0.88561\n",
      "batch 013 / 024 | loss: 0.88356\n",
      "batch 014 / 024 | loss: 0.88213\n",
      "batch 015 / 024 | loss: 0.87797\n",
      "batch 016 / 024 | loss: 0.87693\n",
      "batch 017 / 024 | loss: 0.87509\n",
      "batch 018 / 024 | loss: 0.88253\n",
      "batch 019 / 024 | loss: 0.88278\n",
      "batch 020 / 024 | loss: 0.87466\n",
      "batch 021 / 024 | loss: 0.87037\n",
      "batch 022 / 024 | loss: 0.86993\n",
      "batch 023 / 024 | loss: 0.87271\n",
      "batch 024 / 024 | loss: 0.86723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved!\n",
      "----- epoch 001 / 010 | time: 166 sec | loss: 0.94809 | err: 0.34133\n",
      "batch 001 / 024 | loss: 0.76680\n",
      "batch 002 / 024 | loss: 0.80538\n",
      "batch 003 / 024 | loss: 0.80663\n",
      "batch 004 / 024 | loss: 0.79905\n",
      "batch 005 / 024 | loss: 0.82849\n",
      "batch 006 / 024 | loss: 0.85917\n",
      "batch 007 / 024 | loss: 0.84983\n",
      "batch 008 / 024 | loss: 0.83992\n",
      "batch 009 / 024 | loss: 0.84345\n",
      "batch 010 / 024 | loss: 0.84251\n",
      "batch 011 / 024 | loss: 0.84807\n",
      "batch 012 / 024 | loss: 0.84727\n",
      "batch 013 / 024 | loss: 0.84384\n",
      "batch 014 / 024 | loss: 0.83987\n",
      "batch 015 / 024 | loss: 0.84245\n",
      "batch 016 / 024 | loss: 0.84507\n",
      "batch 017 / 024 | loss: 0.84348\n",
      "batch 018 / 024 | loss: 0.83958\n",
      "batch 019 / 024 | loss: 0.84300\n",
      "batch 020 / 024 | loss: 0.84181\n",
      "batch 021 / 024 | loss: 0.84600\n",
      "batch 022 / 024 | loss: 0.84869\n",
      "batch 023 / 024 | loss: 0.84642\n",
      "batch 024 / 024 | loss: 0.84746\n",
      "----- epoch 002 / 010 | time: 164 sec | loss: 0.93262 | err: 0.35200\n",
      "batch 001 / 024 | loss: 0.77914\n",
      "batch 002 / 024 | loss: 0.75295\n",
      "batch 003 / 024 | loss: 0.78068\n",
      "batch 004 / 024 | loss: 0.80132\n",
      "batch 005 / 024 | loss: 0.84257\n",
      "batch 006 / 024 | loss: 0.84209\n",
      "batch 007 / 024 | loss: 0.86182\n",
      "batch 008 / 024 | loss: 0.85012\n",
      "batch 009 / 024 | loss: 0.85345\n",
      "batch 010 / 024 | loss: 0.86050\n",
      "batch 011 / 024 | loss: 0.86067\n",
      "batch 012 / 024 | loss: 0.86252\n",
      "batch 013 / 024 | loss: 0.86685\n",
      "batch 014 / 024 | loss: 0.86574\n",
      "batch 015 / 024 | loss: 0.85936\n",
      "batch 016 / 024 | loss: 0.86334\n",
      "batch 017 / 024 | loss: 0.86115\n",
      "batch 018 / 024 | loss: 0.86340\n",
      "batch 019 / 024 | loss: 0.85516\n",
      "batch 020 / 024 | loss: 0.85543\n",
      "batch 021 / 024 | loss: 0.85229\n",
      "batch 022 / 024 | loss: 0.86178\n",
      "batch 023 / 024 | loss: 0.85773\n",
      "batch 024 / 024 | loss: 0.85230\n",
      "model saved!\n",
      "----- epoch 003 / 010 | time: 156 sec | loss: 1.03442 | err: 0.33067\n",
      "batch 001 / 024 | loss: 0.66322\n",
      "batch 002 / 024 | loss: 0.77844\n",
      "batch 003 / 024 | loss: 0.74681\n",
      "batch 004 / 024 | loss: 0.78851\n",
      "batch 005 / 024 | loss: 0.78533\n",
      "batch 006 / 024 | loss: 0.76873\n",
      "batch 007 / 024 | loss: 0.77071\n",
      "batch 008 / 024 | loss: 0.76929\n",
      "batch 009 / 024 | loss: 0.75033\n",
      "batch 010 / 024 | loss: 0.76137\n",
      "batch 011 / 024 | loss: 0.77388\n",
      "batch 012 / 024 | loss: 0.78277\n",
      "batch 013 / 024 | loss: 0.77951\n",
      "batch 014 / 024 | loss: 0.78879\n",
      "batch 015 / 024 | loss: 0.79728\n",
      "batch 016 / 024 | loss: 0.80625\n",
      "batch 017 / 024 | loss: 0.81063\n",
      "batch 018 / 024 | loss: 0.81739\n",
      "batch 019 / 024 | loss: 0.82020\n",
      "batch 020 / 024 | loss: 0.82474\n",
      "batch 021 / 024 | loss: 0.83562\n",
      "batch 022 / 024 | loss: 0.83314\n",
      "batch 023 / 024 | loss: 0.83289\n",
      "batch 024 / 024 | loss: 0.83652\n",
      "----- epoch 004 / 010 | time: 143 sec | loss: 0.91254 | err: 0.34933\n",
      "batch 001 / 024 | loss: 0.96358\n",
      "batch 002 / 024 | loss: 0.90580\n",
      "batch 003 / 024 | loss: 0.93415\n",
      "batch 004 / 024 | loss: 0.85013\n",
      "batch 005 / 024 | loss: 0.85135\n",
      "batch 006 / 024 | loss: 0.83148\n",
      "batch 007 / 024 | loss: 0.83102\n",
      "batch 008 / 024 | loss: 0.82566\n",
      "batch 009 / 024 | loss: 0.83527\n",
      "batch 010 / 024 | loss: 0.81689\n",
      "batch 011 / 024 | loss: 0.81502\n",
      "batch 012 / 024 | loss: 0.82423\n",
      "batch 013 / 024 | loss: 0.80928\n",
      "batch 014 / 024 | loss: 0.80454\n",
      "batch 015 / 024 | loss: 0.80328\n",
      "batch 016 / 024 | loss: 0.80586\n",
      "batch 017 / 024 | loss: 0.80284\n",
      "batch 018 / 024 | loss: 0.80330\n",
      "batch 019 / 024 | loss: 0.80413\n",
      "batch 020 / 024 | loss: 0.81143\n",
      "batch 021 / 024 | loss: 0.81555\n",
      "batch 022 / 024 | loss: 0.81917\n",
      "batch 023 / 024 | loss: 0.81756\n",
      "batch 024 / 024 | loss: 0.81330\n",
      "----- epoch 005 / 010 | time: 142 sec | loss: 1.06705 | err: 0.36400\n",
      "batch 001 / 024 | loss: 0.84647\n",
      "batch 002 / 024 | loss: 0.78831\n",
      "batch 003 / 024 | loss: 0.83391\n",
      "batch 004 / 024 | loss: 0.83201\n",
      "batch 005 / 024 | loss: 0.84417\n",
      "batch 006 / 024 | loss: 0.86588\n",
      "batch 007 / 024 | loss: 0.86610\n",
      "batch 008 / 024 | loss: 0.88495\n",
      "batch 009 / 024 | loss: 0.87913\n",
      "batch 010 / 024 | loss: 0.86944\n",
      "batch 011 / 024 | loss: 0.86654\n",
      "batch 012 / 024 | loss: 0.87238\n",
      "batch 013 / 024 | loss: 0.88171\n",
      "batch 014 / 024 | loss: 0.88396\n",
      "batch 015 / 024 | loss: 0.88018\n",
      "batch 016 / 024 | loss: 0.88101\n",
      "batch 017 / 024 | loss: 0.87811\n",
      "batch 018 / 024 | loss: 0.88755\n",
      "batch 019 / 024 | loss: 0.88849\n",
      "batch 020 / 024 | loss: 0.87432\n",
      "batch 021 / 024 | loss: 0.87279\n",
      "batch 022 / 024 | loss: 0.87561\n",
      "batch 023 / 024 | loss: 0.87188\n",
      "batch 024 / 024 | loss: 0.86711\n",
      "----- epoch 006 / 010 | time: 131 sec | loss: 0.94430 | err: 0.37200\n",
      "batch 001 / 024 | loss: 0.72329\n",
      "batch 002 / 024 | loss: 0.75927\n",
      "batch 003 / 024 | loss: 0.81494\n",
      "batch 004 / 024 | loss: 0.80339\n",
      "batch 005 / 024 | loss: 0.83501\n",
      "batch 006 / 024 | loss: 0.85538\n",
      "batch 007 / 024 | loss: 0.84748\n",
      "batch 008 / 024 | loss: 0.84770\n",
      "batch 009 / 024 | loss: 0.85254\n",
      "batch 010 / 024 | loss: 0.84327\n",
      "batch 011 / 024 | loss: 0.82488\n",
      "batch 012 / 024 | loss: 0.83741\n",
      "batch 013 / 024 | loss: 0.84558\n",
      "batch 014 / 024 | loss: 0.83607\n",
      "batch 015 / 024 | loss: 0.83136\n",
      "batch 016 / 024 | loss: 0.83674\n",
      "batch 017 / 024 | loss: 0.83060\n",
      "batch 018 / 024 | loss: 0.83203\n",
      "batch 019 / 024 | loss: 0.83753\n",
      "batch 020 / 024 | loss: 0.83947\n",
      "batch 021 / 024 | loss: 0.83604\n",
      "batch 022 / 024 | loss: 0.83475\n",
      "batch 023 / 024 | loss: 0.83010\n",
      "batch 024 / 024 | loss: 0.82397\n",
      "----- epoch 007 / 010 | time: 135 sec | loss: 1.13205 | err: 0.33733\n",
      "batch 001 / 024 | loss: 0.83849\n",
      "batch 002 / 024 | loss: 0.77875\n",
      "batch 003 / 024 | loss: 0.82269\n",
      "batch 004 / 024 | loss: 0.83275\n",
      "batch 005 / 024 | loss: 0.80672\n",
      "batch 006 / 024 | loss: 0.81079\n",
      "batch 007 / 024 | loss: 0.80032\n",
      "batch 008 / 024 | loss: 0.80624\n",
      "batch 009 / 024 | loss: 0.82561\n",
      "batch 010 / 024 | loss: 0.81398\n",
      "batch 011 / 024 | loss: 0.79674\n",
      "batch 012 / 024 | loss: 0.78540\n",
      "batch 013 / 024 | loss: 0.78072\n",
      "batch 014 / 024 | loss: 0.79283\n",
      "batch 015 / 024 | loss: 0.79039\n",
      "batch 016 / 024 | loss: 0.78886\n",
      "batch 017 / 024 | loss: 0.78481\n",
      "batch 018 / 024 | loss: 0.79250\n",
      "batch 019 / 024 | loss: 0.80027\n",
      "batch 020 / 024 | loss: 0.80298\n",
      "batch 021 / 024 | loss: 0.80749\n",
      "batch 022 / 024 | loss: 0.80827\n",
      "batch 023 / 024 | loss: 0.80584\n",
      "batch 024 / 024 | loss: 0.81233\n",
      "training time: 1178.140424489975 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.03795861385781316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 28 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 29 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 30 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.79952\n",
      "batch 002 / 024 | loss: 0.89635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 003 / 024 | loss: 0.88683\n",
      "batch 004 / 024 | loss: 0.89273\n",
      "batch 005 / 024 | loss: 0.92010\n",
      "batch 006 / 024 | loss: 0.92080\n",
      "batch 007 / 024 | loss: 0.92015\n",
      "batch 008 / 024 | loss: 0.91029\n",
      "batch 009 / 024 | loss: 0.91444\n",
      "batch 010 / 024 | loss: 0.90928\n",
      "batch 011 / 024 | loss: 0.88874\n",
      "batch 012 / 024 | loss: 0.89249\n",
      "batch 013 / 024 | loss: 0.88930\n",
      "batch 014 / 024 | loss: 0.88888\n",
      "batch 015 / 024 | loss: 0.88459\n",
      "batch 016 / 024 | loss: 0.88321\n",
      "batch 017 / 024 | loss: 0.88003\n",
      "batch 018 / 024 | loss: 0.88781\n",
      "batch 019 / 024 | loss: 0.88814\n",
      "batch 020 / 024 | loss: 0.87981\n",
      "batch 021 / 024 | loss: 0.87463\n",
      "batch 022 / 024 | loss: 0.87378\n",
      "batch 023 / 024 | loss: 0.87637\n",
      "batch 024 / 024 | loss: 0.87095\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 136 sec | loss: 0.97387 | err: 0.34000\n",
      "batch 001 / 024 | loss: 0.76377\n",
      "batch 002 / 024 | loss: 0.80478\n",
      "batch 003 / 024 | loss: 0.80078\n",
      "batch 004 / 024 | loss: 0.79606\n",
      "batch 005 / 024 | loss: 0.82712\n",
      "batch 006 / 024 | loss: 0.85893\n",
      "batch 007 / 024 | loss: 0.85137\n",
      "batch 008 / 024 | loss: 0.83711\n",
      "batch 009 / 024 | loss: 0.83719\n",
      "batch 010 / 024 | loss: 0.83367\n",
      "batch 011 / 024 | loss: 0.83808\n",
      "batch 012 / 024 | loss: 0.83228\n",
      "batch 013 / 024 | loss: 0.82768\n",
      "batch 014 / 024 | loss: 0.82071\n",
      "batch 015 / 024 | loss: 0.81608\n",
      "batch 016 / 024 | loss: 0.81946\n",
      "batch 017 / 024 | loss: 0.81910\n",
      "batch 018 / 024 | loss: 0.81548\n",
      "batch 019 / 024 | loss: 0.82001\n",
      "batch 020 / 024 | loss: 0.82005\n",
      "batch 021 / 024 | loss: 0.82564\n",
      "batch 022 / 024 | loss: 0.82935\n",
      "batch 023 / 024 | loss: 0.82810\n",
      "batch 024 / 024 | loss: 0.83089\n",
      "----- epoch 002 / 010 | time: 132 sec | loss: 0.96273 | err: 0.37467\n",
      "batch 001 / 024 | loss: 0.80349\n",
      "batch 002 / 024 | loss: 0.76729\n",
      "batch 003 / 024 | loss: 0.78510\n",
      "batch 004 / 024 | loss: 0.80483\n",
      "batch 005 / 024 | loss: 0.84049\n",
      "batch 006 / 024 | loss: 0.84538\n",
      "batch 007 / 024 | loss: 0.86482\n",
      "batch 008 / 024 | loss: 0.85486\n",
      "batch 009 / 024 | loss: 0.85773\n",
      "batch 010 / 024 | loss: 0.86612\n",
      "batch 011 / 024 | loss: 0.86568\n",
      "batch 012 / 024 | loss: 0.86740\n",
      "batch 013 / 024 | loss: 0.87242\n",
      "batch 014 / 024 | loss: 0.87144\n",
      "batch 015 / 024 | loss: 0.86404\n",
      "batch 016 / 024 | loss: 0.86926\n",
      "batch 017 / 024 | loss: 0.86813\n",
      "batch 018 / 024 | loss: 0.87367\n",
      "batch 019 / 024 | loss: 0.86506\n",
      "batch 020 / 024 | loss: 0.86636\n",
      "batch 021 / 024 | loss: 0.86657\n",
      "batch 022 / 024 | loss: 0.87397\n",
      "batch 023 / 024 | loss: 0.87277\n",
      "batch 024 / 024 | loss: 0.87050\n",
      "----- epoch 003 / 010 | time: 129 sec | loss: 0.86492 | err: 0.40667\n",
      "batch 001 / 024 | loss: 0.69293\n",
      "batch 002 / 024 | loss: 0.83171\n",
      "batch 003 / 024 | loss: 0.83346\n",
      "batch 004 / 024 | loss: 0.87884\n",
      "batch 005 / 024 | loss: 0.86472\n",
      "batch 006 / 024 | loss: 0.83976\n",
      "batch 007 / 024 | loss: 0.83847\n",
      "batch 008 / 024 | loss: 0.83617\n",
      "batch 009 / 024 | loss: 0.81371\n",
      "batch 010 / 024 | loss: 0.82672\n",
      "batch 011 / 024 | loss: 0.83522\n",
      "batch 012 / 024 | loss: 0.84182\n",
      "batch 013 / 024 | loss: 0.83417\n",
      "batch 014 / 024 | loss: 0.84041\n",
      "batch 015 / 024 | loss: 0.84628\n",
      "batch 016 / 024 | loss: 0.85394\n",
      "batch 017 / 024 | loss: 0.85530\n",
      "batch 018 / 024 | loss: 0.85886\n",
      "batch 019 / 024 | loss: 0.85845\n",
      "batch 020 / 024 | loss: 0.86221\n",
      "batch 021 / 024 | loss: 0.87173\n",
      "batch 022 / 024 | loss: 0.86704\n",
      "batch 023 / 024 | loss: 0.86459\n",
      "batch 024 / 024 | loss: 0.86813\n",
      "----- epoch 004 / 010 | time: 132 sec | loss: 0.92846 | err: 0.35333\n",
      "batch 001 / 024 | loss: 0.99048\n",
      "batch 002 / 024 | loss: 0.91195\n",
      "batch 003 / 024 | loss: 0.93986\n",
      "batch 004 / 024 | loss: 0.86392\n",
      "batch 005 / 024 | loss: 0.87265\n",
      "batch 006 / 024 | loss: 0.86040\n",
      "batch 007 / 024 | loss: 0.86070\n",
      "batch 008 / 024 | loss: 0.85178\n",
      "batch 009 / 024 | loss: 0.85965\n",
      "batch 010 / 024 | loss: 0.84212\n",
      "batch 011 / 024 | loss: 0.83943\n",
      "batch 012 / 024 | loss: 0.84931\n",
      "batch 013 / 024 | loss: 0.83434\n",
      "batch 014 / 024 | loss: 0.83003\n",
      "batch 015 / 024 | loss: 0.82675\n",
      "batch 016 / 024 | loss: 0.82837\n",
      "batch 017 / 024 | loss: 0.82445\n",
      "batch 018 / 024 | loss: 0.82446\n",
      "batch 019 / 024 | loss: 0.82726\n",
      "batch 020 / 024 | loss: 0.83367\n",
      "batch 021 / 024 | loss: 0.83863\n",
      "batch 022 / 024 | loss: 0.84293\n",
      "batch 023 / 024 | loss: 0.84055\n",
      "batch 024 / 024 | loss: 0.83709\n",
      "----- epoch 005 / 010 | time: 138 sec | loss: 0.95573 | err: 0.36800\n",
      "batch 001 / 024 | loss: 0.83146\n",
      "batch 002 / 024 | loss: 0.78641\n",
      "batch 003 / 024 | loss: 0.81401\n",
      "batch 004 / 024 | loss: 0.81852\n",
      "batch 005 / 024 | loss: 0.83498\n",
      "batch 006 / 024 | loss: 0.86426\n",
      "batch 007 / 024 | loss: 0.86920\n",
      "batch 008 / 024 | loss: 0.88732\n",
      "batch 009 / 024 | loss: 0.87647\n",
      "batch 010 / 024 | loss: 0.86342\n",
      "batch 011 / 024 | loss: 0.85984\n",
      "batch 012 / 024 | loss: 0.86385\n",
      "batch 013 / 024 | loss: 0.87496\n",
      "batch 014 / 024 | loss: 0.87891\n",
      "batch 015 / 024 | loss: 0.87421\n",
      "batch 016 / 024 | loss: 0.87375\n",
      "batch 017 / 024 | loss: 0.87065\n",
      "batch 018 / 024 | loss: 0.88135\n",
      "batch 019 / 024 | loss: 0.88174\n",
      "batch 020 / 024 | loss: 0.86669\n",
      "batch 021 / 024 | loss: 0.86579\n",
      "batch 022 / 024 | loss: 0.86678\n",
      "batch 023 / 024 | loss: 0.86279\n",
      "batch 024 / 024 | loss: 0.85988\n",
      "training time: 805.1642413139343 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.044846043921615494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 31 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 32 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 33 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.80269\n",
      "batch 002 / 024 | loss: 0.90042\n",
      "batch 003 / 024 | loss: 0.88974\n",
      "batch 004 / 024 | loss: 0.89497\n",
      "batch 005 / 024 | loss: 0.92210\n",
      "batch 006 / 024 | loss: 0.92294\n",
      "batch 007 / 024 | loss: 0.92234\n",
      "batch 008 / 024 | loss: 0.91265\n",
      "batch 009 / 024 | loss: 0.91697\n",
      "batch 010 / 024 | loss: 0.91194\n",
      "batch 011 / 024 | loss: 0.89285\n",
      "batch 012 / 024 | loss: 0.89901\n",
      "batch 013 / 024 | loss: 0.89620\n",
      "batch 014 / 024 | loss: 0.89952\n",
      "batch 015 / 024 | loss: 0.90073\n",
      "batch 016 / 024 | loss: 0.90024\n",
      "batch 017 / 024 | loss: 0.90291\n",
      "batch 018 / 024 | loss: 0.91172\n",
      "batch 019 / 024 | loss: 0.91377\n",
      "batch 020 / 024 | loss: 0.91031\n",
      "batch 021 / 024 | loss: 0.90630\n",
      "batch 022 / 024 | loss: 0.90648\n",
      "batch 023 / 024 | loss: 0.90819\n",
      "batch 024 / 024 | loss: 0.90319\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 139 sec | loss: 0.88000 | err: 0.34400\n",
      "batch 001 / 024 | loss: 0.78489\n",
      "batch 002 / 024 | loss: 0.82866\n",
      "batch 003 / 024 | loss: 0.83804\n",
      "batch 004 / 024 | loss: 0.82290\n",
      "batch 005 / 024 | loss: 0.83974\n",
      "batch 006 / 024 | loss: 0.85965\n",
      "batch 007 / 024 | loss: 0.84470\n",
      "batch 008 / 024 | loss: 0.83042\n",
      "batch 009 / 024 | loss: 0.82934\n",
      "batch 010 / 024 | loss: 0.82478\n",
      "batch 011 / 024 | loss: 0.83710\n",
      "batch 012 / 024 | loss: 0.83497\n",
      "batch 013 / 024 | loss: 0.83420\n",
      "batch 014 / 024 | loss: 0.83260\n",
      "batch 015 / 024 | loss: 0.82852\n",
      "batch 016 / 024 | loss: 0.82745\n",
      "batch 017 / 024 | loss: 0.82933\n",
      "batch 018 / 024 | loss: 0.82564\n",
      "batch 019 / 024 | loss: 0.82843\n",
      "batch 020 / 024 | loss: 0.82647\n",
      "batch 021 / 024 | loss: 0.83080\n",
      "batch 022 / 024 | loss: 0.83464\n",
      "batch 023 / 024 | loss: 0.83092\n",
      "batch 024 / 024 | loss: 0.83291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- epoch 002 / 010 | time: 142 sec | loss: 0.99052 | err: 0.36667\n",
      "batch 001 / 024 | loss: 0.78693\n",
      "batch 002 / 024 | loss: 0.76363\n",
      "batch 003 / 024 | loss: 0.78976\n",
      "batch 004 / 024 | loss: 0.81015\n",
      "batch 005 / 024 | loss: 0.84852\n",
      "batch 006 / 024 | loss: 0.85269\n",
      "batch 007 / 024 | loss: 0.86827\n",
      "batch 008 / 024 | loss: 0.85811\n",
      "batch 009 / 024 | loss: 0.86135\n",
      "batch 010 / 024 | loss: 0.86874\n",
      "batch 011 / 024 | loss: 0.86858\n",
      "batch 012 / 024 | loss: 0.87007\n",
      "batch 013 / 024 | loss: 0.87555\n",
      "batch 014 / 024 | loss: 0.87404\n",
      "batch 015 / 024 | loss: 0.86691\n",
      "batch 016 / 024 | loss: 0.87168\n",
      "batch 017 / 024 | loss: 0.86992\n",
      "batch 018 / 024 | loss: 0.87520\n",
      "batch 019 / 024 | loss: 0.86668\n",
      "batch 020 / 024 | loss: 0.86797\n",
      "batch 021 / 024 | loss: 0.86831\n",
      "batch 022 / 024 | loss: 0.87586\n",
      "batch 023 / 024 | loss: 0.87494\n",
      "batch 024 / 024 | loss: 0.87257\n",
      "----- epoch 003 / 010 | time: 132 sec | loss: 0.90585 | err: 0.44133\n",
      "batch 001 / 024 | loss: 0.68584\n",
      "batch 002 / 024 | loss: 0.82454\n",
      "batch 003 / 024 | loss: 0.82756\n",
      "batch 004 / 024 | loss: 0.87588\n",
      "batch 005 / 024 | loss: 0.86356\n",
      "batch 006 / 024 | loss: 0.83634\n",
      "batch 007 / 024 | loss: 0.83745\n",
      "batch 008 / 024 | loss: 0.83640\n",
      "batch 009 / 024 | loss: 0.81357\n",
      "batch 010 / 024 | loss: 0.82751\n",
      "batch 011 / 024 | loss: 0.83686\n",
      "batch 012 / 024 | loss: 0.84505\n",
      "batch 013 / 024 | loss: 0.83733\n",
      "batch 014 / 024 | loss: 0.84469\n",
      "batch 015 / 024 | loss: 0.85022\n",
      "batch 016 / 024 | loss: 0.85691\n",
      "batch 017 / 024 | loss: 0.85899\n",
      "batch 018 / 024 | loss: 0.86321\n",
      "batch 019 / 024 | loss: 0.86629\n",
      "batch 020 / 024 | loss: 0.87142\n",
      "batch 021 / 024 | loss: 0.88099\n",
      "batch 022 / 024 | loss: 0.87594\n",
      "batch 023 / 024 | loss: 0.87416\n",
      "batch 024 / 024 | loss: 0.87770\n",
      "----- epoch 004 / 010 | time: 139 sec | loss: 0.82564 | err: 0.40000\n",
      "batch 001 / 024 | loss: 0.98584\n",
      "batch 002 / 024 | loss: 0.92713\n",
      "batch 003 / 024 | loss: 0.95510\n",
      "batch 004 / 024 | loss: 0.88558\n",
      "batch 005 / 024 | loss: 0.89218\n",
      "batch 006 / 024 | loss: 0.87581\n",
      "batch 007 / 024 | loss: 0.87550\n",
      "batch 008 / 024 | loss: 0.86603\n",
      "batch 009 / 024 | loss: 0.87420\n",
      "batch 010 / 024 | loss: 0.85340\n",
      "batch 011 / 024 | loss: 0.84977\n",
      "batch 012 / 024 | loss: 0.85871\n",
      "batch 013 / 024 | loss: 0.84073\n",
      "batch 014 / 024 | loss: 0.83487\n",
      "batch 015 / 024 | loss: 0.82940\n",
      "batch 016 / 024 | loss: 0.83067\n",
      "batch 017 / 024 | loss: 0.82606\n",
      "batch 018 / 024 | loss: 0.82549\n",
      "batch 019 / 024 | loss: 0.82817\n",
      "batch 020 / 024 | loss: 0.83523\n",
      "batch 021 / 024 | loss: 0.84092\n",
      "batch 022 / 024 | loss: 0.84548\n",
      "batch 023 / 024 | loss: 0.84362\n",
      "batch 024 / 024 | loss: 0.84200\n",
      "----- epoch 005 / 010 | time: 142 sec | loss: 0.90479 | err: 0.38400\n",
      "batch 001 / 024 | loss: 0.84850\n",
      "batch 002 / 024 | loss: 0.79833\n",
      "batch 003 / 024 | loss: 0.82214\n",
      "batch 004 / 024 | loss: 0.82431\n",
      "batch 005 / 024 | loss: 0.84298\n",
      "batch 006 / 024 | loss: 0.86468\n",
      "batch 007 / 024 | loss: 0.86663\n",
      "batch 008 / 024 | loss: 0.88883\n",
      "batch 009 / 024 | loss: 0.87617\n",
      "batch 010 / 024 | loss: 0.85934\n",
      "batch 011 / 024 | loss: 0.85389\n",
      "batch 012 / 024 | loss: 0.85459\n",
      "batch 013 / 024 | loss: 0.86522\n",
      "batch 014 / 024 | loss: 0.86989\n",
      "batch 015 / 024 | loss: 0.86542\n",
      "batch 016 / 024 | loss: 0.86456\n",
      "batch 017 / 024 | loss: 0.85871\n",
      "batch 018 / 024 | loss: 0.86541\n",
      "batch 019 / 024 | loss: 0.86705\n",
      "batch 020 / 024 | loss: 0.85070\n",
      "batch 021 / 024 | loss: 0.84786\n",
      "batch 022 / 024 | loss: 0.84850\n",
      "batch 023 / 024 | loss: 0.84079\n",
      "batch 024 / 024 | loss: 0.83854\n",
      "training time: 833.9987523555756 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.0529831690628371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 34 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 35 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 36 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.80643\n",
      "batch 002 / 024 | loss: 0.90523\n",
      "batch 003 / 024 | loss: 0.89316\n",
      "batch 004 / 024 | loss: 0.89760\n",
      "batch 005 / 024 | loss: 0.92443\n",
      "batch 006 / 024 | loss: 0.92541\n",
      "batch 007 / 024 | loss: 0.92487\n",
      "batch 008 / 024 | loss: 0.91526\n",
      "batch 009 / 024 | loss: 0.91965\n",
      "batch 010 / 024 | loss: 0.91461\n",
      "batch 011 / 024 | loss: 0.89390\n",
      "batch 012 / 024 | loss: 0.90033\n",
      "batch 013 / 024 | loss: 0.89780\n",
      "batch 014 / 024 | loss: 0.90143\n",
      "batch 015 / 024 | loss: 0.90172\n",
      "batch 016 / 024 | loss: 0.90115\n",
      "batch 017 / 024 | loss: 0.90334\n",
      "batch 018 / 024 | loss: 0.91184\n",
      "batch 019 / 024 | loss: 0.91465\n",
      "batch 020 / 024 | loss: 0.91123\n",
      "batch 021 / 024 | loss: 0.90707\n",
      "batch 022 / 024 | loss: 0.90711\n",
      "batch 023 / 024 | loss: 0.90858\n",
      "batch 024 / 024 | loss: 0.90419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved!\n",
      "----- epoch 001 / 010 | time: 139 sec | loss: 0.88779 | err: 0.36267\n",
      "batch 001 / 024 | loss: 0.80508\n",
      "batch 002 / 024 | loss: 0.84598\n",
      "batch 003 / 024 | loss: 0.84740\n",
      "batch 004 / 024 | loss: 0.83182\n",
      "batch 005 / 024 | loss: 0.84770\n",
      "batch 006 / 024 | loss: 0.87062\n",
      "batch 007 / 024 | loss: 0.85466\n",
      "batch 008 / 024 | loss: 0.83977\n",
      "batch 009 / 024 | loss: 0.83924\n",
      "batch 010 / 024 | loss: 0.83391\n",
      "batch 011 / 024 | loss: 0.84241\n",
      "batch 012 / 024 | loss: 0.84108\n",
      "batch 013 / 024 | loss: 0.84155\n",
      "batch 014 / 024 | loss: 0.83989\n",
      "batch 015 / 024 | loss: 0.83916\n",
      "batch 016 / 024 | loss: 0.83983\n",
      "batch 017 / 024 | loss: 0.84285\n",
      "batch 018 / 024 | loss: 0.83987\n",
      "batch 019 / 024 | loss: 0.84190\n",
      "batch 020 / 024 | loss: 0.84112\n",
      "batch 021 / 024 | loss: 0.84573\n",
      "batch 022 / 024 | loss: 0.84882\n",
      "batch 023 / 024 | loss: 0.84707\n",
      "batch 024 / 024 | loss: 0.84853\n",
      "----- epoch 002 / 010 | time: 138 sec | loss: 0.95703 | err: 0.38000\n",
      "batch 001 / 024 | loss: 0.79343\n",
      "batch 002 / 024 | loss: 0.76265\n",
      "batch 003 / 024 | loss: 0.78998\n",
      "batch 004 / 024 | loss: 0.80899\n",
      "batch 005 / 024 | loss: 0.84773\n",
      "batch 006 / 024 | loss: 0.85179\n",
      "batch 007 / 024 | loss: 0.86948\n",
      "batch 008 / 024 | loss: 0.85742\n",
      "batch 009 / 024 | loss: 0.86130\n",
      "batch 010 / 024 | loss: 0.86829\n",
      "batch 011 / 024 | loss: 0.86875\n",
      "batch 012 / 024 | loss: 0.87052\n",
      "batch 013 / 024 | loss: 0.87632\n",
      "batch 014 / 024 | loss: 0.87495\n",
      "batch 015 / 024 | loss: 0.86740\n",
      "batch 016 / 024 | loss: 0.87232\n",
      "batch 017 / 024 | loss: 0.87081\n",
      "batch 018 / 024 | loss: 0.87634\n",
      "batch 019 / 024 | loss: 0.86753\n",
      "batch 020 / 024 | loss: 0.86878\n",
      "batch 021 / 024 | loss: 0.86929\n",
      "batch 022 / 024 | loss: 0.87672\n",
      "batch 023 / 024 | loss: 0.87609\n",
      "batch 024 / 024 | loss: 0.87384\n",
      "----- epoch 003 / 010 | time: 133 sec | loss: 0.92108 | err: 0.45467\n",
      "batch 001 / 024 | loss: 0.68340\n",
      "batch 002 / 024 | loss: 0.82596\n",
      "batch 003 / 024 | loss: 0.82901\n",
      "batch 004 / 024 | loss: 0.87771\n",
      "batch 005 / 024 | loss: 0.86465\n",
      "batch 006 / 024 | loss: 0.83773\n",
      "batch 007 / 024 | loss: 0.83864\n",
      "batch 008 / 024 | loss: 0.83824\n",
      "batch 009 / 024 | loss: 0.81498\n",
      "batch 010 / 024 | loss: 0.82904\n",
      "batch 011 / 024 | loss: 0.83825\n",
      "batch 012 / 024 | loss: 0.84612\n",
      "batch 013 / 024 | loss: 0.83821\n",
      "batch 014 / 024 | loss: 0.84564\n",
      "batch 015 / 024 | loss: 0.85140\n",
      "batch 016 / 024 | loss: 0.85790\n",
      "batch 017 / 024 | loss: 0.86058\n",
      "batch 018 / 024 | loss: 0.86532\n",
      "batch 019 / 024 | loss: 0.86882\n",
      "batch 020 / 024 | loss: 0.87375\n",
      "batch 021 / 024 | loss: 0.88314\n",
      "batch 022 / 024 | loss: 0.87791\n",
      "batch 023 / 024 | loss: 0.87614\n",
      "batch 024 / 024 | loss: 0.87982\n",
      "----- epoch 004 / 010 | time: 129 sec | loss: 0.84593 | err: 0.40400\n",
      "batch 001 / 024 | loss: 0.99947\n",
      "batch 002 / 024 | loss: 0.93196\n",
      "batch 003 / 024 | loss: 0.96379\n",
      "batch 004 / 024 | loss: 0.88870\n",
      "batch 005 / 024 | loss: 0.89727\n",
      "batch 006 / 024 | loss: 0.87963\n",
      "batch 007 / 024 | loss: 0.88192\n",
      "batch 008 / 024 | loss: 0.87168\n",
      "batch 009 / 024 | loss: 0.88301\n",
      "batch 010 / 024 | loss: 0.87123\n",
      "batch 011 / 024 | loss: 0.86803\n",
      "batch 012 / 024 | loss: 0.87545\n",
      "batch 013 / 024 | loss: 0.85993\n",
      "batch 014 / 024 | loss: 0.85427\n",
      "batch 015 / 024 | loss: 0.85000\n",
      "batch 016 / 024 | loss: 0.84851\n",
      "batch 017 / 024 | loss: 0.84383\n",
      "batch 018 / 024 | loss: 0.84386\n",
      "batch 019 / 024 | loss: 0.84606\n",
      "batch 020 / 024 | loss: 0.85239\n",
      "batch 021 / 024 | loss: 0.85760\n",
      "batch 022 / 024 | loss: 0.86160\n",
      "batch 023 / 024 | loss: 0.85915\n",
      "batch 024 / 024 | loss: 0.85649\n",
      "----- epoch 005 / 010 | time: 136 sec | loss: 0.96070 | err: 0.37733\n",
      "batch 001 / 024 | loss: 0.86125\n",
      "batch 002 / 024 | loss: 0.80176\n",
      "batch 003 / 024 | loss: 0.82436\n",
      "batch 004 / 024 | loss: 0.82669\n",
      "batch 005 / 024 | loss: 0.84781\n",
      "batch 006 / 024 | loss: 0.87083\n",
      "batch 007 / 024 | loss: 0.87237\n",
      "batch 008 / 024 | loss: 0.89314\n",
      "batch 009 / 024 | loss: 0.88184\n",
      "batch 010 / 024 | loss: 0.86407\n",
      "batch 011 / 024 | loss: 0.86141\n",
      "batch 012 / 024 | loss: 0.86408\n",
      "batch 013 / 024 | loss: 0.87595\n",
      "batch 014 / 024 | loss: 0.88111\n",
      "batch 015 / 024 | loss: 0.87723\n",
      "batch 016 / 024 | loss: 0.87728\n",
      "batch 017 / 024 | loss: 0.87474\n",
      "batch 018 / 024 | loss: 0.88558\n",
      "batch 019 / 024 | loss: 0.88679\n",
      "batch 020 / 024 | loss: 0.87153\n",
      "batch 021 / 024 | loss: 0.87100\n",
      "batch 022 / 024 | loss: 0.87280\n",
      "batch 023 / 024 | loss: 0.87008\n",
      "batch 024 / 024 | loss: 0.86808\n",
      "training time: 824.6288256645203 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.06259674117181421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 37 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 38 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 39 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.81085\n",
      "batch 002 / 024 | loss: 0.91092\n",
      "batch 003 / 024 | loss: 0.89719\n",
      "batch 004 / 024 | loss: 0.90070\n",
      "batch 005 / 024 | loss: 0.92714\n",
      "batch 006 / 024 | loss: 0.92826\n",
      "batch 007 / 024 | loss: 0.92776\n",
      "batch 008 / 024 | loss: 0.91832\n",
      "batch 009 / 024 | loss: 0.92282\n",
      "batch 010 / 024 | loss: 0.91791\n",
      "batch 011 / 024 | loss: 0.89826\n",
      "batch 012 / 024 | loss: 0.90465\n",
      "batch 013 / 024 | loss: 0.90188\n",
      "batch 014 / 024 | loss: 0.90554\n",
      "batch 015 / 024 | loss: 0.90613\n",
      "batch 016 / 024 | loss: 0.90557\n",
      "batch 017 / 024 | loss: 0.90747\n",
      "batch 018 / 024 | loss: 0.91632\n",
      "batch 019 / 024 | loss: 0.91945\n",
      "batch 020 / 024 | loss: 0.91503\n",
      "batch 021 / 024 | loss: 0.91092\n",
      "batch 022 / 024 | loss: 0.91050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 023 / 024 | loss: 0.91218\n",
      "batch 024 / 024 | loss: 0.90758\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 148 sec | loss: 0.89895 | err: 0.36267\n",
      "batch 001 / 024 | loss: 0.81898\n",
      "batch 002 / 024 | loss: 0.86341\n",
      "batch 003 / 024 | loss: 0.85256\n",
      "batch 004 / 024 | loss: 0.83958\n",
      "batch 005 / 024 | loss: 0.86110\n",
      "batch 006 / 024 | loss: 0.88758\n",
      "batch 007 / 024 | loss: 0.88122\n",
      "batch 008 / 024 | loss: 0.86461\n",
      "batch 009 / 024 | loss: 0.86577\n",
      "batch 010 / 024 | loss: 0.86775\n",
      "batch 011 / 024 | loss: 0.87915\n",
      "batch 012 / 024 | loss: 0.87758\n",
      "batch 013 / 024 | loss: 0.87485\n",
      "batch 014 / 024 | loss: 0.87296\n",
      "batch 015 / 024 | loss: 0.87459\n",
      "batch 016 / 024 | loss: 0.87403\n",
      "batch 017 / 024 | loss: 0.87418\n",
      "batch 018 / 024 | loss: 0.87052\n",
      "batch 019 / 024 | loss: 0.87184\n",
      "batch 020 / 024 | loss: 0.87101\n",
      "batch 021 / 024 | loss: 0.87474\n",
      "batch 022 / 024 | loss: 0.87647\n",
      "batch 023 / 024 | loss: 0.87438\n",
      "batch 024 / 024 | loss: 0.87469\n",
      "----- epoch 002 / 010 | time: 133 sec | loss: 1.00939 | err: 0.41600\n",
      "batch 001 / 024 | loss: 0.78957\n",
      "batch 002 / 024 | loss: 0.76773\n",
      "batch 003 / 024 | loss: 0.79650\n",
      "batch 004 / 024 | loss: 0.81487\n",
      "batch 005 / 024 | loss: 0.85217\n",
      "batch 006 / 024 | loss: 0.85534\n",
      "batch 007 / 024 | loss: 0.87404\n",
      "batch 008 / 024 | loss: 0.86432\n",
      "batch 009 / 024 | loss: 0.86763\n",
      "batch 010 / 024 | loss: 0.87455\n",
      "batch 011 / 024 | loss: 0.87436\n",
      "batch 012 / 024 | loss: 0.87528\n",
      "batch 013 / 024 | loss: 0.88125\n",
      "batch 014 / 024 | loss: 0.87974\n",
      "batch 015 / 024 | loss: 0.87168\n",
      "batch 016 / 024 | loss: 0.87638\n",
      "batch 017 / 024 | loss: 0.87517\n",
      "batch 018 / 024 | loss: 0.88103\n",
      "batch 019 / 024 | loss: 0.87154\n",
      "batch 020 / 024 | loss: 0.87274\n",
      "batch 021 / 024 | loss: 0.87334\n",
      "batch 022 / 024 | loss: 0.88065\n",
      "batch 023 / 024 | loss: 0.88007\n",
      "batch 024 / 024 | loss: 0.87751\n",
      "----- epoch 003 / 010 | time: 135 sec | loss: 0.91754 | err: 0.46133\n",
      "batch 001 / 024 | loss: 0.68712\n",
      "batch 002 / 024 | loss: 0.82656\n",
      "batch 003 / 024 | loss: 0.82974\n",
      "batch 004 / 024 | loss: 0.87827\n",
      "batch 005 / 024 | loss: 0.86576\n",
      "batch 006 / 024 | loss: 0.83958\n",
      "batch 007 / 024 | loss: 0.84086\n",
      "batch 008 / 024 | loss: 0.84129\n",
      "batch 009 / 024 | loss: 0.81751\n",
      "batch 010 / 024 | loss: 0.83131\n",
      "batch 011 / 024 | loss: 0.84039\n",
      "batch 012 / 024 | loss: 0.84839\n",
      "batch 013 / 024 | loss: 0.84003\n",
      "batch 014 / 024 | loss: 0.84718\n",
      "batch 015 / 024 | loss: 0.85297\n",
      "batch 016 / 024 | loss: 0.85941\n",
      "batch 017 / 024 | loss: 0.86248\n",
      "batch 018 / 024 | loss: 0.86736\n",
      "batch 019 / 024 | loss: 0.87138\n",
      "batch 020 / 024 | loss: 0.87671\n",
      "batch 021 / 024 | loss: 0.88627\n",
      "batch 022 / 024 | loss: 0.88066\n",
      "batch 023 / 024 | loss: 0.87877\n",
      "batch 024 / 024 | loss: 0.88238\n",
      "----- epoch 004 / 010 | time: 151 sec | loss: 0.88487 | err: 0.43067\n",
      "batch 001 / 024 | loss: 1.00964\n",
      "batch 002 / 024 | loss: 0.93703\n",
      "batch 003 / 024 | loss: 0.97072\n",
      "batch 004 / 024 | loss: 0.89393\n",
      "batch 005 / 024 | loss: 0.90238\n",
      "batch 006 / 024 | loss: 0.88377\n",
      "batch 007 / 024 | loss: 0.88766\n",
      "batch 008 / 024 | loss: 0.87595\n",
      "batch 009 / 024 | loss: 0.88815\n",
      "batch 010 / 024 | loss: 0.88113\n",
      "batch 011 / 024 | loss: 0.88029\n",
      "batch 012 / 024 | loss: 0.88955\n",
      "batch 013 / 024 | loss: 0.87946\n",
      "batch 014 / 024 | loss: 0.87536\n",
      "batch 015 / 024 | loss: 0.87752\n",
      "batch 016 / 024 | loss: 0.88145\n",
      "batch 017 / 024 | loss: 0.87519\n",
      "batch 018 / 024 | loss: 0.87371\n",
      "batch 019 / 024 | loss: 0.87530\n",
      "batch 020 / 024 | loss: 0.88082\n",
      "batch 021 / 024 | loss: 0.88482\n",
      "batch 022 / 024 | loss: 0.88798\n",
      "batch 023 / 024 | loss: 0.88532\n",
      "batch 024 / 024 | loss: 0.88164\n",
      "----- epoch 005 / 010 | time: 136 sec | loss: 0.98576 | err: 0.43200\n",
      "batch 001 / 024 | loss: 0.86775\n",
      "batch 002 / 024 | loss: 0.81162\n",
      "batch 003 / 024 | loss: 0.85211\n",
      "batch 004 / 024 | loss: 0.85838\n",
      "batch 005 / 024 | loss: 0.86564\n",
      "batch 006 / 024 | loss: 0.88883\n",
      "batch 007 / 024 | loss: 0.89018\n",
      "batch 008 / 024 | loss: 0.90589\n",
      "batch 009 / 024 | loss: 0.90010\n",
      "batch 010 / 024 | loss: 0.88837\n",
      "batch 011 / 024 | loss: 0.88897\n",
      "batch 012 / 024 | loss: 0.89494\n",
      "batch 013 / 024 | loss: 0.90233\n",
      "batch 014 / 024 | loss: 0.90444\n",
      "batch 015 / 024 | loss: 0.89973\n",
      "batch 016 / 024 | loss: 0.89986\n",
      "batch 017 / 024 | loss: 0.89642\n",
      "batch 018 / 024 | loss: 0.90518\n",
      "batch 019 / 024 | loss: 0.90563\n",
      "batch 020 / 024 | loss: 0.89067\n",
      "batch 021 / 024 | loss: 0.88911\n",
      "batch 022 / 024 | loss: 0.89162\n",
      "batch 023 / 024 | loss: 0.88773\n",
      "batch 024 / 024 | loss: 0.88511\n",
      "training time: 843.184164762497 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.07395465531108579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 40 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 41 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 42 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.81608\n",
      "batch 002 / 024 | loss: 0.91763\n",
      "batch 003 / 024 | loss: 0.90195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 004 / 024 | loss: 0.90435\n",
      "batch 005 / 024 | loss: 0.93032\n",
      "batch 006 / 024 | loss: 0.93129\n",
      "batch 007 / 024 | loss: 0.93041\n",
      "batch 008 / 024 | loss: 0.92048\n",
      "batch 009 / 024 | loss: 0.92477\n",
      "batch 010 / 024 | loss: 0.91954\n",
      "batch 011 / 024 | loss: 0.90361\n",
      "batch 012 / 024 | loss: 0.91092\n",
      "batch 013 / 024 | loss: 0.90908\n",
      "batch 014 / 024 | loss: 0.91432\n",
      "batch 015 / 024 | loss: 0.91501\n",
      "batch 016 / 024 | loss: 0.91551\n",
      "batch 017 / 024 | loss: 0.91875\n",
      "batch 018 / 024 | loss: 0.92842\n",
      "batch 019 / 024 | loss: 0.93084\n",
      "batch 020 / 024 | loss: 0.92723\n",
      "batch 021 / 024 | loss: 0.92323\n",
      "batch 022 / 024 | loss: 0.92404\n",
      "batch 023 / 024 | loss: 0.92555\n",
      "batch 024 / 024 | loss: 0.92073\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 145 sec | loss: 0.86588 | err: 0.40533\n",
      "batch 001 / 024 | loss: 0.85915\n",
      "batch 002 / 024 | loss: 0.89726\n",
      "batch 003 / 024 | loss: 0.90616\n",
      "batch 004 / 024 | loss: 0.89135\n",
      "batch 005 / 024 | loss: 0.90629\n",
      "batch 006 / 024 | loss: 0.92474\n",
      "batch 007 / 024 | loss: 0.91826\n",
      "batch 008 / 024 | loss: 0.90366\n",
      "batch 009 / 024 | loss: 0.90128\n",
      "batch 010 / 024 | loss: 0.89812\n",
      "batch 011 / 024 | loss: 0.90437\n",
      "batch 012 / 024 | loss: 0.90184\n",
      "batch 013 / 024 | loss: 0.89580\n",
      "batch 014 / 024 | loss: 0.89215\n",
      "batch 015 / 024 | loss: 0.89494\n",
      "batch 016 / 024 | loss: 0.89228\n",
      "batch 017 / 024 | loss: 0.89171\n",
      "batch 018 / 024 | loss: 0.88741\n",
      "batch 019 / 024 | loss: 0.88974\n",
      "batch 020 / 024 | loss: 0.88790\n",
      "batch 021 / 024 | loss: 0.88984\n",
      "batch 022 / 024 | loss: 0.89032\n",
      "batch 023 / 024 | loss: 0.88851\n",
      "batch 024 / 024 | loss: 0.88776\n",
      "----- epoch 002 / 010 | time: 136 sec | loss: 0.97045 | err: 0.45067\n",
      "batch 001 / 024 | loss: 0.79346\n",
      "batch 002 / 024 | loss: 0.78133\n",
      "batch 003 / 024 | loss: 0.79874\n",
      "batch 004 / 024 | loss: 0.81609\n",
      "batch 005 / 024 | loss: 0.85327\n",
      "batch 006 / 024 | loss: 0.85842\n",
      "batch 007 / 024 | loss: 0.87495\n",
      "batch 008 / 024 | loss: 0.86835\n",
      "batch 009 / 024 | loss: 0.87167\n",
      "batch 010 / 024 | loss: 0.87950\n",
      "batch 011 / 024 | loss: 0.87767\n",
      "batch 012 / 024 | loss: 0.87800\n",
      "batch 013 / 024 | loss: 0.88388\n",
      "batch 014 / 024 | loss: 0.88228\n",
      "batch 015 / 024 | loss: 0.87417\n",
      "batch 016 / 024 | loss: 0.87850\n",
      "batch 017 / 024 | loss: 0.87696\n",
      "batch 018 / 024 | loss: 0.88215\n",
      "batch 019 / 024 | loss: 0.87280\n",
      "batch 020 / 024 | loss: 0.87379\n",
      "batch 021 / 024 | loss: 0.87441\n",
      "batch 022 / 024 | loss: 0.88230\n",
      "batch 023 / 024 | loss: 0.88184\n",
      "batch 024 / 024 | loss: 0.87887\n",
      "----- epoch 003 / 010 | time: 138 sec | loss: 0.92369 | err: 0.46267\n",
      "batch 001 / 024 | loss: 0.68519\n",
      "batch 002 / 024 | loss: 0.82912\n",
      "batch 003 / 024 | loss: 0.83226\n",
      "batch 004 / 024 | loss: 0.88203\n",
      "batch 005 / 024 | loss: 0.86897\n",
      "batch 006 / 024 | loss: 0.84235\n",
      "batch 007 / 024 | loss: 0.84341\n",
      "batch 008 / 024 | loss: 0.84339\n",
      "batch 009 / 024 | loss: 0.81957\n",
      "batch 010 / 024 | loss: 0.83340\n",
      "batch 011 / 024 | loss: 0.84243\n",
      "batch 012 / 024 | loss: 0.85063\n",
      "batch 013 / 024 | loss: 0.84238\n",
      "batch 014 / 024 | loss: 0.84938\n",
      "batch 015 / 024 | loss: 0.85528\n",
      "batch 016 / 024 | loss: 0.86191\n",
      "batch 017 / 024 | loss: 0.86539\n",
      "batch 018 / 024 | loss: 0.87000\n",
      "batch 019 / 024 | loss: 0.87418\n",
      "batch 020 / 024 | loss: 0.87958\n",
      "batch 021 / 024 | loss: 0.88937\n",
      "batch 022 / 024 | loss: 0.88385\n",
      "batch 023 / 024 | loss: 0.88194\n",
      "batch 024 / 024 | loss: 0.88552\n",
      "----- epoch 004 / 010 | time: 126 sec | loss: 0.90558 | err: 0.45067\n",
      "batch 001 / 024 | loss: 1.01953\n",
      "batch 002 / 024 | loss: 0.94146\n",
      "batch 003 / 024 | loss: 0.97114\n",
      "batch 004 / 024 | loss: 0.89820\n",
      "batch 005 / 024 | loss: 0.90985\n",
      "batch 006 / 024 | loss: 0.89199\n",
      "batch 007 / 024 | loss: 0.89454\n",
      "batch 008 / 024 | loss: 0.88313\n",
      "batch 009 / 024 | loss: 0.89436\n",
      "batch 010 / 024 | loss: 0.88652\n",
      "batch 011 / 024 | loss: 0.88509\n",
      "batch 012 / 024 | loss: 0.89598\n",
      "batch 013 / 024 | loss: 0.88587\n",
      "batch 014 / 024 | loss: 0.88186\n",
      "batch 015 / 024 | loss: 0.88435\n",
      "batch 016 / 024 | loss: 0.88784\n",
      "batch 017 / 024 | loss: 0.88143\n",
      "batch 018 / 024 | loss: 0.87957\n",
      "batch 019 / 024 | loss: 0.88128\n",
      "batch 020 / 024 | loss: 0.88611\n",
      "batch 021 / 024 | loss: 0.89040\n",
      "batch 022 / 024 | loss: 0.89345\n",
      "batch 023 / 024 | loss: 0.89090\n",
      "batch 024 / 024 | loss: 0.88743\n",
      "----- epoch 005 / 010 | time: 120 sec | loss: 0.94797 | err: 0.44800\n",
      "batch 001 / 024 | loss: 0.86363\n",
      "batch 002 / 024 | loss: 0.81762\n",
      "batch 003 / 024 | loss: 0.86103\n",
      "batch 004 / 024 | loss: 0.87085\n",
      "batch 005 / 024 | loss: 0.87758\n",
      "batch 006 / 024 | loss: 0.90239\n",
      "batch 007 / 024 | loss: 0.90387\n",
      "batch 008 / 024 | loss: 0.91774\n",
      "batch 009 / 024 | loss: 0.90958\n",
      "batch 010 / 024 | loss: 0.89779\n",
      "batch 011 / 024 | loss: 0.89859\n",
      "batch 012 / 024 | loss: 0.90384\n",
      "batch 013 / 024 | loss: 0.91066\n",
      "batch 014 / 024 | loss: 0.91202\n",
      "batch 015 / 024 | loss: 0.90654\n",
      "batch 016 / 024 | loss: 0.90636\n",
      "batch 017 / 024 | loss: 0.90302\n",
      "batch 018 / 024 | loss: 0.91146\n",
      "batch 019 / 024 | loss: 0.91154\n",
      "batch 020 / 024 | loss: 0.89562\n",
      "batch 021 / 024 | loss: 0.89433\n",
      "batch 022 / 024 | loss: 0.89671\n",
      "batch 023 / 024 | loss: 0.89266\n",
      "batch 024 / 024 | loss: 0.89023\n",
      "training time: 789.9427828788757 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.08737341497011028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 43 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 44 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 45 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.82225\n",
      "batch 002 / 024 | loss: 0.92556\n",
      "batch 003 / 024 | loss: 0.90756\n",
      "batch 004 / 024 | loss: 0.90864\n",
      "batch 005 / 024 | loss: 0.93391\n",
      "batch 006 / 024 | loss: 0.93479\n",
      "batch 007 / 024 | loss: 0.93375\n",
      "batch 008 / 024 | loss: 0.92400\n",
      "batch 009 / 024 | loss: 0.92845\n",
      "batch 010 / 024 | loss: 0.92375\n",
      "batch 011 / 024 | loss: 0.90721\n",
      "batch 012 / 024 | loss: 0.91460\n",
      "batch 013 / 024 | loss: 0.91285\n",
      "batch 014 / 024 | loss: 0.91924\n",
      "batch 015 / 024 | loss: 0.91999\n",
      "batch 016 / 024 | loss: 0.91978\n",
      "batch 017 / 024 | loss: 0.92340\n",
      "batch 018 / 024 | loss: 0.93315\n",
      "batch 019 / 024 | loss: 0.93583\n",
      "batch 020 / 024 | loss: 0.93209\n",
      "batch 021 / 024 | loss: 0.92793\n",
      "batch 022 / 024 | loss: 0.92866\n",
      "batch 023 / 024 | loss: 0.93008\n",
      "batch 024 / 024 | loss: 0.92486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved!\n",
      "----- epoch 001 / 010 | time: 130 sec | loss: 0.88945 | err: 0.41600\n",
      "batch 001 / 024 | loss: 0.85920\n",
      "batch 002 / 024 | loss: 0.89737\n",
      "batch 003 / 024 | loss: 0.91152\n",
      "batch 004 / 024 | loss: 0.89493\n",
      "batch 005 / 024 | loss: 0.90916\n",
      "batch 006 / 024 | loss: 0.92652\n",
      "batch 007 / 024 | loss: 0.91987\n",
      "batch 008 / 024 | loss: 0.90458\n",
      "batch 009 / 024 | loss: 0.90299\n",
      "batch 010 / 024 | loss: 0.90194\n",
      "batch 011 / 024 | loss: 0.90768\n",
      "batch 012 / 024 | loss: 0.90482\n",
      "batch 013 / 024 | loss: 0.89854\n",
      "batch 014 / 024 | loss: 0.89481\n",
      "batch 015 / 024 | loss: 0.89873\n",
      "batch 016 / 024 | loss: 0.89619\n",
      "batch 017 / 024 | loss: 0.89537\n",
      "batch 018 / 024 | loss: 0.89082\n",
      "batch 019 / 024 | loss: 0.89288\n",
      "batch 020 / 024 | loss: 0.89096\n",
      "batch 021 / 024 | loss: 0.89283\n",
      "batch 022 / 024 | loss: 0.89316\n",
      "batch 023 / 024 | loss: 0.89115\n",
      "batch 024 / 024 | loss: 0.89053\n",
      "----- epoch 002 / 010 | time: 133 sec | loss: 0.99172 | err: 0.45333\n",
      "batch 001 / 024 | loss: 0.79338\n",
      "batch 002 / 024 | loss: 0.78179\n",
      "batch 003 / 024 | loss: 0.79958\n",
      "batch 004 / 024 | loss: 0.81786\n",
      "batch 005 / 024 | loss: 0.85557\n",
      "batch 006 / 024 | loss: 0.86114\n",
      "batch 007 / 024 | loss: 0.87711\n",
      "batch 008 / 024 | loss: 0.87134\n",
      "batch 009 / 024 | loss: 0.87507\n",
      "batch 010 / 024 | loss: 0.88333\n",
      "batch 011 / 024 | loss: 0.88147\n",
      "batch 012 / 024 | loss: 0.88153\n",
      "batch 013 / 024 | loss: 0.88624\n",
      "batch 014 / 024 | loss: 0.88485\n",
      "batch 015 / 024 | loss: 0.87708\n",
      "batch 016 / 024 | loss: 0.88229\n",
      "batch 017 / 024 | loss: 0.88042\n",
      "batch 018 / 024 | loss: 0.88545\n",
      "batch 019 / 024 | loss: 0.87591\n",
      "batch 020 / 024 | loss: 0.87681\n",
      "batch 021 / 024 | loss: 0.87757\n",
      "batch 022 / 024 | loss: 0.88511\n",
      "batch 023 / 024 | loss: 0.88496\n",
      "batch 024 / 024 | loss: 0.88178\n",
      "----- epoch 003 / 010 | time: 118 sec | loss: 0.96565 | err: 0.46400\n",
      "batch 001 / 024 | loss: 0.66597\n",
      "batch 002 / 024 | loss: 0.82216\n",
      "batch 003 / 024 | loss: 0.82807\n",
      "batch 004 / 024 | loss: 0.88194\n",
      "batch 005 / 024 | loss: 0.86771\n",
      "batch 006 / 024 | loss: 0.84191\n",
      "batch 007 / 024 | loss: 0.84186\n",
      "batch 008 / 024 | loss: 0.84279\n",
      "batch 009 / 024 | loss: 0.81838\n",
      "batch 010 / 024 | loss: 0.83265\n",
      "batch 011 / 024 | loss: 0.84221\n",
      "batch 012 / 024 | loss: 0.85063\n",
      "batch 013 / 024 | loss: 0.84194\n",
      "batch 014 / 024 | loss: 0.84920\n",
      "batch 015 / 024 | loss: 0.85527\n",
      "batch 016 / 024 | loss: 0.86159\n",
      "batch 017 / 024 | loss: 0.86585\n",
      "batch 018 / 024 | loss: 0.87080\n",
      "batch 019 / 024 | loss: 0.87531\n",
      "batch 020 / 024 | loss: 0.88040\n",
      "batch 021 / 024 | loss: 0.89006\n",
      "batch 022 / 024 | loss: 0.88444\n",
      "batch 023 / 024 | loss: 0.88260\n",
      "batch 024 / 024 | loss: 0.88627\n",
      "----- epoch 004 / 010 | time: 126 sec | loss: 0.90383 | err: 0.45467\n",
      "batch 001 / 024 | loss: 1.03309\n",
      "batch 002 / 024 | loss: 0.94797\n",
      "batch 003 / 024 | loss: 0.97606\n",
      "batch 004 / 024 | loss: 0.90165\n",
      "batch 005 / 024 | loss: 0.91328\n",
      "batch 006 / 024 | loss: 0.89491\n",
      "batch 007 / 024 | loss: 0.89683\n",
      "batch 008 / 024 | loss: 0.88511\n",
      "batch 009 / 024 | loss: 0.89619\n",
      "batch 010 / 024 | loss: 0.88803\n",
      "batch 011 / 024 | loss: 0.88770\n",
      "batch 012 / 024 | loss: 0.89777\n",
      "batch 013 / 024 | loss: 0.88840\n",
      "batch 014 / 024 | loss: 0.88457\n",
      "batch 015 / 024 | loss: 0.88825\n",
      "batch 016 / 024 | loss: 0.89140\n",
      "batch 017 / 024 | loss: 0.88509\n",
      "batch 018 / 024 | loss: 0.88303\n",
      "batch 019 / 024 | loss: 0.88451\n",
      "batch 020 / 024 | loss: 0.88926\n",
      "batch 021 / 024 | loss: 0.89365\n",
      "batch 022 / 024 | loss: 0.89665\n",
      "batch 023 / 024 | loss: 0.89393\n",
      "batch 024 / 024 | loss: 0.89027\n",
      "----- epoch 005 / 010 | time: 123 sec | loss: 0.95193 | err: 0.45200\n",
      "batch 001 / 024 | loss: 0.86760\n",
      "batch 002 / 024 | loss: 0.81886\n",
      "batch 003 / 024 | loss: 0.86217\n",
      "batch 004 / 024 | loss: 0.87139\n",
      "batch 005 / 024 | loss: 0.87842\n",
      "batch 006 / 024 | loss: 0.90442\n",
      "batch 007 / 024 | loss: 0.90588\n",
      "batch 008 / 024 | loss: 0.91933\n",
      "batch 009 / 024 | loss: 0.91170\n",
      "batch 010 / 024 | loss: 0.89993\n",
      "batch 011 / 024 | loss: 0.90283\n",
      "batch 012 / 024 | loss: 0.90817\n",
      "batch 013 / 024 | loss: 0.91477\n",
      "batch 014 / 024 | loss: 0.91592\n",
      "batch 015 / 024 | loss: 0.91013\n",
      "batch 016 / 024 | loss: 0.90985\n",
      "batch 017 / 024 | loss: 0.90661\n",
      "batch 018 / 024 | loss: 0.91523\n",
      "batch 019 / 024 | loss: 0.91522\n",
      "batch 020 / 024 | loss: 0.89864\n",
      "batch 021 / 024 | loss: 0.89751\n",
      "batch 022 / 024 | loss: 0.89992\n",
      "batch 023 / 024 | loss: 0.89584\n",
      "batch 024 / 024 | loss: 0.89372\n",
      "training time: 759.0235404968262 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.10322695185890128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 46 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 47 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 48 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.82954\n",
      "batch 002 / 024 | loss: 0.93493\n",
      "batch 003 / 024 | loss: 0.91419\n",
      "batch 004 / 024 | loss: 0.91370\n",
      "batch 005 / 024 | loss: 0.93813\n",
      "batch 006 / 024 | loss: 0.93887\n",
      "batch 007 / 024 | loss: 0.94213\n",
      "batch 008 / 024 | loss: 0.93182\n",
      "batch 009 / 024 | loss: 0.93581\n",
      "batch 010 / 024 | loss: 0.93106\n",
      "batch 011 / 024 | loss: 0.91676\n",
      "batch 012 / 024 | loss: 0.92294\n",
      "batch 013 / 024 | loss: 0.92051\n",
      "batch 014 / 024 | loss: 0.92670\n",
      "batch 015 / 024 | loss: 0.92682\n",
      "batch 016 / 024 | loss: 0.92723\n",
      "batch 017 / 024 | loss: 0.93065\n",
      "batch 018 / 024 | loss: 0.93984\n",
      "batch 019 / 024 | loss: 0.94201\n",
      "batch 020 / 024 | loss: 0.93800\n",
      "batch 021 / 024 | loss: 0.93351\n",
      "batch 022 / 024 | loss: 0.93400\n",
      "batch 023 / 024 | loss: 0.93531\n",
      "batch 024 / 024 | loss: 0.92956\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 133 sec | loss: 0.94972 | err: 0.45600\n",
      "batch 001 / 024 | loss: 0.85975\n",
      "batch 002 / 024 | loss: 0.89584\n",
      "batch 003 / 024 | loss: 0.91038\n",
      "batch 004 / 024 | loss: 0.89255\n",
      "batch 005 / 024 | loss: 0.90693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 006 / 024 | loss: 0.92368\n",
      "batch 007 / 024 | loss: 0.91720\n",
      "batch 008 / 024 | loss: 0.90186\n",
      "batch 009 / 024 | loss: 0.90096\n",
      "batch 010 / 024 | loss: 0.90101\n",
      "batch 011 / 024 | loss: 0.90690\n",
      "batch 012 / 024 | loss: 0.90430\n",
      "batch 013 / 024 | loss: 0.89807\n",
      "batch 014 / 024 | loss: 0.89458\n",
      "batch 015 / 024 | loss: 0.89970\n",
      "batch 016 / 024 | loss: 0.89743\n",
      "batch 017 / 024 | loss: 0.89670\n",
      "batch 018 / 024 | loss: 0.89217\n",
      "batch 019 / 024 | loss: 0.89411\n",
      "batch 020 / 024 | loss: 0.89210\n",
      "batch 021 / 024 | loss: 0.89398\n",
      "batch 022 / 024 | loss: 0.89435\n",
      "batch 023 / 024 | loss: 0.89229\n",
      "batch 024 / 024 | loss: 0.89174\n",
      "----- epoch 002 / 010 | time: 132 sec | loss: 0.98980 | err: 0.46133\n",
      "batch 001 / 024 | loss: 0.79723\n",
      "batch 002 / 024 | loss: 0.78468\n",
      "batch 003 / 024 | loss: 0.80213\n",
      "batch 004 / 024 | loss: 0.82171\n",
      "batch 005 / 024 | loss: 0.86089\n",
      "batch 006 / 024 | loss: 0.86583\n",
      "batch 007 / 024 | loss: 0.88235\n",
      "batch 008 / 024 | loss: 0.87546\n",
      "batch 009 / 024 | loss: 0.87888\n",
      "batch 010 / 024 | loss: 0.88734\n",
      "batch 011 / 024 | loss: 0.88537\n",
      "batch 012 / 024 | loss: 0.88540\n",
      "batch 013 / 024 | loss: 0.89046\n",
      "batch 014 / 024 | loss: 0.88892\n",
      "batch 015 / 024 | loss: 0.88063\n",
      "batch 016 / 024 | loss: 0.88573\n",
      "batch 017 / 024 | loss: 0.88408\n",
      "batch 018 / 024 | loss: 0.88917\n",
      "batch 019 / 024 | loss: 0.87916\n",
      "batch 020 / 024 | loss: 0.87988\n",
      "batch 021 / 024 | loss: 0.88089\n",
      "batch 022 / 024 | loss: 0.88810\n",
      "batch 023 / 024 | loss: 0.88833\n",
      "batch 024 / 024 | loss: 0.88533\n",
      "----- epoch 003 / 010 | time: 126 sec | loss: 0.95106 | err: 0.47467\n",
      "batch 001 / 024 | loss: 0.67106\n",
      "batch 002 / 024 | loss: 0.82516\n",
      "batch 003 / 024 | loss: 0.83052\n",
      "batch 004 / 024 | loss: 0.88439\n",
      "batch 005 / 024 | loss: 0.86997\n",
      "batch 006 / 024 | loss: 0.84544\n",
      "batch 007 / 024 | loss: 0.84506\n",
      "batch 008 / 024 | loss: 0.84601\n",
      "batch 009 / 024 | loss: 0.82155\n",
      "batch 010 / 024 | loss: 0.83537\n",
      "batch 011 / 024 | loss: 0.84480\n",
      "batch 012 / 024 | loss: 0.85350\n",
      "batch 013 / 024 | loss: 0.84425\n",
      "batch 014 / 024 | loss: 0.85154\n",
      "batch 015 / 024 | loss: 0.85790\n",
      "batch 016 / 024 | loss: 0.86438\n",
      "batch 017 / 024 | loss: 0.86907\n",
      "batch 018 / 024 | loss: 0.87420\n",
      "batch 019 / 024 | loss: 0.87911\n",
      "batch 020 / 024 | loss: 0.88449\n",
      "batch 021 / 024 | loss: 0.89428\n",
      "batch 022 / 024 | loss: 0.88817\n",
      "batch 023 / 024 | loss: 0.88598\n",
      "batch 024 / 024 | loss: 0.88937\n",
      "----- epoch 004 / 010 | time: 123 sec | loss: 0.93101 | err: 0.47200\n",
      "batch 001 / 024 | loss: 1.04722\n",
      "batch 002 / 024 | loss: 0.95169\n",
      "batch 003 / 024 | loss: 0.97991\n",
      "batch 004 / 024 | loss: 0.90377\n",
      "batch 005 / 024 | loss: 0.91683\n",
      "batch 006 / 024 | loss: 0.89807\n",
      "batch 007 / 024 | loss: 0.89963\n",
      "batch 008 / 024 | loss: 0.88713\n",
      "batch 009 / 024 | loss: 0.89905\n",
      "batch 010 / 024 | loss: 0.89047\n",
      "batch 011 / 024 | loss: 0.88965\n",
      "batch 012 / 024 | loss: 0.89948\n",
      "batch 013 / 024 | loss: 0.89024\n",
      "batch 014 / 024 | loss: 0.88591\n",
      "batch 015 / 024 | loss: 0.88981\n",
      "batch 016 / 024 | loss: 0.89281\n",
      "batch 017 / 024 | loss: 0.88636\n",
      "batch 018 / 024 | loss: 0.88422\n",
      "batch 019 / 024 | loss: 0.88587\n",
      "batch 020 / 024 | loss: 0.89048\n",
      "batch 021 / 024 | loss: 0.89483\n",
      "batch 022 / 024 | loss: 0.89768\n",
      "batch 023 / 024 | loss: 0.89480\n",
      "batch 024 / 024 | loss: 0.89090\n",
      "----- epoch 005 / 010 | time: 126 sec | loss: 0.96477 | err: 0.45733\n",
      "batch 001 / 024 | loss: 0.87189\n",
      "batch 002 / 024 | loss: 0.82090\n",
      "batch 003 / 024 | loss: 0.86283\n",
      "batch 004 / 024 | loss: 0.86982\n",
      "batch 005 / 024 | loss: 0.87567\n",
      "batch 006 / 024 | loss: 0.90038\n",
      "batch 007 / 024 | loss: 0.90073\n",
      "batch 008 / 024 | loss: 0.91470\n",
      "batch 009 / 024 | loss: 0.90827\n",
      "batch 010 / 024 | loss: 0.89717\n",
      "batch 011 / 024 | loss: 0.90158\n",
      "batch 012 / 024 | loss: 0.90708\n",
      "batch 013 / 024 | loss: 0.91361\n",
      "batch 014 / 024 | loss: 0.91501\n",
      "batch 015 / 024 | loss: 0.90924\n",
      "batch 016 / 024 | loss: 0.90881\n",
      "batch 017 / 024 | loss: 0.90559\n",
      "batch 018 / 024 | loss: 0.91468\n",
      "batch 019 / 024 | loss: 0.91475\n",
      "batch 020 / 024 | loss: 0.89807\n",
      "batch 021 / 024 | loss: 0.89693\n",
      "batch 022 / 024 | loss: 0.89901\n",
      "batch 023 / 024 | loss: 0.89462\n",
      "batch 024 / 024 | loss: 0.89238\n",
      "model saved!\n",
      "----- epoch 006 / 010 | time: 155 sec | loss: 0.97619 | err: 0.45467\n",
      "batch 001 / 024 | loss: 0.75671\n",
      "batch 002 / 024 | loss: 0.79972\n",
      "batch 003 / 024 | loss: 0.84925\n",
      "batch 004 / 024 | loss: 0.84921\n",
      "batch 005 / 024 | loss: 0.88188\n",
      "batch 006 / 024 | loss: 0.89488\n",
      "batch 007 / 024 | loss: 0.88601\n",
      "batch 008 / 024 | loss: 0.88603\n",
      "batch 009 / 024 | loss: 0.88978\n",
      "batch 010 / 024 | loss: 0.88024\n",
      "batch 011 / 024 | loss: 0.86055\n",
      "batch 012 / 024 | loss: 0.87213\n",
      "batch 013 / 024 | loss: 0.88083\n",
      "batch 014 / 024 | loss: 0.87603\n",
      "batch 015 / 024 | loss: 0.87478\n",
      "batch 016 / 024 | loss: 0.88059\n",
      "batch 017 / 024 | loss: 0.87521\n",
      "batch 018 / 024 | loss: 0.87520\n",
      "batch 019 / 024 | loss: 0.88284\n",
      "batch 020 / 024 | loss: 0.88751\n",
      "batch 021 / 024 | loss: 0.88686\n",
      "batch 022 / 024 | loss: 0.88816\n",
      "batch 023 / 024 | loss: 0.88633\n",
      "batch 024 / 024 | loss: 0.88226\n",
      "----- epoch 007 / 010 | time: 124 sec | loss: 0.96664 | err: 0.47200\n",
      "batch 001 / 024 | loss: 0.89408\n",
      "batch 002 / 024 | loss: 0.85762\n",
      "batch 003 / 024 | loss: 0.89565\n",
      "batch 004 / 024 | loss: 0.89899\n",
      "batch 005 / 024 | loss: 0.89869\n",
      "batch 006 / 024 | loss: 0.90114\n",
      "batch 007 / 024 | loss: 0.88808\n",
      "batch 008 / 024 | loss: 0.89563\n",
      "batch 009 / 024 | loss: 0.91702\n",
      "batch 010 / 024 | loss: 0.90308\n",
      "batch 011 / 024 | loss: 0.88340\n",
      "batch 012 / 024 | loss: 0.87007\n",
      "batch 013 / 024 | loss: 0.86699\n",
      "batch 014 / 024 | loss: 0.87759\n",
      "batch 015 / 024 | loss: 0.87394\n",
      "batch 016 / 024 | loss: 0.86685\n",
      "batch 017 / 024 | loss: 0.86169\n",
      "batch 018 / 024 | loss: 0.86855\n",
      "batch 019 / 024 | loss: 0.87408\n",
      "batch 020 / 024 | loss: 0.87811\n",
      "batch 021 / 024 | loss: 0.88169\n",
      "batch 022 / 024 | loss: 0.88399\n",
      "batch 023 / 024 | loss: 0.88234\n",
      "batch 024 / 024 | loss: 0.88837\n",
      "----- epoch 008 / 010 | time: 121 sec | loss: 0.93491 | err: 0.47733\n",
      "batch 001 / 024 | loss: 0.93935\n",
      "batch 002 / 024 | loss: 0.92196\n",
      "batch 003 / 024 | loss: 0.92300\n",
      "batch 004 / 024 | loss: 0.93215\n",
      "batch 005 / 024 | loss: 0.92139\n",
      "batch 006 / 024 | loss: 0.92227\n",
      "batch 007 / 024 | loss: 0.89429\n",
      "batch 008 / 024 | loss: 0.89215\n",
      "batch 009 / 024 | loss: 0.91079\n",
      "batch 010 / 024 | loss: 0.91253\n",
      "batch 011 / 024 | loss: 0.90972\n",
      "batch 012 / 024 | loss: 0.90258\n",
      "batch 013 / 024 | loss: 0.90116\n",
      "batch 014 / 024 | loss: 0.89744\n",
      "batch 015 / 024 | loss: 0.88974\n",
      "batch 016 / 024 | loss: 0.88485\n",
      "batch 017 / 024 | loss: 0.87809\n",
      "batch 018 / 024 | loss: 0.87531\n",
      "batch 019 / 024 | loss: 0.87300\n",
      "batch 020 / 024 | loss: 0.87512\n",
      "batch 021 / 024 | loss: 0.87889\n",
      "batch 022 / 024 | loss: 0.88425\n",
      "batch 023 / 024 | loss: 0.88415\n",
      "batch 024 / 024 | loss: 0.88745\n",
      "----- epoch 009 / 010 | time: 118 sec | loss: 0.93685 | err: 0.47600\n",
      "batch 001 / 024 | loss: 0.73353\n",
      "batch 002 / 024 | loss: 0.85732\n",
      "batch 003 / 024 | loss: 0.90240\n",
      "batch 004 / 024 | loss: 0.89439\n",
      "batch 005 / 024 | loss: 0.89332\n",
      "batch 006 / 024 | loss: 0.89145\n",
      "batch 007 / 024 | loss: 0.88110\n",
      "batch 008 / 024 | loss: 0.89929\n",
      "batch 009 / 024 | loss: 0.89692\n",
      "batch 010 / 024 | loss: 0.89475\n",
      "batch 011 / 024 | loss: 0.90238\n",
      "batch 012 / 024 | loss: 0.89768\n",
      "batch 013 / 024 | loss: 0.88740\n",
      "batch 014 / 024 | loss: 0.89087\n",
      "batch 015 / 024 | loss: 0.89731\n",
      "batch 016 / 024 | loss: 0.89789\n",
      "batch 017 / 024 | loss: 0.89289\n",
      "batch 018 / 024 | loss: 0.88159\n",
      "batch 019 / 024 | loss: 0.88124\n",
      "batch 020 / 024 | loss: 0.88019\n",
      "batch 021 / 024 | loss: 0.88374\n",
      "batch 022 / 024 | loss: 0.88154\n",
      "batch 023 / 024 | loss: 0.88226\n",
      "batch 024 / 024 | loss: 0.88702\n",
      "----- epoch 010 / 010 | time: 131 sec | loss: 0.98061 | err: 0.45733\n",
      "training time: 1293.4504179954529 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.12195704601594412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 49 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 50 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 51 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.83815\n",
      "batch 002 / 024 | loss: 0.94601\n",
      "batch 003 / 024 | loss: 0.92203\n",
      "batch 004 / 024 | loss: 0.91967\n",
      "batch 005 / 024 | loss: 0.94321\n",
      "batch 006 / 024 | loss: 0.94389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 007 / 024 | loss: 0.94878\n",
      "batch 008 / 024 | loss: 0.94039\n",
      "batch 009 / 024 | loss: 0.94751\n",
      "batch 010 / 024 | loss: 0.94333\n",
      "batch 011 / 024 | loss: 0.92856\n",
      "batch 012 / 024 | loss: 0.93378\n",
      "batch 013 / 024 | loss: 0.93120\n",
      "batch 014 / 024 | loss: 0.93718\n",
      "batch 015 / 024 | loss: 0.93685\n",
      "batch 016 / 024 | loss: 0.93743\n",
      "batch 017 / 024 | loss: 0.94036\n",
      "batch 018 / 024 | loss: 0.94878\n",
      "batch 019 / 024 | loss: 0.95052\n",
      "batch 020 / 024 | loss: 0.94603\n",
      "batch 021 / 024 | loss: 0.94113\n",
      "batch 022 / 024 | loss: 0.94140\n",
      "batch 023 / 024 | loss: 0.94263\n",
      "batch 024 / 024 | loss: 0.93615\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 131 sec | loss: 0.99729 | err: 0.47200\n",
      "batch 001 / 024 | loss: 0.86169\n",
      "batch 002 / 024 | loss: 0.89660\n",
      "batch 003 / 024 | loss: 0.91360\n",
      "batch 004 / 024 | loss: 0.89407\n",
      "batch 005 / 024 | loss: 0.90906\n",
      "batch 006 / 024 | loss: 0.92594\n",
      "batch 007 / 024 | loss: 0.91903\n",
      "batch 008 / 024 | loss: 0.90289\n",
      "batch 009 / 024 | loss: 0.90240\n",
      "batch 010 / 024 | loss: 0.90307\n",
      "batch 011 / 024 | loss: 0.90906\n",
      "batch 012 / 024 | loss: 0.90654\n",
      "batch 013 / 024 | loss: 0.90021\n",
      "batch 014 / 024 | loss: 0.89663\n",
      "batch 015 / 024 | loss: 0.90291\n",
      "batch 016 / 024 | loss: 0.90139\n",
      "batch 017 / 024 | loss: 0.90047\n",
      "batch 018 / 024 | loss: 0.89579\n",
      "batch 019 / 024 | loss: 0.89758\n",
      "batch 020 / 024 | loss: 0.89552\n",
      "batch 021 / 024 | loss: 0.89735\n",
      "batch 022 / 024 | loss: 0.89742\n",
      "batch 023 / 024 | loss: 0.89524\n",
      "batch 024 / 024 | loss: 0.89469\n",
      "model saved!\n",
      "----- epoch 002 / 010 | time: 120 sec | loss: 0.98729 | err: 0.46933\n",
      "batch 001 / 024 | loss: 0.79326\n",
      "batch 002 / 024 | loss: 0.78505\n",
      "batch 003 / 024 | loss: 0.80414\n",
      "batch 004 / 024 | loss: 0.82508\n",
      "batch 005 / 024 | loss: 0.86553\n",
      "batch 006 / 024 | loss: 0.87041\n",
      "batch 007 / 024 | loss: 0.88808\n",
      "batch 008 / 024 | loss: 0.88039\n",
      "batch 009 / 024 | loss: 0.88336\n",
      "batch 010 / 024 | loss: 0.89155\n",
      "batch 011 / 024 | loss: 0.88931\n",
      "batch 012 / 024 | loss: 0.88895\n",
      "batch 013 / 024 | loss: 0.89437\n",
      "batch 014 / 024 | loss: 0.89275\n",
      "batch 015 / 024 | loss: 0.88422\n",
      "batch 016 / 024 | loss: 0.88867\n",
      "batch 017 / 024 | loss: 0.88682\n",
      "batch 018 / 024 | loss: 0.89165\n",
      "batch 019 / 024 | loss: 0.88166\n",
      "batch 020 / 024 | loss: 0.88224\n",
      "batch 021 / 024 | loss: 0.88348\n",
      "batch 022 / 024 | loss: 0.89077\n",
      "batch 023 / 024 | loss: 0.89149\n",
      "batch 024 / 024 | loss: 0.88843\n",
      "----- epoch 003 / 010 | time: 120 sec | loss: 0.96780 | err: 0.47333\n",
      "batch 001 / 024 | loss: 0.66943\n",
      "batch 002 / 024 | loss: 0.82723\n",
      "batch 003 / 024 | loss: 0.83170\n",
      "batch 004 / 024 | loss: 0.88662\n",
      "batch 005 / 024 | loss: 0.87161\n",
      "batch 006 / 024 | loss: 0.84622\n",
      "batch 007 / 024 | loss: 0.84614\n",
      "batch 008 / 024 | loss: 0.84764\n",
      "batch 009 / 024 | loss: 0.82316\n",
      "batch 010 / 024 | loss: 0.83684\n",
      "batch 011 / 024 | loss: 0.84609\n",
      "batch 012 / 024 | loss: 0.85481\n",
      "batch 013 / 024 | loss: 0.84562\n",
      "batch 014 / 024 | loss: 0.85314\n",
      "batch 015 / 024 | loss: 0.85964\n",
      "batch 016 / 024 | loss: 0.86621\n",
      "batch 017 / 024 | loss: 0.87150\n",
      "batch 018 / 024 | loss: 0.87646\n",
      "batch 019 / 024 | loss: 0.88173\n",
      "batch 020 / 024 | loss: 0.88698\n",
      "batch 021 / 024 | loss: 0.89679\n",
      "batch 022 / 024 | loss: 0.89058\n",
      "batch 023 / 024 | loss: 0.88824\n",
      "batch 024 / 024 | loss: 0.89167\n",
      "----- epoch 004 / 010 | time: 112 sec | loss: 0.94160 | err: 0.47333\n",
      "batch 001 / 024 | loss: 1.06573\n",
      "batch 002 / 024 | loss: 0.96029\n",
      "batch 003 / 024 | loss: 0.98730\n",
      "batch 004 / 024 | loss: 0.90782\n",
      "batch 005 / 024 | loss: 0.92054\n",
      "batch 006 / 024 | loss: 0.90112\n",
      "batch 007 / 024 | loss: 0.90246\n",
      "batch 008 / 024 | loss: 0.88904\n",
      "batch 009 / 024 | loss: 0.90136\n",
      "batch 010 / 024 | loss: 0.89291\n",
      "batch 011 / 024 | loss: 0.89166\n",
      "batch 012 / 024 | loss: 0.90073\n",
      "batch 013 / 024 | loss: 0.89093\n",
      "batch 014 / 024 | loss: 0.88599\n",
      "batch 015 / 024 | loss: 0.89065\n",
      "batch 016 / 024 | loss: 0.89370\n",
      "batch 017 / 024 | loss: 0.88691\n",
      "batch 018 / 024 | loss: 0.88485\n",
      "batch 019 / 024 | loss: 0.88659\n",
      "batch 020 / 024 | loss: 0.89112\n",
      "batch 021 / 024 | loss: 0.89525\n",
      "batch 022 / 024 | loss: 0.89786\n",
      "batch 023 / 024 | loss: 0.89493\n",
      "batch 024 / 024 | loss: 0.89093\n",
      "model saved!\n",
      "----- epoch 005 / 010 | time: 112 sec | loss: 0.98447 | err: 0.46400\n",
      "batch 001 / 024 | loss: 0.88281\n",
      "batch 002 / 024 | loss: 0.82824\n",
      "batch 003 / 024 | loss: 0.86849\n",
      "batch 004 / 024 | loss: 0.87420\n",
      "batch 005 / 024 | loss: 0.87898\n",
      "batch 006 / 024 | loss: 0.90236\n",
      "batch 007 / 024 | loss: 0.90191\n",
      "batch 008 / 024 | loss: 0.91608\n",
      "batch 009 / 024 | loss: 0.91052\n",
      "batch 010 / 024 | loss: 0.89914\n",
      "batch 011 / 024 | loss: 0.90570\n",
      "batch 012 / 024 | loss: 0.91223\n",
      "batch 013 / 024 | loss: 0.91886\n",
      "batch 014 / 024 | loss: 0.92080\n",
      "batch 015 / 024 | loss: 0.91456\n",
      "batch 016 / 024 | loss: 0.91384\n",
      "batch 017 / 024 | loss: 0.91044\n",
      "batch 018 / 024 | loss: 0.91961\n",
      "batch 019 / 024 | loss: 0.91958\n",
      "batch 020 / 024 | loss: 0.90249\n",
      "batch 021 / 024 | loss: 0.90140\n",
      "batch 022 / 024 | loss: 0.90343\n",
      "batch 023 / 024 | loss: 0.89915\n",
      "batch 024 / 024 | loss: 0.89700\n",
      "----- epoch 006 / 010 | time: 171 sec | loss: 0.96579 | err: 0.46667\n",
      "batch 001 / 024 | loss: 0.76105\n",
      "batch 002 / 024 | loss: 0.80879\n",
      "batch 003 / 024 | loss: 0.85418\n",
      "batch 004 / 024 | loss: 0.85243\n",
      "batch 005 / 024 | loss: 0.88728\n",
      "batch 006 / 024 | loss: 0.89929\n",
      "batch 007 / 024 | loss: 0.88985\n",
      "batch 008 / 024 | loss: 0.88949\n",
      "batch 009 / 024 | loss: 0.89323\n",
      "batch 010 / 024 | loss: 0.88324\n",
      "batch 011 / 024 | loss: 0.86294\n",
      "batch 012 / 024 | loss: 0.87430\n",
      "batch 013 / 024 | loss: 0.88288\n",
      "batch 014 / 024 | loss: 0.87825\n",
      "batch 015 / 024 | loss: 0.87735\n",
      "batch 016 / 024 | loss: 0.88291\n",
      "batch 017 / 024 | loss: 0.87761\n",
      "batch 018 / 024 | loss: 0.87753\n",
      "batch 019 / 024 | loss: 0.88514\n",
      "batch 020 / 024 | loss: 0.89056\n",
      "batch 021 / 024 | loss: 0.89016\n",
      "batch 022 / 024 | loss: 0.89151\n",
      "batch 023 / 024 | loss: 0.88962\n",
      "batch 024 / 024 | loss: 0.88569\n",
      "----- epoch 007 / 010 | time: 143 sec | loss: 0.96604 | err: 0.47200\n",
      "batch 001 / 024 | loss: 0.89525\n",
      "batch 002 / 024 | loss: 0.85690\n",
      "batch 003 / 024 | loss: 0.89445\n",
      "batch 004 / 024 | loss: 0.89847\n",
      "batch 005 / 024 | loss: 0.89906\n",
      "batch 006 / 024 | loss: 0.90165\n",
      "batch 007 / 024 | loss: 0.88895\n",
      "batch 008 / 024 | loss: 0.89646\n",
      "batch 009 / 024 | loss: 0.91765\n",
      "batch 010 / 024 | loss: 0.90382\n",
      "batch 011 / 024 | loss: 0.88419\n",
      "batch 012 / 024 | loss: 0.87070\n",
      "batch 013 / 024 | loss: 0.86782\n",
      "batch 014 / 024 | loss: 0.87889\n",
      "batch 015 / 024 | loss: 0.87521\n",
      "batch 016 / 024 | loss: 0.86797\n",
      "batch 017 / 024 | loss: 0.86280\n",
      "batch 018 / 024 | loss: 0.86971\n",
      "batch 019 / 024 | loss: 0.87534\n",
      "batch 020 / 024 | loss: 0.88021\n",
      "batch 021 / 024 | loss: 0.88410\n",
      "batch 022 / 024 | loss: 0.88620\n",
      "batch 023 / 024 | loss: 0.88448\n",
      "batch 024 / 024 | loss: 0.89050\n",
      "----- epoch 008 / 010 | time: 138 sec | loss: 0.93257 | err: 0.47733\n",
      "batch 001 / 024 | loss: 0.93487\n",
      "batch 002 / 024 | loss: 0.92143\n",
      "batch 003 / 024 | loss: 0.92274\n",
      "batch 004 / 024 | loss: 0.93216\n",
      "batch 005 / 024 | loss: 0.92169\n",
      "batch 006 / 024 | loss: 0.92254\n",
      "batch 007 / 024 | loss: 0.89485\n",
      "batch 008 / 024 | loss: 0.89258\n",
      "batch 009 / 024 | loss: 0.91196\n",
      "batch 010 / 024 | loss: 0.91373\n",
      "batch 011 / 024 | loss: 0.91093\n",
      "batch 012 / 024 | loss: 0.90376\n",
      "batch 013 / 024 | loss: 0.90231\n",
      "batch 014 / 024 | loss: 0.89906\n",
      "batch 015 / 024 | loss: 0.89138\n",
      "batch 016 / 024 | loss: 0.88645\n",
      "batch 017 / 024 | loss: 0.87973\n",
      "batch 018 / 024 | loss: 0.87692\n",
      "batch 019 / 024 | loss: 0.87448\n",
      "batch 020 / 024 | loss: 0.87645\n",
      "batch 021 / 024 | loss: 0.88042\n",
      "batch 022 / 024 | loss: 0.88600\n",
      "batch 023 / 024 | loss: 0.88601\n",
      "batch 024 / 024 | loss: 0.89063\n",
      "----- epoch 009 / 010 | time: 139 sec | loss: 0.93049 | err: 0.47733\n",
      "batch 001 / 024 | loss: 0.73208\n",
      "batch 002 / 024 | loss: 0.85596\n",
      "batch 003 / 024 | loss: 0.90255\n",
      "batch 004 / 024 | loss: 0.89426\n",
      "batch 005 / 024 | loss: 0.89357\n",
      "batch 006 / 024 | loss: 0.89222\n",
      "batch 007 / 024 | loss: 0.88151\n",
      "batch 008 / 024 | loss: 0.89935\n",
      "batch 009 / 024 | loss: 0.89770\n",
      "batch 010 / 024 | loss: 0.89569\n",
      "batch 011 / 024 | loss: 0.90301\n",
      "batch 012 / 024 | loss: 0.89946\n",
      "batch 013 / 024 | loss: 0.88972\n",
      "batch 014 / 024 | loss: 0.89364\n",
      "batch 015 / 024 | loss: 0.89990\n",
      "batch 016 / 024 | loss: 0.90035\n",
      "batch 017 / 024 | loss: 0.89518\n",
      "batch 018 / 024 | loss: 0.88389\n",
      "batch 019 / 024 | loss: 0.88365\n",
      "batch 020 / 024 | loss: 0.88256\n",
      "batch 021 / 024 | loss: 0.88595\n",
      "batch 022 / 024 | loss: 0.88364\n",
      "batch 023 / 024 | loss: 0.88434\n",
      "batch 024 / 024 | loss: 0.88901\n",
      "training time: 1337.6993067264557 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.14408563660065649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 52 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 53 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 54 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.84833\n",
      "batch 002 / 024 | loss: 0.95909\n",
      "batch 003 / 024 | loss: 0.93129\n",
      "batch 004 / 024 | loss: 0.92673\n",
      "batch 005 / 024 | loss: 0.94929\n",
      "batch 006 / 024 | loss: 0.94994\n",
      "batch 007 / 024 | loss: 0.95445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 008 / 024 | loss: 0.94541\n",
      "batch 009 / 024 | loss: 0.95124\n",
      "batch 010 / 024 | loss: 0.94750\n",
      "batch 011 / 024 | loss: 0.93426\n",
      "batch 012 / 024 | loss: 0.93931\n",
      "batch 013 / 024 | loss: 0.93645\n",
      "batch 014 / 024 | loss: 0.94310\n",
      "batch 015 / 024 | loss: 0.94274\n",
      "batch 016 / 024 | loss: 0.94359\n",
      "batch 017 / 024 | loss: 0.94716\n",
      "batch 018 / 024 | loss: 0.95610\n",
      "batch 019 / 024 | loss: 0.95774\n",
      "batch 020 / 024 | loss: 0.95304\n",
      "batch 021 / 024 | loss: 0.94771\n",
      "batch 022 / 024 | loss: 0.94754\n",
      "batch 023 / 024 | loss: 0.94866\n",
      "batch 024 / 024 | loss: 0.94148\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 162 sec | loss: 0.99973 | err: 0.47333\n",
      "batch 001 / 024 | loss: 0.85765\n",
      "batch 002 / 024 | loss: 0.89331\n",
      "batch 003 / 024 | loss: 0.91038\n",
      "batch 004 / 024 | loss: 0.89056\n",
      "batch 005 / 024 | loss: 0.90579\n",
      "batch 006 / 024 | loss: 0.92295\n",
      "batch 007 / 024 | loss: 0.91721\n",
      "batch 008 / 024 | loss: 0.90143\n",
      "batch 009 / 024 | loss: 0.90242\n",
      "batch 010 / 024 | loss: 0.90370\n",
      "batch 011 / 024 | loss: 0.91004\n",
      "batch 012 / 024 | loss: 0.90799\n",
      "batch 013 / 024 | loss: 0.90182\n",
      "batch 014 / 024 | loss: 0.89842\n",
      "batch 015 / 024 | loss: 0.90579\n",
      "batch 016 / 024 | loss: 0.90398\n",
      "batch 017 / 024 | loss: 0.90309\n",
      "batch 018 / 024 | loss: 0.89850\n",
      "batch 019 / 024 | loss: 0.90035\n",
      "batch 020 / 024 | loss: 0.89841\n",
      "batch 021 / 024 | loss: 0.90046\n",
      "batch 022 / 024 | loss: 0.90053\n",
      "batch 023 / 024 | loss: 0.89830\n",
      "batch 024 / 024 | loss: 0.89786\n",
      "model saved!\n",
      "----- epoch 002 / 010 | time: 156 sec | loss: 1.00370 | err: 0.46800\n",
      "batch 001 / 024 | loss: 0.79628\n",
      "batch 002 / 024 | loss: 0.78539\n",
      "batch 003 / 024 | loss: 0.80487\n",
      "batch 004 / 024 | loss: 0.82583\n",
      "batch 005 / 024 | loss: 0.86639\n",
      "batch 006 / 024 | loss: 0.87046\n",
      "batch 007 / 024 | loss: 0.88854\n",
      "batch 008 / 024 | loss: 0.88171\n",
      "batch 009 / 024 | loss: 0.88536\n",
      "batch 010 / 024 | loss: 0.89328\n",
      "batch 011 / 024 | loss: 0.89134\n",
      "batch 012 / 024 | loss: 0.89074\n",
      "batch 013 / 024 | loss: 0.89592\n",
      "batch 014 / 024 | loss: 0.89413\n",
      "batch 015 / 024 | loss: 0.88566\n",
      "batch 016 / 024 | loss: 0.89055\n",
      "batch 017 / 024 | loss: 0.88876\n",
      "batch 018 / 024 | loss: 0.89362\n",
      "batch 019 / 024 | loss: 0.88354\n",
      "batch 020 / 024 | loss: 0.88417\n",
      "batch 021 / 024 | loss: 0.88577\n",
      "batch 022 / 024 | loss: 0.89311\n",
      "batch 023 / 024 | loss: 0.89432\n",
      "batch 024 / 024 | loss: 0.89098\n",
      "----- epoch 003 / 010 | time: 145 sec | loss: 0.95974 | err: 0.47467\n",
      "batch 001 / 024 | loss: 0.66700\n",
      "batch 002 / 024 | loss: 0.82375\n",
      "batch 003 / 024 | loss: 0.83021\n",
      "batch 004 / 024 | loss: 0.88498\n",
      "batch 005 / 024 | loss: 0.87128\n",
      "batch 006 / 024 | loss: 0.84786\n",
      "batch 007 / 024 | loss: 0.84793\n",
      "batch 008 / 024 | loss: 0.84988\n",
      "batch 009 / 024 | loss: 0.82508\n",
      "batch 010 / 024 | loss: 0.83960\n",
      "batch 011 / 024 | loss: 0.84875\n",
      "batch 012 / 024 | loss: 0.85772\n",
      "batch 013 / 024 | loss: 0.84783\n",
      "batch 014 / 024 | loss: 0.85526\n",
      "batch 015 / 024 | loss: 0.86174\n",
      "batch 016 / 024 | loss: 0.86818\n",
      "batch 017 / 024 | loss: 0.87473\n",
      "batch 018 / 024 | loss: 0.87998\n",
      "batch 019 / 024 | loss: 0.88587\n",
      "batch 020 / 024 | loss: 0.89093\n",
      "batch 021 / 024 | loss: 0.90066\n",
      "batch 022 / 024 | loss: 0.89441\n",
      "batch 023 / 024 | loss: 0.89199\n",
      "batch 024 / 024 | loss: 0.89571\n",
      "----- epoch 004 / 010 | time: 138 sec | loss: 0.93579 | err: 0.47200\n",
      "batch 001 / 024 | loss: 1.08539\n",
      "batch 002 / 024 | loss: 0.96998\n",
      "batch 003 / 024 | loss: 0.99161\n",
      "batch 004 / 024 | loss: 0.91309\n",
      "batch 005 / 024 | loss: 0.92564\n",
      "batch 006 / 024 | loss: 0.90654\n",
      "batch 007 / 024 | loss: 0.90719\n",
      "batch 008 / 024 | loss: 0.89448\n",
      "batch 009 / 024 | loss: 0.90606\n",
      "batch 010 / 024 | loss: 0.89690\n",
      "batch 011 / 024 | loss: 0.89523\n",
      "batch 012 / 024 | loss: 0.90499\n",
      "batch 013 / 024 | loss: 0.89545\n",
      "batch 014 / 024 | loss: 0.89098\n",
      "batch 015 / 024 | loss: 0.89601\n",
      "batch 016 / 024 | loss: 0.89866\n",
      "batch 017 / 024 | loss: 0.89155\n",
      "batch 018 / 024 | loss: 0.88923\n",
      "batch 019 / 024 | loss: 0.89073\n",
      "batch 020 / 024 | loss: 0.89520\n",
      "batch 021 / 024 | loss: 0.89935\n",
      "batch 022 / 024 | loss: 0.90195\n",
      "batch 023 / 024 | loss: 0.89901\n",
      "batch 024 / 024 | loss: 0.89486\n",
      "----- epoch 005 / 010 | time: 108 sec | loss: 0.98390 | err: 0.47333\n",
      "batch 001 / 024 | loss: 0.88959\n",
      "batch 002 / 024 | loss: 0.83116\n",
      "batch 003 / 024 | loss: 0.87162\n",
      "batch 004 / 024 | loss: 0.87625\n",
      "batch 005 / 024 | loss: 0.88085\n",
      "batch 006 / 024 | loss: 0.90475\n",
      "batch 007 / 024 | loss: 0.90404\n",
      "batch 008 / 024 | loss: 0.91798\n",
      "batch 009 / 024 | loss: 0.91218\n",
      "batch 010 / 024 | loss: 0.90111\n",
      "batch 011 / 024 | loss: 0.90936\n",
      "batch 012 / 024 | loss: 0.91530\n",
      "batch 013 / 024 | loss: 0.92129\n",
      "batch 014 / 024 | loss: 0.92268\n",
      "batch 015 / 024 | loss: 0.91649\n",
      "batch 016 / 024 | loss: 0.91568\n",
      "batch 017 / 024 | loss: 0.91259\n",
      "batch 018 / 024 | loss: 0.92147\n",
      "batch 019 / 024 | loss: 0.92130\n",
      "batch 020 / 024 | loss: 0.90432\n",
      "batch 021 / 024 | loss: 0.90313\n",
      "batch 022 / 024 | loss: 0.90526\n",
      "batch 023 / 024 | loss: 0.90098\n",
      "batch 024 / 024 | loss: 0.89878\n",
      "----- epoch 006 / 010 | time: 111 sec | loss: 0.96475 | err: 0.46933\n",
      "batch 001 / 024 | loss: 0.76366\n",
      "batch 002 / 024 | loss: 0.80950\n",
      "batch 003 / 024 | loss: 0.85529\n",
      "batch 004 / 024 | loss: 0.85342\n",
      "batch 005 / 024 | loss: 0.89013\n",
      "batch 006 / 024 | loss: 0.90123\n",
      "batch 007 / 024 | loss: 0.89142\n",
      "batch 008 / 024 | loss: 0.89111\n",
      "batch 009 / 024 | loss: 0.89439\n",
      "batch 010 / 024 | loss: 0.88438\n",
      "batch 011 / 024 | loss: 0.86356\n",
      "batch 012 / 024 | loss: 0.87516\n",
      "batch 013 / 024 | loss: 0.88423\n",
      "batch 014 / 024 | loss: 0.87956\n",
      "batch 015 / 024 | loss: 0.87835\n",
      "batch 016 / 024 | loss: 0.88436\n",
      "batch 017 / 024 | loss: 0.87893\n",
      "batch 018 / 024 | loss: 0.87859\n",
      "batch 019 / 024 | loss: 0.88624\n",
      "batch 020 / 024 | loss: 0.89245\n",
      "batch 021 / 024 | loss: 0.89181\n",
      "batch 022 / 024 | loss: 0.89318\n",
      "batch 023 / 024 | loss: 0.89123\n",
      "batch 024 / 024 | loss: 0.88717\n",
      "training time: 931.0389201641083 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.1702293664271131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 55 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 56 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 57 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.86035\n",
      "batch 002 / 024 | loss: 0.97454\n",
      "batch 003 / 024 | loss: 0.94224\n",
      "batch 004 / 024 | loss: 0.93509\n",
      "batch 005 / 024 | loss: 0.95639\n",
      "batch 006 / 024 | loss: 0.95694\n",
      "batch 007 / 024 | loss: 0.96152\n",
      "batch 008 / 024 | loss: 0.95081\n",
      "batch 009 / 024 | loss: 0.95673\n",
      "batch 010 / 024 | loss: 0.95372\n",
      "batch 011 / 024 | loss: 0.94072\n",
      "batch 012 / 024 | loss: 0.94675\n",
      "batch 013 / 024 | loss: 0.94369\n",
      "batch 014 / 024 | loss: 0.95055\n",
      "batch 015 / 024 | loss: 0.95013\n",
      "batch 016 / 024 | loss: 0.95083\n",
      "batch 017 / 024 | loss: 0.95464\n",
      "batch 018 / 024 | loss: 0.96354\n",
      "batch 019 / 024 | loss: 0.96513\n",
      "batch 020 / 024 | loss: 0.96032\n",
      "batch 021 / 024 | loss: 0.95491\n",
      "batch 022 / 024 | loss: 0.95439\n",
      "batch 023 / 024 | loss: 0.95521\n",
      "batch 024 / 024 | loss: 0.94782\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 117 sec | loss: 0.99461 | err: 0.47333\n",
      "batch 001 / 024 | loss: 0.85580\n",
      "batch 002 / 024 | loss: 0.89290\n",
      "batch 003 / 024 | loss: 0.91217\n",
      "batch 004 / 024 | loss: 0.89296\n",
      "batch 005 / 024 | loss: 0.90857\n",
      "batch 006 / 024 | loss: 0.92578\n",
      "batch 007 / 024 | loss: 0.92060\n",
      "batch 008 / 024 | loss: 0.90482\n",
      "batch 009 / 024 | loss: 0.90630\n",
      "batch 010 / 024 | loss: 0.90836\n",
      "batch 011 / 024 | loss: 0.91471\n",
      "batch 012 / 024 | loss: 0.91226\n",
      "batch 013 / 024 | loss: 0.90584\n",
      "batch 014 / 024 | loss: 0.90214\n",
      "batch 015 / 024 | loss: 0.91082\n",
      "batch 016 / 024 | loss: 0.90901\n",
      "batch 017 / 024 | loss: 0.90776\n",
      "batch 018 / 024 | loss: 0.90285\n",
      "batch 019 / 024 | loss: 0.90445\n",
      "batch 020 / 024 | loss: 0.90238\n",
      "batch 021 / 024 | loss: 0.90421\n",
      "batch 022 / 024 | loss: 0.90424\n",
      "batch 023 / 024 | loss: 0.90214\n",
      "batch 024 / 024 | loss: 0.90166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved!\n",
      "----- epoch 002 / 010 | time: 113 sec | loss: 0.99836 | err: 0.47200\n",
      "batch 001 / 024 | loss: 0.79669\n",
      "batch 002 / 024 | loss: 0.78557\n",
      "batch 003 / 024 | loss: 0.80513\n",
      "batch 004 / 024 | loss: 0.82644\n",
      "batch 005 / 024 | loss: 0.86801\n",
      "batch 006 / 024 | loss: 0.87255\n",
      "batch 007 / 024 | loss: 0.89128\n",
      "batch 008 / 024 | loss: 0.88392\n",
      "batch 009 / 024 | loss: 0.88710\n",
      "batch 010 / 024 | loss: 0.89514\n",
      "batch 011 / 024 | loss: 0.89281\n",
      "batch 012 / 024 | loss: 0.89228\n",
      "batch 013 / 024 | loss: 0.89789\n",
      "batch 014 / 024 | loss: 0.89632\n",
      "batch 015 / 024 | loss: 0.88777\n",
      "batch 016 / 024 | loss: 0.89245\n",
      "batch 017 / 024 | loss: 0.89060\n",
      "batch 018 / 024 | loss: 0.89533\n",
      "batch 019 / 024 | loss: 0.88511\n",
      "batch 020 / 024 | loss: 0.88564\n",
      "batch 021 / 024 | loss: 0.88767\n",
      "batch 022 / 024 | loss: 0.89514\n",
      "batch 023 / 024 | loss: 0.89703\n",
      "batch 024 / 024 | loss: 0.89365\n",
      "----- epoch 003 / 010 | time: 111 sec | loss: 0.97574 | err: 0.47467\n",
      "batch 001 / 024 | loss: 0.66255\n",
      "batch 002 / 024 | loss: 0.82487\n",
      "batch 003 / 024 | loss: 0.83061\n",
      "batch 004 / 024 | loss: 0.88535\n",
      "batch 005 / 024 | loss: 0.87160\n",
      "batch 006 / 024 | loss: 0.84831\n",
      "batch 007 / 024 | loss: 0.84836\n",
      "batch 008 / 024 | loss: 0.85056\n",
      "batch 009 / 024 | loss: 0.82634\n",
      "batch 010 / 024 | loss: 0.84113\n",
      "batch 011 / 024 | loss: 0.85057\n",
      "batch 012 / 024 | loss: 0.85972\n",
      "batch 013 / 024 | loss: 0.84975\n",
      "batch 014 / 024 | loss: 0.85770\n",
      "batch 015 / 024 | loss: 0.86425\n",
      "batch 016 / 024 | loss: 0.87084\n",
      "batch 017 / 024 | loss: 0.87821\n",
      "batch 018 / 024 | loss: 0.88292\n",
      "batch 019 / 024 | loss: 0.88919\n",
      "batch 020 / 024 | loss: 0.89400\n",
      "batch 021 / 024 | loss: 0.90365\n",
      "batch 022 / 024 | loss: 0.89752\n",
      "batch 023 / 024 | loss: 0.89514\n",
      "batch 024 / 024 | loss: 0.89879\n",
      "----- epoch 004 / 010 | time: 111 sec | loss: 0.94917 | err: 0.47200\n",
      "batch 001 / 024 | loss: 1.11148\n",
      "batch 002 / 024 | loss: 0.98390\n",
      "batch 003 / 024 | loss: 1.00487\n",
      "batch 004 / 024 | loss: 0.92266\n",
      "batch 005 / 024 | loss: 0.93487\n",
      "batch 006 / 024 | loss: 0.91424\n",
      "batch 007 / 024 | loss: 0.91397\n",
      "batch 008 / 024 | loss: 0.90060\n",
      "batch 009 / 024 | loss: 0.91172\n",
      "batch 010 / 024 | loss: 0.90261\n",
      "batch 011 / 024 | loss: 0.90062\n",
      "batch 012 / 024 | loss: 0.90962\n",
      "batch 013 / 024 | loss: 0.89949\n",
      "batch 014 / 024 | loss: 0.89415\n",
      "batch 015 / 024 | loss: 0.90004\n",
      "batch 016 / 024 | loss: 0.90275\n",
      "batch 017 / 024 | loss: 0.89487\n",
      "batch 018 / 024 | loss: 0.89247\n",
      "batch 019 / 024 | loss: 0.89388\n",
      "batch 020 / 024 | loss: 0.89836\n",
      "batch 021 / 024 | loss: 0.90256\n",
      "batch 022 / 024 | loss: 0.90503\n",
      "batch 023 / 024 | loss: 0.90206\n",
      "batch 024 / 024 | loss: 0.89789\n",
      "----- epoch 005 / 010 | time: 106 sec | loss: 0.98127 | err: 0.47333\n",
      "batch 001 / 024 | loss: 0.89517\n",
      "batch 002 / 024 | loss: 0.83543\n",
      "batch 003 / 024 | loss: 0.87554\n",
      "batch 004 / 024 | loss: 0.87894\n",
      "batch 005 / 024 | loss: 0.88360\n",
      "batch 006 / 024 | loss: 0.90752\n",
      "batch 007 / 024 | loss: 0.90698\n",
      "batch 008 / 024 | loss: 0.92063\n",
      "batch 009 / 024 | loss: 0.91437\n",
      "batch 010 / 024 | loss: 0.90352\n",
      "batch 011 / 024 | loss: 0.91360\n",
      "batch 012 / 024 | loss: 0.91931\n",
      "batch 013 / 024 | loss: 0.92497\n",
      "batch 014 / 024 | loss: 0.92597\n",
      "batch 015 / 024 | loss: 0.91960\n",
      "batch 016 / 024 | loss: 0.91875\n",
      "batch 017 / 024 | loss: 0.91590\n",
      "batch 018 / 024 | loss: 0.92456\n",
      "batch 019 / 024 | loss: 0.92421\n",
      "batch 020 / 024 | loss: 0.90687\n",
      "batch 021 / 024 | loss: 0.90585\n",
      "batch 022 / 024 | loss: 0.90776\n",
      "batch 023 / 024 | loss: 0.90351\n",
      "batch 024 / 024 | loss: 0.90173\n",
      "model saved!\n",
      "----- epoch 006 / 010 | time: 113 sec | loss: 0.98437 | err: 0.46933\n",
      "batch 001 / 024 | loss: 0.76495\n",
      "batch 002 / 024 | loss: 0.80309\n",
      "batch 003 / 024 | loss: 0.85335\n",
      "batch 004 / 024 | loss: 0.85181\n",
      "batch 005 / 024 | loss: 0.89127\n",
      "batch 006 / 024 | loss: 0.90136\n",
      "batch 007 / 024 | loss: 0.89209\n",
      "batch 008 / 024 | loss: 0.89199\n",
      "batch 009 / 024 | loss: 0.89479\n",
      "batch 010 / 024 | loss: 0.88497\n",
      "batch 011 / 024 | loss: 0.86429\n",
      "batch 012 / 024 | loss: 0.87598\n",
      "batch 013 / 024 | loss: 0.88534\n",
      "batch 014 / 024 | loss: 0.88086\n",
      "batch 015 / 024 | loss: 0.87960\n",
      "batch 016 / 024 | loss: 0.88565\n",
      "batch 017 / 024 | loss: 0.88025\n",
      "batch 018 / 024 | loss: 0.87978\n",
      "batch 019 / 024 | loss: 0.88728\n",
      "batch 020 / 024 | loss: 0.89432\n",
      "batch 021 / 024 | loss: 0.89360\n",
      "batch 022 / 024 | loss: 0.89488\n",
      "batch 023 / 024 | loss: 0.89274\n",
      "batch 024 / 024 | loss: 0.88874\n",
      "----- epoch 007 / 010 | time: 108 sec | loss: 0.95498 | err: 0.47467\n",
      "batch 001 / 024 | loss: 0.90472\n",
      "batch 002 / 024 | loss: 0.86409\n",
      "batch 003 / 024 | loss: 0.90106\n",
      "batch 004 / 024 | loss: 0.90408\n",
      "batch 005 / 024 | loss: 0.90370\n",
      "batch 006 / 024 | loss: 0.90531\n",
      "batch 007 / 024 | loss: 0.89212\n",
      "batch 008 / 024 | loss: 0.89977\n",
      "batch 009 / 024 | loss: 0.92080\n",
      "batch 010 / 024 | loss: 0.90655\n",
      "batch 011 / 024 | loss: 0.88632\n",
      "batch 012 / 024 | loss: 0.87283\n",
      "batch 013 / 024 | loss: 0.86979\n",
      "batch 014 / 024 | loss: 0.88093\n",
      "batch 015 / 024 | loss: 0.87688\n",
      "batch 016 / 024 | loss: 0.86923\n",
      "batch 017 / 024 | loss: 0.86412\n",
      "batch 018 / 024 | loss: 0.87108\n",
      "batch 019 / 024 | loss: 0.87645\n",
      "batch 020 / 024 | loss: 0.88251\n",
      "batch 021 / 024 | loss: 0.88662\n",
      "batch 022 / 024 | loss: 0.88875\n",
      "batch 023 / 024 | loss: 0.88724\n",
      "batch 024 / 024 | loss: 0.89369\n",
      "----- epoch 008 / 010 | time: 106 sec | loss: 0.92963 | err: 0.47733\n",
      "batch 001 / 024 | loss: 0.94062\n",
      "batch 002 / 024 | loss: 0.92727\n",
      "batch 003 / 024 | loss: 0.92743\n",
      "batch 004 / 024 | loss: 0.93638\n",
      "batch 005 / 024 | loss: 0.92604\n",
      "batch 006 / 024 | loss: 0.92620\n",
      "batch 007 / 024 | loss: 0.89858\n",
      "batch 008 / 024 | loss: 0.89678\n",
      "batch 009 / 024 | loss: 0.91786\n",
      "batch 010 / 024 | loss: 0.91957\n",
      "batch 011 / 024 | loss: 0.91655\n",
      "batch 012 / 024 | loss: 0.90907\n",
      "batch 013 / 024 | loss: 0.90743\n",
      "batch 014 / 024 | loss: 0.90487\n",
      "batch 015 / 024 | loss: 0.89687\n",
      "batch 016 / 024 | loss: 0.89189\n",
      "batch 017 / 024 | loss: 0.88501\n",
      "batch 018 / 024 | loss: 0.88220\n",
      "batch 019 / 024 | loss: 0.87938\n",
      "batch 020 / 024 | loss: 0.88094\n",
      "batch 021 / 024 | loss: 0.88524\n",
      "batch 022 / 024 | loss: 0.89129\n",
      "batch 023 / 024 | loss: 0.89149\n",
      "batch 024 / 024 | loss: 0.89960\n",
      "----- epoch 009 / 010 | time: 102 sec | loss: 0.91679 | err: 0.47867\n",
      "batch 001 / 024 | loss: 0.73747\n",
      "batch 002 / 024 | loss: 0.86133\n",
      "batch 003 / 024 | loss: 0.90828\n",
      "batch 004 / 024 | loss: 0.89810\n",
      "batch 005 / 024 | loss: 0.89661\n",
      "batch 006 / 024 | loss: 0.89554\n",
      "batch 007 / 024 | loss: 0.88434\n",
      "batch 008 / 024 | loss: 0.90218\n",
      "batch 009 / 024 | loss: 0.90110\n",
      "batch 010 / 024 | loss: 0.90044\n",
      "batch 011 / 024 | loss: 0.90779\n",
      "batch 012 / 024 | loss: 0.90667\n",
      "batch 013 / 024 | loss: 0.89617\n",
      "batch 014 / 024 | loss: 0.90146\n",
      "batch 015 / 024 | loss: 0.90741\n",
      "batch 016 / 024 | loss: 0.90755\n",
      "batch 017 / 024 | loss: 0.90193\n",
      "batch 018 / 024 | loss: 0.89084\n",
      "batch 019 / 024 | loss: 0.89067\n",
      "batch 020 / 024 | loss: 0.88922\n",
      "batch 021 / 024 | loss: 0.89233\n",
      "batch 022 / 024 | loss: 0.88983\n",
      "batch 023 / 024 | loss: 0.89051\n",
      "batch 024 / 024 | loss: 0.89524\n",
      "----- epoch 010 / 010 | time: 107 sec | loss: 1.00767 | err: 0.47200\n",
      "training time: 1099.5078208446503 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.2011167655419465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 58 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 59 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 60 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.87456\n",
      "batch 002 / 024 | loss: 0.98841\n",
      "batch 003 / 024 | loss: 0.94671\n",
      "batch 004 / 024 | loss: 0.93399\n",
      "batch 005 / 024 | loss: 0.95526\n",
      "batch 006 / 024 | loss: 0.95762\n",
      "batch 007 / 024 | loss: 0.96690\n",
      "batch 008 / 024 | loss: 0.95625\n",
      "batch 009 / 024 | loss: 0.96463\n",
      "batch 010 / 024 | loss: 0.96154\n",
      "batch 011 / 024 | loss: 0.94694\n",
      "batch 012 / 024 | loss: 0.95367\n",
      "batch 013 / 024 | loss: 0.95069\n",
      "batch 014 / 024 | loss: 0.95827\n",
      "batch 015 / 024 | loss: 0.95757\n",
      "batch 016 / 024 | loss: 0.95851\n",
      "batch 017 / 024 | loss: 0.96180\n",
      "batch 018 / 024 | loss: 0.97077\n",
      "batch 019 / 024 | loss: 0.97189\n",
      "batch 020 / 024 | loss: 0.96698\n",
      "batch 021 / 024 | loss: 0.96113\n",
      "batch 022 / 024 | loss: 0.96047\n",
      "batch 023 / 024 | loss: 0.96135\n",
      "batch 024 / 024 | loss: 0.95344\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 116 sec | loss: 1.00192 | err: 0.47333\n",
      "batch 001 / 024 | loss: 0.85528\n",
      "batch 002 / 024 | loss: 0.89401\n",
      "batch 003 / 024 | loss: 0.91373\n",
      "batch 004 / 024 | loss: 0.89325\n",
      "batch 005 / 024 | loss: 0.90921\n",
      "batch 006 / 024 | loss: 0.92710\n",
      "batch 007 / 024 | loss: 0.92192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 008 / 024 | loss: 0.90583\n",
      "batch 009 / 024 | loss: 0.90778\n",
      "batch 010 / 024 | loss: 0.91125\n",
      "batch 011 / 024 | loss: 0.91761\n",
      "batch 012 / 024 | loss: 0.91505\n",
      "batch 013 / 024 | loss: 0.90842\n",
      "batch 014 / 024 | loss: 0.90456\n",
      "batch 015 / 024 | loss: 0.91489\n",
      "batch 016 / 024 | loss: 0.91330\n",
      "batch 017 / 024 | loss: 0.91199\n",
      "batch 018 / 024 | loss: 0.90693\n",
      "batch 019 / 024 | loss: 0.90837\n",
      "batch 020 / 024 | loss: 0.90621\n",
      "batch 021 / 024 | loss: 0.90793\n",
      "batch 022 / 024 | loss: 0.90792\n",
      "batch 023 / 024 | loss: 0.90584\n",
      "batch 024 / 024 | loss: 0.90540\n",
      "----- epoch 002 / 010 | time: 151 sec | loss: 0.99530 | err: 0.47333\n",
      "batch 001 / 024 | loss: 0.80221\n",
      "batch 002 / 024 | loss: 0.78573\n",
      "batch 003 / 024 | loss: 0.80288\n",
      "batch 004 / 024 | loss: 0.82479\n",
      "batch 005 / 024 | loss: 0.86741\n",
      "batch 006 / 024 | loss: 0.87216\n",
      "batch 007 / 024 | loss: 0.89154\n",
      "batch 008 / 024 | loss: 0.88377\n",
      "batch 009 / 024 | loss: 0.88659\n",
      "batch 010 / 024 | loss: 0.89523\n",
      "batch 011 / 024 | loss: 0.89261\n",
      "batch 012 / 024 | loss: 0.89231\n",
      "batch 013 / 024 | loss: 0.89843\n",
      "batch 014 / 024 | loss: 0.89707\n",
      "batch 015 / 024 | loss: 0.88845\n",
      "batch 016 / 024 | loss: 0.89325\n",
      "batch 017 / 024 | loss: 0.89146\n",
      "batch 018 / 024 | loss: 0.89631\n",
      "batch 019 / 024 | loss: 0.88612\n",
      "batch 020 / 024 | loss: 0.88681\n",
      "batch 021 / 024 | loss: 0.88934\n",
      "batch 022 / 024 | loss: 0.89655\n",
      "batch 023 / 024 | loss: 0.89930\n",
      "batch 024 / 024 | loss: 0.89600\n",
      "----- epoch 003 / 010 | time: 151 sec | loss: 0.95397 | err: 0.47600\n",
      "batch 001 / 024 | loss: 0.66648\n",
      "batch 002 / 024 | loss: 0.82634\n",
      "batch 003 / 024 | loss: 0.83221\n",
      "batch 004 / 024 | loss: 0.88695\n",
      "batch 005 / 024 | loss: 0.87335\n",
      "batch 006 / 024 | loss: 0.84992\n",
      "batch 007 / 024 | loss: 0.84993\n",
      "batch 008 / 024 | loss: 0.85233\n",
      "batch 009 / 024 | loss: 0.82768\n",
      "batch 010 / 024 | loss: 0.84329\n",
      "batch 011 / 024 | loss: 0.85254\n",
      "batch 012 / 024 | loss: 0.86176\n",
      "batch 013 / 024 | loss: 0.85154\n",
      "batch 014 / 024 | loss: 0.85896\n",
      "batch 015 / 024 | loss: 0.86548\n",
      "batch 016 / 024 | loss: 0.87203\n",
      "batch 017 / 024 | loss: 0.88071\n",
      "batch 018 / 024 | loss: 0.88545\n",
      "batch 019 / 024 | loss: 0.89230\n",
      "batch 020 / 024 | loss: 0.89717\n",
      "batch 021 / 024 | loss: 0.90674\n",
      "batch 022 / 024 | loss: 0.90064\n",
      "batch 023 / 024 | loss: 0.89824\n",
      "batch 024 / 024 | loss: 0.90198\n",
      "model saved!\n",
      "----- epoch 004 / 010 | time: 123 sec | loss: 0.95362 | err: 0.47200\n",
      "batch 001 / 024 | loss: 1.13813\n",
      "batch 002 / 024 | loss: 0.99661\n",
      "batch 003 / 024 | loss: 1.01448\n",
      "batch 004 / 024 | loss: 0.93041\n",
      "batch 005 / 024 | loss: 0.94231\n",
      "batch 006 / 024 | loss: 0.92079\n",
      "batch 007 / 024 | loss: 0.91960\n",
      "batch 008 / 024 | loss: 0.90525\n",
      "batch 009 / 024 | loss: 0.91600\n",
      "batch 010 / 024 | loss: 0.90678\n",
      "batch 011 / 024 | loss: 0.90474\n",
      "batch 012 / 024 | loss: 0.91349\n",
      "batch 013 / 024 | loss: 0.90291\n",
      "batch 014 / 024 | loss: 0.89692\n",
      "batch 015 / 024 | loss: 0.90374\n",
      "batch 016 / 024 | loss: 0.90660\n",
      "batch 017 / 024 | loss: 0.89820\n",
      "batch 018 / 024 | loss: 0.89576\n",
      "batch 019 / 024 | loss: 0.89721\n",
      "batch 020 / 024 | loss: 0.90150\n",
      "batch 021 / 024 | loss: 0.90551\n",
      "batch 022 / 024 | loss: 0.90783\n",
      "batch 023 / 024 | loss: 0.90479\n",
      "batch 024 / 024 | loss: 0.90045\n",
      "----- epoch 005 / 010 | time: 109 sec | loss: 0.98893 | err: 0.47467\n",
      "batch 001 / 024 | loss: 0.89893\n",
      "batch 002 / 024 | loss: 0.83754\n",
      "batch 003 / 024 | loss: 0.87796\n",
      "batch 004 / 024 | loss: 0.88143\n",
      "batch 005 / 024 | loss: 0.88708\n",
      "batch 006 / 024 | loss: 0.91078\n",
      "batch 007 / 024 | loss: 0.90995\n",
      "batch 008 / 024 | loss: 0.92342\n",
      "batch 009 / 024 | loss: 0.91741\n",
      "batch 010 / 024 | loss: 0.90683\n",
      "batch 011 / 024 | loss: 0.92000\n",
      "batch 012 / 024 | loss: 0.92595\n",
      "batch 013 / 024 | loss: 0.93155\n",
      "batch 014 / 024 | loss: 0.93223\n",
      "batch 015 / 024 | loss: 0.92544\n",
      "batch 016 / 024 | loss: 0.92442\n",
      "batch 017 / 024 | loss: 0.92170\n",
      "batch 018 / 024 | loss: 0.93006\n",
      "batch 019 / 024 | loss: 0.92937\n",
      "batch 020 / 024 | loss: 0.91174\n",
      "batch 021 / 024 | loss: 0.91080\n",
      "batch 022 / 024 | loss: 0.91268\n",
      "batch 023 / 024 | loss: 0.90855\n",
      "batch 024 / 024 | loss: 0.90677\n",
      "----- epoch 006 / 010 | time: 112 sec | loss: 0.97371 | err: 0.47200\n",
      "batch 001 / 024 | loss: 0.76799\n",
      "batch 002 / 024 | loss: 0.80518\n",
      "batch 003 / 024 | loss: 0.85451\n",
      "batch 004 / 024 | loss: 0.85309\n",
      "batch 005 / 024 | loss: 0.89570\n",
      "batch 006 / 024 | loss: 0.90532\n",
      "batch 007 / 024 | loss: 0.89528\n",
      "batch 008 / 024 | loss: 0.89479\n",
      "batch 009 / 024 | loss: 0.89734\n",
      "batch 010 / 024 | loss: 0.88720\n",
      "batch 011 / 024 | loss: 0.86595\n",
      "batch 012 / 024 | loss: 0.87733\n",
      "batch 013 / 024 | loss: 0.88676\n",
      "batch 014 / 024 | loss: 0.88227\n",
      "batch 015 / 024 | loss: 0.88111\n",
      "batch 016 / 024 | loss: 0.88723\n",
      "batch 017 / 024 | loss: 0.88188\n",
      "batch 018 / 024 | loss: 0.88130\n",
      "batch 019 / 024 | loss: 0.88876\n",
      "batch 020 / 024 | loss: 0.89695\n",
      "batch 021 / 024 | loss: 0.89631\n",
      "batch 022 / 024 | loss: 0.89756\n",
      "batch 023 / 024 | loss: 0.89542\n",
      "batch 024 / 024 | loss: 0.89139\n",
      "----- epoch 007 / 010 | time: 113 sec | loss: 0.95619 | err: 0.47467\n",
      "batch 001 / 024 | loss: 0.90594\n",
      "batch 002 / 024 | loss: 0.86484\n",
      "batch 003 / 024 | loss: 0.90162\n",
      "batch 004 / 024 | loss: 0.90420\n",
      "batch 005 / 024 | loss: 0.90448\n",
      "batch 006 / 024 | loss: 0.90668\n",
      "batch 007 / 024 | loss: 0.89364\n",
      "batch 008 / 024 | loss: 0.90140\n",
      "batch 009 / 024 | loss: 0.92274\n",
      "batch 010 / 024 | loss: 0.90822\n",
      "batch 011 / 024 | loss: 0.88750\n",
      "batch 012 / 024 | loss: 0.87362\n",
      "batch 013 / 024 | loss: 0.87054\n",
      "batch 014 / 024 | loss: 0.88194\n",
      "batch 015 / 024 | loss: 0.87777\n",
      "batch 016 / 024 | loss: 0.87002\n",
      "batch 017 / 024 | loss: 0.86509\n",
      "batch 018 / 024 | loss: 0.87223\n",
      "batch 019 / 024 | loss: 0.87771\n",
      "batch 020 / 024 | loss: 0.88470\n",
      "batch 021 / 024 | loss: 0.88921\n",
      "batch 022 / 024 | loss: 0.89117\n",
      "batch 023 / 024 | loss: 0.88948\n",
      "batch 024 / 024 | loss: 0.89592\n",
      "----- epoch 008 / 010 | time: 149 sec | loss: 0.94261 | err: 0.47733\n",
      "batch 001 / 024 | loss: 0.93703\n",
      "batch 002 / 024 | loss: 0.92545\n",
      "batch 003 / 024 | loss: 0.92662\n",
      "batch 004 / 024 | loss: 0.93670\n",
      "batch 005 / 024 | loss: 0.92595\n",
      "batch 006 / 024 | loss: 0.92631\n",
      "batch 007 / 024 | loss: 0.89919\n",
      "batch 008 / 024 | loss: 0.89695\n",
      "batch 009 / 024 | loss: 0.91803\n",
      "batch 010 / 024 | loss: 0.91945\n",
      "batch 011 / 024 | loss: 0.91616\n",
      "batch 012 / 024 | loss: 0.90848\n",
      "batch 013 / 024 | loss: 0.90686\n",
      "batch 014 / 024 | loss: 0.90471\n",
      "batch 015 / 024 | loss: 0.89678\n",
      "batch 016 / 024 | loss: 0.89188\n",
      "batch 017 / 024 | loss: 0.88495\n",
      "batch 018 / 024 | loss: 0.88205\n",
      "batch 019 / 024 | loss: 0.87919\n",
      "batch 020 / 024 | loss: 0.88087\n",
      "batch 021 / 024 | loss: 0.88543\n",
      "batch 022 / 024 | loss: 0.89172\n",
      "batch 023 / 024 | loss: 0.89200\n",
      "batch 024 / 024 | loss: 0.90156\n",
      "training time: 1171.706725358963 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.23760855268983702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 61 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 62 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 63 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.89134\n",
      "batch 002 / 024 | loss: 1.00960\n",
      "batch 003 / 024 | loss: 0.96156\n",
      "batch 004 / 024 | loss: 0.94519\n",
      "batch 005 / 024 | loss: 0.96448\n",
      "batch 006 / 024 | loss: 0.96658\n",
      "batch 007 / 024 | loss: 0.97530\n",
      "batch 008 / 024 | loss: 0.96450\n",
      "batch 009 / 024 | loss: 0.97489\n",
      "batch 010 / 024 | loss: 0.97301\n",
      "batch 011 / 024 | loss: 0.95732\n",
      "batch 012 / 024 | loss: 0.96396\n",
      "batch 013 / 024 | loss: 0.96061\n",
      "batch 014 / 024 | loss: 0.97033\n",
      "batch 015 / 024 | loss: 0.96983\n",
      "batch 016 / 024 | loss: 0.97051\n",
      "batch 017 / 024 | loss: 0.97404\n",
      "batch 018 / 024 | loss: 0.98265\n",
      "batch 019 / 024 | loss: 0.98337\n",
      "batch 020 / 024 | loss: 0.97781\n",
      "batch 021 / 024 | loss: 0.97142\n",
      "batch 022 / 024 | loss: 0.97030\n",
      "batch 023 / 024 | loss: 0.97071\n",
      "batch 024 / 024 | loss: 0.96221\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 169 sec | loss: 1.01718 | err: 0.47467\n",
      "batch 001 / 024 | loss: 0.85445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 002 / 024 | loss: 0.89379\n",
      "batch 003 / 024 | loss: 0.91411\n",
      "batch 004 / 024 | loss: 0.89287\n",
      "batch 005 / 024 | loss: 0.91043\n",
      "batch 006 / 024 | loss: 0.92905\n",
      "batch 007 / 024 | loss: 0.92450\n",
      "batch 008 / 024 | loss: 0.90832\n",
      "batch 009 / 024 | loss: 0.91094\n",
      "batch 010 / 024 | loss: 0.91508\n",
      "batch 011 / 024 | loss: 0.92135\n",
      "batch 012 / 024 | loss: 0.91883\n",
      "batch 013 / 024 | loss: 0.91222\n",
      "batch 014 / 024 | loss: 0.90817\n",
      "batch 015 / 024 | loss: 0.92027\n",
      "batch 016 / 024 | loss: 0.91868\n",
      "batch 017 / 024 | loss: 0.91734\n",
      "batch 018 / 024 | loss: 0.91210\n",
      "batch 019 / 024 | loss: 0.91342\n",
      "batch 020 / 024 | loss: 0.91121\n",
      "batch 021 / 024 | loss: 0.91279\n",
      "batch 022 / 024 | loss: 0.91264\n",
      "batch 023 / 024 | loss: 0.91067\n",
      "batch 024 / 024 | loss: 0.91018\n",
      "----- epoch 002 / 010 | time: 172 sec | loss: 0.99032 | err: 0.47467\n",
      "batch 001 / 024 | loss: 0.80189\n",
      "batch 002 / 024 | loss: 0.78443\n",
      "batch 003 / 024 | loss: 0.80102\n",
      "batch 004 / 024 | loss: 0.82406\n",
      "batch 005 / 024 | loss: 0.86750\n",
      "batch 006 / 024 | loss: 0.87205\n",
      "batch 007 / 024 | loss: 0.89203\n",
      "batch 008 / 024 | loss: 0.88451\n",
      "batch 009 / 024 | loss: 0.88724\n",
      "batch 010 / 024 | loss: 0.89596\n",
      "batch 011 / 024 | loss: 0.89325\n",
      "batch 012 / 024 | loss: 0.89294\n",
      "batch 013 / 024 | loss: 0.89902\n",
      "batch 014 / 024 | loss: 0.89756\n",
      "batch 015 / 024 | loss: 0.88899\n",
      "batch 016 / 024 | loss: 0.89406\n",
      "batch 017 / 024 | loss: 0.89230\n",
      "batch 018 / 024 | loss: 0.89717\n",
      "batch 019 / 024 | loss: 0.88696\n",
      "batch 020 / 024 | loss: 0.88765\n",
      "batch 021 / 024 | loss: 0.89080\n",
      "batch 022 / 024 | loss: 0.89810\n",
      "batch 023 / 024 | loss: 0.90185\n",
      "batch 024 / 024 | loss: 0.89840\n",
      "----- epoch 003 / 010 | time: 167 sec | loss: 0.95195 | err: 0.47733\n",
      "batch 001 / 024 | loss: 0.66214\n",
      "batch 002 / 024 | loss: 0.82445\n",
      "batch 003 / 024 | loss: 0.83089\n",
      "batch 004 / 024 | loss: 0.88635\n",
      "batch 005 / 024 | loss: 0.87292\n",
      "batch 006 / 024 | loss: 0.84892\n",
      "batch 007 / 024 | loss: 0.84906\n",
      "batch 008 / 024 | loss: 0.85160\n",
      "batch 009 / 024 | loss: 0.82662\n",
      "batch 010 / 024 | loss: 0.84241\n",
      "batch 011 / 024 | loss: 0.85163\n",
      "batch 012 / 024 | loss: 0.86101\n",
      "batch 013 / 024 | loss: 0.85084\n",
      "batch 014 / 024 | loss: 0.85837\n",
      "batch 015 / 024 | loss: 0.86492\n",
      "batch 016 / 024 | loss: 0.87140\n",
      "batch 017 / 024 | loss: 0.88152\n",
      "batch 018 / 024 | loss: 0.88620\n",
      "batch 019 / 024 | loss: 0.89376\n",
      "batch 020 / 024 | loss: 0.89880\n",
      "batch 021 / 024 | loss: 0.90851\n",
      "batch 022 / 024 | loss: 0.90219\n",
      "batch 023 / 024 | loss: 0.89960\n",
      "batch 024 / 024 | loss: 0.90317\n",
      "----- epoch 004 / 010 | time: 157 sec | loss: 0.94571 | err: 0.47467\n",
      "batch 001 / 024 | loss: 1.16487\n",
      "batch 002 / 024 | loss: 1.00915\n",
      "batch 003 / 024 | loss: 1.02335\n",
      "batch 004 / 024 | loss: 0.93728\n",
      "batch 005 / 024 | loss: 0.94920\n",
      "batch 006 / 024 | loss: 0.92672\n",
      "batch 007 / 024 | loss: 0.92453\n",
      "batch 008 / 024 | loss: 0.90898\n",
      "batch 009 / 024 | loss: 0.91970\n",
      "batch 010 / 024 | loss: 0.91021\n",
      "batch 011 / 024 | loss: 0.90795\n",
      "batch 012 / 024 | loss: 0.91687\n",
      "batch 013 / 024 | loss: 0.90593\n",
      "batch 014 / 024 | loss: 0.89955\n",
      "batch 015 / 024 | loss: 0.90752\n",
      "batch 016 / 024 | loss: 0.91036\n",
      "batch 017 / 024 | loss: 0.90178\n",
      "batch 018 / 024 | loss: 0.89919\n",
      "batch 019 / 024 | loss: 0.90054\n",
      "batch 020 / 024 | loss: 0.90476\n",
      "batch 021 / 024 | loss: 0.90870\n",
      "batch 022 / 024 | loss: 0.91082\n",
      "batch 023 / 024 | loss: 0.90757\n",
      "batch 024 / 024 | loss: 0.90320\n",
      "model saved!\n",
      "----- epoch 005 / 010 | time: 152 sec | loss: 0.99373 | err: 0.47333\n",
      "batch 001 / 024 | loss: 0.90092\n",
      "batch 002 / 024 | loss: 0.83899\n",
      "batch 003 / 024 | loss: 0.87845\n",
      "batch 004 / 024 | loss: 0.88079\n",
      "batch 005 / 024 | loss: 0.88605\n",
      "batch 006 / 024 | loss: 0.90930\n",
      "batch 007 / 024 | loss: 0.90890\n",
      "batch 008 / 024 | loss: 0.92278\n",
      "batch 009 / 024 | loss: 0.91705\n",
      "batch 010 / 024 | loss: 0.90656\n",
      "batch 011 / 024 | loss: 0.92293\n",
      "batch 012 / 024 | loss: 0.92875\n",
      "batch 013 / 024 | loss: 0.93400\n",
      "batch 014 / 024 | loss: 0.93434\n",
      "batch 015 / 024 | loss: 0.92739\n",
      "batch 016 / 024 | loss: 0.92629\n",
      "batch 017 / 024 | loss: 0.92428\n",
      "batch 018 / 024 | loss: 0.93283\n",
      "batch 019 / 024 | loss: 0.93203\n",
      "batch 020 / 024 | loss: 0.91418\n",
      "batch 021 / 024 | loss: 0.91311\n",
      "batch 022 / 024 | loss: 0.91471\n",
      "batch 023 / 024 | loss: 0.91041\n",
      "batch 024 / 024 | loss: 0.90846\n",
      "----- epoch 006 / 010 | time: 164 sec | loss: 0.96494 | err: 0.47600\n",
      "batch 001 / 024 | loss: 0.77205\n",
      "batch 002 / 024 | loss: 0.80834\n",
      "batch 003 / 024 | loss: 0.85710\n",
      "batch 004 / 024 | loss: 0.85488\n",
      "batch 005 / 024 | loss: 0.90030\n",
      "batch 006 / 024 | loss: 0.90885\n",
      "batch 007 / 024 | loss: 0.89843\n",
      "batch 008 / 024 | loss: 0.89767\n",
      "batch 009 / 024 | loss: 0.89975\n",
      "batch 010 / 024 | loss: 0.88976\n",
      "batch 011 / 024 | loss: 0.86875\n",
      "batch 012 / 024 | loss: 0.87987\n",
      "batch 013 / 024 | loss: 0.88876\n",
      "batch 014 / 024 | loss: 0.88372\n",
      "batch 015 / 024 | loss: 0.88235\n",
      "batch 016 / 024 | loss: 0.88843\n",
      "batch 017 / 024 | loss: 0.88278\n",
      "batch 018 / 024 | loss: 0.88241\n",
      "batch 019 / 024 | loss: 0.88983\n",
      "batch 020 / 024 | loss: 0.89941\n",
      "batch 021 / 024 | loss: 0.89873\n",
      "batch 022 / 024 | loss: 0.90003\n",
      "batch 023 / 024 | loss: 0.89797\n",
      "batch 024 / 024 | loss: 0.89392\n",
      "----- epoch 007 / 010 | time: 118 sec | loss: 0.94373 | err: 0.47467\n",
      "batch 001 / 024 | loss: 0.90723\n",
      "batch 002 / 024 | loss: 0.86706\n",
      "batch 003 / 024 | loss: 0.90403\n",
      "batch 004 / 024 | loss: 0.90658\n",
      "batch 005 / 024 | loss: 0.90770\n",
      "batch 006 / 024 | loss: 0.91069\n",
      "batch 007 / 024 | loss: 0.89842\n",
      "batch 008 / 024 | loss: 0.90551\n",
      "batch 009 / 024 | loss: 0.92682\n",
      "batch 010 / 024 | loss: 0.91158\n",
      "batch 011 / 024 | loss: 0.89018\n",
      "batch 012 / 024 | loss: 0.87576\n",
      "batch 013 / 024 | loss: 0.87282\n",
      "batch 014 / 024 | loss: 0.88474\n",
      "batch 015 / 024 | loss: 0.88067\n",
      "batch 016 / 024 | loss: 0.87280\n",
      "batch 017 / 024 | loss: 0.86800\n",
      "batch 018 / 024 | loss: 0.87524\n",
      "batch 019 / 024 | loss: 0.88083\n",
      "batch 020 / 024 | loss: 0.88857\n",
      "batch 021 / 024 | loss: 0.89348\n",
      "batch 022 / 024 | loss: 0.89523\n",
      "batch 023 / 024 | loss: 0.89325\n",
      "batch 024 / 024 | loss: 0.89949\n",
      "----- epoch 008 / 010 | time: 123 sec | loss: 0.94361 | err: 0.47733\n",
      "batch 001 / 024 | loss: 0.93685\n",
      "batch 002 / 024 | loss: 0.92773\n",
      "batch 003 / 024 | loss: 0.92817\n",
      "batch 004 / 024 | loss: 0.93766\n",
      "batch 005 / 024 | loss: 0.92837\n",
      "batch 006 / 024 | loss: 0.92842\n",
      "batch 007 / 024 | loss: 0.90171\n",
      "batch 008 / 024 | loss: 0.89945\n",
      "batch 009 / 024 | loss: 0.92084\n",
      "batch 010 / 024 | loss: 0.92222\n",
      "batch 011 / 024 | loss: 0.91891\n",
      "batch 012 / 024 | loss: 0.91131\n",
      "batch 013 / 024 | loss: 0.90949\n",
      "batch 014 / 024 | loss: 0.90871\n",
      "batch 015 / 024 | loss: 0.90066\n",
      "batch 016 / 024 | loss: 0.89572\n",
      "batch 017 / 024 | loss: 0.88920\n",
      "batch 018 / 024 | loss: 0.88666\n",
      "batch 019 / 024 | loss: 0.88400\n",
      "batch 020 / 024 | loss: 0.88572\n",
      "batch 021 / 024 | loss: 0.89056\n",
      "batch 022 / 024 | loss: 0.89728\n",
      "batch 023 / 024 | loss: 0.89772\n",
      "batch 024 / 024 | loss: 0.91000\n",
      "----- epoch 009 / 010 | time: 127 sec | loss: 0.93380 | err: 0.47867\n",
      "batch 001 / 024 | loss: 0.72902\n",
      "batch 002 / 024 | loss: 0.85438\n",
      "batch 003 / 024 | loss: 0.90366\n",
      "batch 004 / 024 | loss: 0.89473\n",
      "batch 005 / 024 | loss: 0.89508\n",
      "batch 006 / 024 | loss: 0.89582\n",
      "batch 007 / 024 | loss: 0.88488\n",
      "batch 008 / 024 | loss: 0.90297\n",
      "batch 009 / 024 | loss: 0.90365\n",
      "batch 010 / 024 | loss: 0.90308\n",
      "batch 011 / 024 | loss: 0.91002\n",
      "batch 012 / 024 | loss: 0.91286\n",
      "batch 013 / 024 | loss: 0.90293\n",
      "batch 014 / 024 | loss: 0.90966\n",
      "batch 015 / 024 | loss: 0.91541\n",
      "batch 016 / 024 | loss: 0.91527\n",
      "batch 017 / 024 | loss: 0.90883\n",
      "batch 018 / 024 | loss: 0.89752\n",
      "batch 019 / 024 | loss: 0.89745\n",
      "batch 020 / 024 | loss: 0.89584\n",
      "batch 021 / 024 | loss: 0.89863\n",
      "batch 022 / 024 | loss: 0.89618\n",
      "batch 023 / 024 | loss: 0.89665\n",
      "batch 024 / 024 | loss: 0.90088\n",
      "training time: 1471.3198564052582 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.28072162039411763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 64 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 65 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 66 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.91117\n",
      "batch 002 / 024 | loss: 1.03462\n",
      "batch 003 / 024 | loss: 0.97923\n",
      "batch 004 / 024 | loss: 0.95864\n",
      "batch 005 / 024 | loss: 0.97559\n",
      "batch 006 / 024 | loss: 0.97723\n",
      "batch 007 / 024 | loss: 0.98528\n",
      "batch 008 / 024 | loss: 0.97427\n",
      "batch 009 / 024 | loss: 0.98397\n",
      "batch 010 / 024 | loss: 0.98191\n",
      "batch 011 / 024 | loss: 0.96570\n",
      "batch 012 / 024 | loss: 0.97238\n",
      "batch 013 / 024 | loss: 0.96866\n",
      "batch 014 / 024 | loss: 0.97995\n",
      "batch 015 / 024 | loss: 0.97975\n",
      "batch 016 / 024 | loss: 0.98074\n",
      "batch 017 / 024 | loss: 0.98465\n",
      "batch 018 / 024 | loss: 0.99352\n",
      "batch 019 / 024 | loss: 0.99450\n",
      "batch 020 / 024 | loss: 0.98886\n",
      "batch 021 / 024 | loss: 0.98185\n",
      "batch 022 / 024 | loss: 0.98022\n",
      "batch 023 / 024 | loss: 0.98076\n",
      "batch 024 / 024 | loss: 0.97151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved!\n",
      "----- epoch 001 / 010 | time: 162 sec | loss: 1.03689 | err: 0.47333\n",
      "batch 001 / 024 | loss: 0.84953\n",
      "batch 002 / 024 | loss: 0.89157\n",
      "batch 003 / 024 | loss: 0.91174\n",
      "batch 004 / 024 | loss: 0.89122\n",
      "batch 005 / 024 | loss: 0.90998\n",
      "batch 006 / 024 | loss: 0.92932\n",
      "batch 007 / 024 | loss: 0.92598\n",
      "batch 008 / 024 | loss: 0.91011\n",
      "batch 009 / 024 | loss: 0.91391\n",
      "batch 010 / 024 | loss: 0.91898\n",
      "batch 011 / 024 | loss: 0.92624\n",
      "batch 012 / 024 | loss: 0.92378\n",
      "batch 013 / 024 | loss: 0.91698\n",
      "batch 014 / 024 | loss: 0.91283\n",
      "batch 015 / 024 | loss: 0.92740\n",
      "batch 016 / 024 | loss: 0.92393\n",
      "batch 017 / 024 | loss: 0.92288\n",
      "batch 018 / 024 | loss: 0.91765\n",
      "batch 019 / 024 | loss: 0.91909\n",
      "batch 020 / 024 | loss: 0.91651\n",
      "batch 021 / 024 | loss: 0.91798\n",
      "batch 022 / 024 | loss: 0.91763\n",
      "batch 023 / 024 | loss: 0.91530\n",
      "batch 024 / 024 | loss: 0.91423\n",
      "----- epoch 002 / 010 | time: 144 sec | loss: 1.02176 | err: 0.47467\n",
      "batch 001 / 024 | loss: 0.81175\n",
      "batch 002 / 024 | loss: 0.78640\n",
      "batch 003 / 024 | loss: 0.80219\n",
      "batch 004 / 024 | loss: 0.82670\n",
      "batch 005 / 024 | loss: 0.87048\n",
      "batch 006 / 024 | loss: 0.87461\n",
      "batch 007 / 024 | loss: 0.89479\n",
      "batch 008 / 024 | loss: 0.88716\n",
      "batch 009 / 024 | loss: 0.88978\n",
      "batch 010 / 024 | loss: 0.89880\n",
      "batch 011 / 024 | loss: 0.89577\n",
      "batch 012 / 024 | loss: 0.89492\n",
      "batch 013 / 024 | loss: 0.90130\n",
      "batch 014 / 024 | loss: 0.89947\n",
      "batch 015 / 024 | loss: 0.89102\n",
      "batch 016 / 024 | loss: 0.89597\n",
      "batch 017 / 024 | loss: 0.89424\n",
      "batch 018 / 024 | loss: 0.89910\n",
      "batch 019 / 024 | loss: 0.88917\n",
      "batch 020 / 024 | loss: 0.88999\n",
      "batch 021 / 024 | loss: 0.89374\n",
      "batch 022 / 024 | loss: 0.90111\n",
      "batch 023 / 024 | loss: 0.90610\n",
      "batch 024 / 024 | loss: 0.90283\n",
      "----- epoch 003 / 010 | time: 142 sec | loss: 0.98399 | err: 0.47733\n",
      "batch 001 / 024 | loss: 0.66145\n",
      "batch 002 / 024 | loss: 0.82614\n",
      "batch 003 / 024 | loss: 0.83305\n",
      "batch 004 / 024 | loss: 0.88802\n",
      "batch 005 / 024 | loss: 0.87584\n",
      "batch 006 / 024 | loss: 0.85234\n",
      "batch 007 / 024 | loss: 0.85238\n",
      "batch 008 / 024 | loss: 0.85527\n",
      "batch 009 / 024 | loss: 0.83054\n",
      "batch 010 / 024 | loss: 0.84748\n",
      "batch 011 / 024 | loss: 0.85668\n",
      "batch 012 / 024 | loss: 0.86609\n",
      "batch 013 / 024 | loss: 0.85523\n",
      "batch 014 / 024 | loss: 0.86244\n",
      "batch 015 / 024 | loss: 0.86878\n",
      "batch 016 / 024 | loss: 0.87518\n",
      "batch 017 / 024 | loss: 0.88655\n",
      "batch 018 / 024 | loss: 0.89063\n",
      "batch 019 / 024 | loss: 0.89892\n",
      "batch 020 / 024 | loss: 0.90404\n",
      "batch 021 / 024 | loss: 0.91327\n",
      "batch 022 / 024 | loss: 0.90679\n",
      "batch 023 / 024 | loss: 0.90423\n",
      "batch 024 / 024 | loss: 0.90759\n",
      "----- epoch 004 / 010 | time: 142 sec | loss: 0.94221 | err: 0.47467\n",
      "batch 001 / 024 | loss: 1.20000\n",
      "batch 002 / 024 | loss: 1.02859\n",
      "batch 003 / 024 | loss: 1.03860\n",
      "batch 004 / 024 | loss: 0.94984\n",
      "batch 005 / 024 | loss: 0.95975\n",
      "batch 006 / 024 | loss: 0.93520\n",
      "batch 007 / 024 | loss: 0.93217\n",
      "batch 008 / 024 | loss: 0.91506\n",
      "batch 009 / 024 | loss: 0.92519\n",
      "batch 010 / 024 | loss: 0.91533\n",
      "batch 011 / 024 | loss: 0.91294\n",
      "batch 012 / 024 | loss: 0.92189\n",
      "batch 013 / 024 | loss: 0.91047\n",
      "batch 014 / 024 | loss: 0.90342\n",
      "batch 015 / 024 | loss: 0.91264\n",
      "batch 016 / 024 | loss: 0.91542\n",
      "batch 017 / 024 | loss: 0.90637\n",
      "batch 018 / 024 | loss: 0.90375\n",
      "batch 019 / 024 | loss: 0.90518\n",
      "batch 020 / 024 | loss: 0.90919\n",
      "batch 021 / 024 | loss: 0.91306\n",
      "batch 022 / 024 | loss: 0.91499\n",
      "batch 023 / 024 | loss: 0.91158\n",
      "batch 024 / 024 | loss: 0.90708\n",
      "----- epoch 005 / 010 | time: 143 sec | loss: 1.00028 | err: 0.47333\n",
      "batch 001 / 024 | loss: 0.90338\n",
      "batch 002 / 024 | loss: 0.84047\n",
      "batch 003 / 024 | loss: 0.87974\n",
      "batch 004 / 024 | loss: 0.88177\n",
      "batch 005 / 024 | loss: 0.88788\n",
      "batch 006 / 024 | loss: 0.91080\n",
      "batch 007 / 024 | loss: 0.91048\n",
      "batch 008 / 024 | loss: 0.92437\n",
      "batch 009 / 024 | loss: 0.91898\n",
      "batch 010 / 024 | loss: 0.90854\n",
      "batch 011 / 024 | loss: 0.92896\n",
      "batch 012 / 024 | loss: 0.93487\n",
      "batch 013 / 024 | loss: 0.93997\n",
      "batch 014 / 024 | loss: 0.93985\n",
      "batch 015 / 024 | loss: 0.93253\n",
      "batch 016 / 024 | loss: 0.93121\n",
      "batch 017 / 024 | loss: 0.92936\n",
      "batch 018 / 024 | loss: 0.93780\n",
      "batch 019 / 024 | loss: 0.93679\n",
      "batch 020 / 024 | loss: 0.91877\n",
      "batch 021 / 024 | loss: 0.91761\n",
      "batch 022 / 024 | loss: 0.91909\n",
      "batch 023 / 024 | loss: 0.91485\n",
      "batch 024 / 024 | loss: 0.91299\n",
      "training time: 888.7256379127502 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.33165737202888873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 67 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 68 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 69 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.93459\n",
      "batch 002 / 024 | loss: 1.06419\n",
      "batch 003 / 024 | loss: 1.00021\n",
      "batch 004 / 024 | loss: 0.97472\n",
      "batch 005 / 024 | loss: 0.98890\n",
      "batch 006 / 024 | loss: 0.98999\n",
      "batch 007 / 024 | loss: 0.99722\n",
      "batch 008 / 024 | loss: 0.98595\n",
      "batch 009 / 024 | loss: 0.99492\n",
      "batch 010 / 024 | loss: 0.99274\n",
      "batch 011 / 024 | loss: 0.97604\n",
      "batch 012 / 024 | loss: 0.98256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 013 / 024 | loss: 0.97837\n",
      "batch 014 / 024 | loss: 0.99140\n",
      "batch 015 / 024 | loss: 0.99140\n",
      "batch 016 / 024 | loss: 0.99278\n",
      "batch 017 / 024 | loss: 0.99734\n",
      "batch 018 / 024 | loss: 1.00631\n",
      "batch 019 / 024 | loss: 1.00735\n",
      "batch 020 / 024 | loss: 1.00099\n",
      "batch 021 / 024 | loss: 0.99379\n",
      "batch 022 / 024 | loss: 0.99206\n",
      "batch 023 / 024 | loss: 0.99244\n",
      "batch 024 / 024 | loss: 0.98296\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 149 sec | loss: 1.04799 | err: 0.47333\n",
      "batch 001 / 024 | loss: 0.84794\n",
      "batch 002 / 024 | loss: 0.89524\n",
      "batch 003 / 024 | loss: 0.91403\n",
      "batch 004 / 024 | loss: 0.89457\n",
      "batch 005 / 024 | loss: 0.91297\n",
      "batch 006 / 024 | loss: 0.93179\n",
      "batch 007 / 024 | loss: 0.92980\n",
      "batch 008 / 024 | loss: 0.91514\n",
      "batch 009 / 024 | loss: 0.92044\n",
      "batch 010 / 024 | loss: 0.92744\n",
      "batch 011 / 024 | loss: 0.93524\n",
      "batch 012 / 024 | loss: 0.93253\n",
      "batch 013 / 024 | loss: 0.92549\n",
      "batch 014 / 024 | loss: 0.92118\n",
      "batch 015 / 024 | loss: 0.93854\n",
      "batch 016 / 024 | loss: 0.93463\n",
      "batch 017 / 024 | loss: 0.93342\n",
      "batch 018 / 024 | loss: 0.92790\n",
      "batch 019 / 024 | loss: 0.92876\n",
      "batch 020 / 024 | loss: 0.92616\n",
      "batch 021 / 024 | loss: 0.92734\n",
      "batch 022 / 024 | loss: 0.92671\n",
      "batch 023 / 024 | loss: 0.92447\n",
      "batch 024 / 024 | loss: 0.92337\n",
      "----- epoch 002 / 010 | time: 136 sec | loss: 0.98273 | err: 0.47600\n",
      "batch 001 / 024 | loss: 0.81116\n",
      "batch 002 / 024 | loss: 0.78582\n",
      "batch 003 / 024 | loss: 0.80030\n",
      "batch 004 / 024 | loss: 0.82483\n",
      "batch 005 / 024 | loss: 0.86912\n",
      "batch 006 / 024 | loss: 0.87343\n",
      "batch 007 / 024 | loss: 0.89412\n",
      "batch 008 / 024 | loss: 0.88615\n",
      "batch 009 / 024 | loss: 0.88845\n",
      "batch 010 / 024 | loss: 0.89718\n",
      "batch 011 / 024 | loss: 0.89422\n",
      "batch 012 / 024 | loss: 0.89367\n",
      "batch 013 / 024 | loss: 0.89994\n",
      "batch 014 / 024 | loss: 0.89830\n",
      "batch 015 / 024 | loss: 0.88993\n",
      "batch 016 / 024 | loss: 0.89499\n",
      "batch 017 / 024 | loss: 0.89346\n",
      "batch 018 / 024 | loss: 0.89858\n",
      "batch 019 / 024 | loss: 0.88844\n",
      "batch 020 / 024 | loss: 0.88923\n",
      "batch 021 / 024 | loss: 0.89388\n",
      "batch 022 / 024 | loss: 0.90122\n",
      "batch 023 / 024 | loss: 0.90767\n",
      "batch 024 / 024 | loss: 0.90415\n",
      "----- epoch 003 / 010 | time: 157 sec | loss: 0.96398 | err: 0.47733\n",
      "batch 001 / 024 | loss: 0.65365\n",
      "batch 002 / 024 | loss: 0.82046\n",
      "batch 003 / 024 | loss: 0.82860\n",
      "batch 004 / 024 | loss: 0.88218\n",
      "batch 005 / 024 | loss: 0.87039\n",
      "batch 006 / 024 | loss: 0.84688\n",
      "batch 007 / 024 | loss: 0.84753\n",
      "batch 008 / 024 | loss: 0.85101\n",
      "batch 009 / 024 | loss: 0.82638\n",
      "batch 010 / 024 | loss: 0.84336\n",
      "batch 011 / 024 | loss: 0.85317\n",
      "batch 012 / 024 | loss: 0.86316\n",
      "batch 013 / 024 | loss: 0.85274\n",
      "batch 014 / 024 | loss: 0.86037\n",
      "batch 015 / 024 | loss: 0.86679\n",
      "batch 016 / 024 | loss: 0.87327\n",
      "batch 017 / 024 | loss: 0.88702\n",
      "batch 018 / 024 | loss: 0.89106\n",
      "batch 019 / 024 | loss: 0.90032\n",
      "batch 020 / 024 | loss: 0.90490\n",
      "batch 021 / 024 | loss: 0.91391\n",
      "batch 022 / 024 | loss: 0.90759\n",
      "batch 023 / 024 | loss: 0.90516\n",
      "batch 024 / 024 | loss: 0.90882\n",
      "----- epoch 004 / 010 | time: 150 sec | loss: 0.92627 | err: 0.47467\n",
      "batch 001 / 024 | loss: 1.23687\n",
      "batch 002 / 024 | loss: 1.04800\n",
      "batch 003 / 024 | loss: 1.05157\n",
      "batch 004 / 024 | loss: 0.96135\n",
      "batch 005 / 024 | loss: 0.97091\n",
      "batch 006 / 024 | loss: 0.94556\n",
      "batch 007 / 024 | loss: 0.94106\n",
      "batch 008 / 024 | loss: 0.92369\n",
      "batch 009 / 024 | loss: 0.93295\n",
      "batch 010 / 024 | loss: 0.92238\n",
      "batch 011 / 024 | loss: 0.91954\n",
      "batch 012 / 024 | loss: 0.92886\n",
      "batch 013 / 024 | loss: 0.91712\n",
      "batch 014 / 024 | loss: 0.90965\n",
      "batch 015 / 024 | loss: 0.92001\n",
      "batch 016 / 024 | loss: 0.92232\n",
      "batch 017 / 024 | loss: 0.91289\n",
      "batch 018 / 024 | loss: 0.91005\n",
      "batch 019 / 024 | loss: 0.91130\n",
      "batch 020 / 024 | loss: 0.91510\n",
      "batch 021 / 024 | loss: 0.91873\n",
      "batch 022 / 024 | loss: 0.92040\n",
      "batch 023 / 024 | loss: 0.91673\n",
      "batch 024 / 024 | loss: 0.91205\n",
      "----- epoch 005 / 010 | time: 150 sec | loss: 0.99025 | err: 0.47600\n",
      "batch 001 / 024 | loss: 0.90374\n",
      "batch 002 / 024 | loss: 0.83966\n",
      "batch 003 / 024 | loss: 0.87799\n",
      "batch 004 / 024 | loss: 0.87974\n",
      "batch 005 / 024 | loss: 0.88609\n",
      "batch 006 / 024 | loss: 0.90929\n",
      "batch 007 / 024 | loss: 0.90920\n",
      "batch 008 / 024 | loss: 0.92338\n",
      "batch 009 / 024 | loss: 0.91842\n",
      "batch 010 / 024 | loss: 0.90821\n",
      "batch 011 / 024 | loss: 0.93342\n",
      "batch 012 / 024 | loss: 0.93939\n",
      "batch 013 / 024 | loss: 0.94422\n",
      "batch 014 / 024 | loss: 0.94354\n",
      "batch 015 / 024 | loss: 0.93596\n",
      "batch 016 / 024 | loss: 0.93449\n",
      "batch 017 / 024 | loss: 0.93297\n",
      "batch 018 / 024 | loss: 0.94159\n",
      "batch 019 / 024 | loss: 0.94048\n",
      "batch 020 / 024 | loss: 0.92239\n",
      "batch 021 / 024 | loss: 0.92106\n",
      "batch 022 / 024 | loss: 0.92236\n",
      "batch 023 / 024 | loss: 0.91819\n",
      "batch 024 / 024 | loss: 0.91634\n",
      "training time: 898.9427497386932 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.39183520053311016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 70 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 71 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 72 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.96227\n",
      "batch 002 / 024 | loss: 1.09913\n",
      "batch 003 / 024 | loss: 1.02504\n",
      "batch 004 / 024 | loss: 0.99379\n",
      "batch 005 / 024 | loss: 1.00852\n",
      "batch 006 / 024 | loss: 1.00833\n",
      "batch 007 / 024 | loss: 1.01390\n",
      "batch 008 / 024 | loss: 1.00181\n",
      "batch 009 / 024 | loss: 1.00965\n",
      "batch 010 / 024 | loss: 1.00705\n",
      "batch 011 / 024 | loss: 0.98976\n",
      "batch 012 / 024 | loss: 0.99569\n",
      "batch 013 / 024 | loss: 0.99071\n",
      "batch 014 / 024 | loss: 1.00537\n",
      "batch 015 / 024 | loss: 1.00527\n",
      "batch 016 / 024 | loss: 1.00654\n",
      "batch 017 / 024 | loss: 1.01162\n",
      "batch 018 / 024 | loss: 1.02107\n",
      "batch 019 / 024 | loss: 1.02220\n",
      "batch 020 / 024 | loss: 1.01499\n",
      "batch 021 / 024 | loss: 1.00766\n",
      "batch 022 / 024 | loss: 1.00565\n",
      "batch 023 / 024 | loss: 1.00601\n",
      "batch 024 / 024 | loss: 0.99619\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 160 sec | loss: 1.06550 | err: 0.47200\n",
      "batch 001 / 024 | loss: 0.84795\n",
      "batch 002 / 024 | loss: 0.89950\n",
      "batch 003 / 024 | loss: 0.91664\n",
      "batch 004 / 024 | loss: 0.89839\n",
      "batch 005 / 024 | loss: 0.91640\n",
      "batch 006 / 024 | loss: 0.93471\n",
      "batch 007 / 024 | loss: 0.93355\n",
      "batch 008 / 024 | loss: 0.91989\n",
      "batch 009 / 024 | loss: 0.92684\n",
      "batch 010 / 024 | loss: 0.93665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 011 / 024 | loss: 0.94492\n",
      "batch 012 / 024 | loss: 0.94205\n",
      "batch 013 / 024 | loss: 0.93465\n",
      "batch 014 / 024 | loss: 0.93045\n",
      "batch 015 / 024 | loss: 0.95142\n",
      "batch 016 / 024 | loss: 0.94753\n",
      "batch 017 / 024 | loss: 0.94626\n",
      "batch 018 / 024 | loss: 0.94010\n",
      "batch 019 / 024 | loss: 0.94063\n",
      "batch 020 / 024 | loss: 0.93740\n",
      "batch 021 / 024 | loss: 0.93807\n",
      "batch 022 / 024 | loss: 0.93705\n",
      "batch 023 / 024 | loss: 0.93467\n",
      "batch 024 / 024 | loss: 0.93321\n",
      "----- epoch 002 / 010 | time: 115 sec | loss: 0.95384 | err: 0.47733\n",
      "batch 001 / 024 | loss: 0.81011\n",
      "batch 002 / 024 | loss: 0.78305\n",
      "batch 003 / 024 | loss: 0.79761\n",
      "batch 004 / 024 | loss: 0.82278\n",
      "batch 005 / 024 | loss: 0.86759\n",
      "batch 006 / 024 | loss: 0.87192\n",
      "batch 007 / 024 | loss: 0.89302\n",
      "batch 008 / 024 | loss: 0.88503\n",
      "batch 009 / 024 | loss: 0.88740\n",
      "batch 010 / 024 | loss: 0.89610\n",
      "batch 011 / 024 | loss: 0.89348\n",
      "batch 012 / 024 | loss: 0.89286\n",
      "batch 013 / 024 | loss: 0.89884\n",
      "batch 014 / 024 | loss: 0.89703\n",
      "batch 015 / 024 | loss: 0.88896\n",
      "batch 016 / 024 | loss: 0.89427\n",
      "batch 017 / 024 | loss: 0.89269\n",
      "batch 018 / 024 | loss: 0.89801\n",
      "batch 019 / 024 | loss: 0.88764\n",
      "batch 020 / 024 | loss: 0.88837\n",
      "batch 021 / 024 | loss: 0.89417\n",
      "batch 022 / 024 | loss: 0.90129\n",
      "batch 023 / 024 | loss: 0.90951\n",
      "batch 024 / 024 | loss: 0.90597\n",
      "----- epoch 003 / 010 | time: 113 sec | loss: 0.96255 | err: 0.47733\n",
      "batch 001 / 024 | loss: 0.65283\n",
      "batch 002 / 024 | loss: 0.81958\n",
      "batch 003 / 024 | loss: 0.82956\n",
      "batch 004 / 024 | loss: 0.88120\n",
      "batch 005 / 024 | loss: 0.86995\n",
      "batch 006 / 024 | loss: 0.84649\n",
      "batch 007 / 024 | loss: 0.84718\n",
      "batch 008 / 024 | loss: 0.85124\n",
      "batch 009 / 024 | loss: 0.82721\n",
      "batch 010 / 024 | loss: 0.84484\n",
      "batch 011 / 024 | loss: 0.85451\n",
      "batch 012 / 024 | loss: 0.86460\n",
      "batch 013 / 024 | loss: 0.85410\n",
      "batch 014 / 024 | loss: 0.86135\n",
      "batch 015 / 024 | loss: 0.86745\n",
      "batch 016 / 024 | loss: 0.87388\n",
      "batch 017 / 024 | loss: 0.88993\n",
      "batch 018 / 024 | loss: 0.89367\n",
      "batch 019 / 024 | loss: 0.90409\n",
      "batch 020 / 024 | loss: 0.90866\n",
      "batch 021 / 024 | loss: 0.91743\n",
      "batch 022 / 024 | loss: 0.91103\n",
      "batch 023 / 024 | loss: 0.90847\n",
      "batch 024 / 024 | loss: 0.91172\n",
      "----- epoch 004 / 010 | time: 109 sec | loss: 0.91784 | err: 0.47467\n",
      "batch 001 / 024 | loss: 1.28337\n",
      "batch 002 / 024 | loss: 1.07362\n",
      "batch 003 / 024 | loss: 1.07144\n",
      "batch 004 / 024 | loss: 0.97725\n",
      "batch 005 / 024 | loss: 0.98467\n",
      "batch 006 / 024 | loss: 0.95709\n",
      "batch 007 / 024 | loss: 0.95111\n",
      "batch 008 / 024 | loss: 0.93238\n",
      "batch 009 / 024 | loss: 0.94078\n",
      "batch 010 / 024 | loss: 0.92966\n",
      "batch 011 / 024 | loss: 0.92635\n",
      "batch 012 / 024 | loss: 0.93570\n",
      "batch 013 / 024 | loss: 0.92349\n",
      "batch 014 / 024 | loss: 0.91543\n",
      "batch 015 / 024 | loss: 0.92732\n",
      "batch 016 / 024 | loss: 0.92944\n",
      "batch 017 / 024 | loss: 0.91962\n",
      "batch 018 / 024 | loss: 0.91660\n",
      "batch 019 / 024 | loss: 0.91749\n",
      "batch 020 / 024 | loss: 0.92081\n",
      "batch 021 / 024 | loss: 0.92414\n",
      "batch 022 / 024 | loss: 0.92548\n",
      "batch 023 / 024 | loss: 0.92164\n",
      "batch 024 / 024 | loss: 0.91682\n",
      "----- epoch 005 / 010 | time: 114 sec | loss: 0.98876 | err: 0.47600\n",
      "batch 001 / 024 | loss: 0.90353\n",
      "batch 002 / 024 | loss: 0.83921\n",
      "batch 003 / 024 | loss: 0.87685\n",
      "batch 004 / 024 | loss: 0.87886\n",
      "batch 005 / 024 | loss: 0.88578\n",
      "batch 006 / 024 | loss: 0.90897\n",
      "batch 007 / 024 | loss: 0.90871\n",
      "batch 008 / 024 | loss: 0.92342\n",
      "batch 009 / 024 | loss: 0.91911\n",
      "batch 010 / 024 | loss: 0.90903\n",
      "batch 011 / 024 | loss: 0.93967\n",
      "batch 012 / 024 | loss: 0.94528\n",
      "batch 013 / 024 | loss: 0.94969\n",
      "batch 014 / 024 | loss: 0.94797\n",
      "batch 015 / 024 | loss: 0.94019\n",
      "batch 016 / 024 | loss: 0.93851\n",
      "batch 017 / 024 | loss: 0.93740\n",
      "batch 018 / 024 | loss: 0.94605\n",
      "batch 019 / 024 | loss: 0.94480\n",
      "batch 020 / 024 | loss: 0.92665\n",
      "batch 021 / 024 | loss: 0.92516\n",
      "batch 022 / 024 | loss: 0.92619\n",
      "batch 023 / 024 | loss: 0.92188\n",
      "batch 024 / 024 | loss: 0.91988\n",
      "training time: 722.3718430995941 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.46293204169587737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 73 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 74 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 75 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 0.99497\n",
      "batch 002 / 024 | loss: 1.14040\n",
      "batch 003 / 024 | loss: 1.05441\n",
      "batch 004 / 024 | loss: 1.01633\n",
      "batch 005 / 024 | loss: 1.02765\n",
      "batch 006 / 024 | loss: 1.02579\n",
      "batch 007 / 024 | loss: 1.02983\n",
      "batch 008 / 024 | loss: 1.01674\n",
      "batch 009 / 024 | loss: 1.02298\n",
      "batch 010 / 024 | loss: 1.01935\n",
      "batch 011 / 024 | loss: 1.00086\n",
      "batch 012 / 024 | loss: 1.00671\n",
      "batch 013 / 024 | loss: 1.00113\n",
      "batch 014 / 024 | loss: 1.01809\n",
      "batch 015 / 024 | loss: 1.01806\n",
      "batch 016 / 024 | loss: 1.01935\n",
      "batch 017 / 024 | loss: 1.02481\n",
      "batch 018 / 024 | loss: 1.03462\n",
      "batch 019 / 024 | loss: 1.03571\n",
      "batch 020 / 024 | loss: 1.02795\n",
      "batch 021 / 024 | loss: 1.02043\n",
      "batch 022 / 024 | loss: 1.01810\n",
      "batch 023 / 024 | loss: 1.01860\n",
      "batch 024 / 024 | loss: 1.00893\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 129 sec | loss: 1.09839 | err: 0.47067\n",
      "batch 001 / 024 | loss: 0.85291\n",
      "batch 002 / 024 | loss: 0.91153\n",
      "batch 003 / 024 | loss: 0.92680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 004 / 024 | loss: 0.90875\n",
      "batch 005 / 024 | loss: 0.92467\n",
      "batch 006 / 024 | loss: 0.94169\n",
      "batch 007 / 024 | loss: 0.94105\n",
      "batch 008 / 024 | loss: 0.92849\n",
      "batch 009 / 024 | loss: 0.93746\n",
      "batch 010 / 024 | loss: 0.95073\n",
      "batch 011 / 024 | loss: 0.95963\n",
      "batch 012 / 024 | loss: 0.95680\n",
      "batch 013 / 024 | loss: 0.95003\n",
      "batch 014 / 024 | loss: 0.94564\n",
      "batch 015 / 024 | loss: 0.97046\n",
      "batch 016 / 024 | loss: 0.96561\n",
      "batch 017 / 024 | loss: 0.96475\n",
      "batch 018 / 024 | loss: 0.95833\n",
      "batch 019 / 024 | loss: 0.95870\n",
      "batch 020 / 024 | loss: 0.95431\n",
      "batch 021 / 024 | loss: 0.95417\n",
      "batch 022 / 024 | loss: 0.95250\n",
      "batch 023 / 024 | loss: 0.94957\n",
      "batch 024 / 024 | loss: 0.94738\n",
      "----- epoch 002 / 010 | time: 117 sec | loss: 0.92868 | err: 0.47867\n",
      "batch 001 / 024 | loss: 0.81212\n",
      "batch 002 / 024 | loss: 0.78895\n",
      "batch 003 / 024 | loss: 0.80204\n",
      "batch 004 / 024 | loss: 0.82724\n",
      "batch 005 / 024 | loss: 0.87062\n",
      "batch 006 / 024 | loss: 0.87415\n",
      "batch 007 / 024 | loss: 0.89589\n",
      "batch 008 / 024 | loss: 0.88780\n",
      "batch 009 / 024 | loss: 0.89064\n",
      "batch 010 / 024 | loss: 0.89900\n",
      "batch 011 / 024 | loss: 0.89678\n",
      "batch 012 / 024 | loss: 0.89624\n",
      "batch 013 / 024 | loss: 0.90215\n",
      "batch 014 / 024 | loss: 0.90009\n",
      "batch 015 / 024 | loss: 0.89202\n",
      "batch 016 / 024 | loss: 0.89659\n",
      "batch 017 / 024 | loss: 0.89507\n",
      "batch 018 / 024 | loss: 0.90071\n",
      "batch 019 / 024 | loss: 0.89067\n",
      "batch 020 / 024 | loss: 0.89153\n",
      "batch 021 / 024 | loss: 0.89846\n",
      "batch 022 / 024 | loss: 0.90546\n",
      "batch 023 / 024 | loss: 0.91563\n",
      "batch 024 / 024 | loss: 0.91225\n",
      "----- epoch 003 / 010 | time: 111 sec | loss: 0.97085 | err: 0.47733\n",
      "batch 001 / 024 | loss: 0.65938\n",
      "batch 002 / 024 | loss: 0.82372\n",
      "batch 003 / 024 | loss: 0.83430\n",
      "batch 004 / 024 | loss: 0.88263\n",
      "batch 005 / 024 | loss: 0.87375\n",
      "batch 006 / 024 | loss: 0.85012\n",
      "batch 007 / 024 | loss: 0.85030\n",
      "batch 008 / 024 | loss: 0.85411\n",
      "batch 009 / 024 | loss: 0.83022\n",
      "batch 010 / 024 | loss: 0.84810\n",
      "batch 011 / 024 | loss: 0.85845\n",
      "batch 012 / 024 | loss: 0.86853\n",
      "batch 013 / 024 | loss: 0.85776\n",
      "batch 014 / 024 | loss: 0.86487\n",
      "batch 015 / 024 | loss: 0.87081\n",
      "batch 016 / 024 | loss: 0.87740\n",
      "batch 017 / 024 | loss: 0.89572\n",
      "batch 018 / 024 | loss: 0.89896\n",
      "batch 019 / 024 | loss: 0.91082\n",
      "batch 020 / 024 | loss: 0.91518\n",
      "batch 021 / 024 | loss: 0.92336\n",
      "batch 022 / 024 | loss: 0.91708\n",
      "batch 023 / 024 | loss: 0.91448\n",
      "batch 024 / 024 | loss: 0.91810\n",
      "----- epoch 004 / 010 | time: 122 sec | loss: 0.90113 | err: 0.47600\n",
      "batch 001 / 024 | loss: 1.33488\n",
      "batch 002 / 024 | loss: 1.10675\n",
      "batch 003 / 024 | loss: 1.09752\n",
      "batch 004 / 024 | loss: 1.00205\n",
      "batch 005 / 024 | loss: 1.00519\n",
      "batch 006 / 024 | loss: 0.97509\n",
      "batch 007 / 024 | loss: 0.96807\n",
      "batch 008 / 024 | loss: 0.94781\n",
      "batch 009 / 024 | loss: 0.95446\n",
      "batch 010 / 024 | loss: 0.94275\n",
      "batch 011 / 024 | loss: 0.93825\n",
      "batch 012 / 024 | loss: 0.94565\n",
      "batch 013 / 024 | loss: 0.93274\n",
      "batch 014 / 024 | loss: 0.92409\n",
      "batch 015 / 024 | loss: 0.93747\n",
      "batch 016 / 024 | loss: 0.93881\n",
      "batch 017 / 024 | loss: 0.92849\n",
      "batch 018 / 024 | loss: 0.92502\n",
      "batch 019 / 024 | loss: 0.92511\n",
      "batch 020 / 024 | loss: 0.92791\n",
      "batch 021 / 024 | loss: 0.93081\n",
      "batch 022 / 024 | loss: 0.93198\n",
      "batch 023 / 024 | loss: 0.92782\n",
      "batch 024 / 024 | loss: 0.92258\n",
      "----- epoch 005 / 010 | time: 122 sec | loss: 0.98994 | err: 0.47600\n",
      "batch 001 / 024 | loss: 0.90158\n",
      "batch 002 / 024 | loss: 0.83750\n",
      "batch 003 / 024 | loss: 0.87695\n",
      "batch 004 / 024 | loss: 0.88039\n",
      "batch 005 / 024 | loss: 0.88996\n",
      "batch 006 / 024 | loss: 0.91306\n",
      "batch 007 / 024 | loss: 0.91270\n",
      "batch 008 / 024 | loss: 0.92675\n",
      "batch 009 / 024 | loss: 0.92192\n",
      "batch 010 / 024 | loss: 0.91222\n",
      "batch 011 / 024 | loss: 0.94923\n",
      "batch 012 / 024 | loss: 0.95456\n",
      "batch 013 / 024 | loss: 0.95890\n",
      "batch 014 / 024 | loss: 0.95638\n",
      "batch 015 / 024 | loss: 0.94772\n",
      "batch 016 / 024 | loss: 0.94558\n",
      "batch 017 / 024 | loss: 0.94340\n",
      "batch 018 / 024 | loss: 0.95174\n",
      "batch 019 / 024 | loss: 0.95023\n",
      "batch 020 / 024 | loss: 0.93124\n",
      "batch 021 / 024 | loss: 0.93030\n",
      "batch 022 / 024 | loss: 0.93128\n",
      "batch 023 / 024 | loss: 0.92692\n",
      "batch 024 / 024 | loss: 0.92482\n",
      "training time: 733.0140540599823 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.546929104218151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 76 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 77 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 78 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 1.03360\n",
      "batch 002 / 024 | loss: 1.18916\n",
      "batch 003 / 024 | loss: 1.08913\n",
      "batch 004 / 024 | loss: 1.04294\n",
      "batch 005 / 024 | loss: 1.04946\n",
      "batch 006 / 024 | loss: 1.04575\n",
      "batch 007 / 024 | loss: 1.04799\n",
      "batch 008 / 024 | loss: 1.03365\n",
      "batch 009 / 024 | loss: 1.03819\n",
      "batch 010 / 024 | loss: 1.03335\n",
      "batch 011 / 024 | loss: 1.01355\n",
      "batch 012 / 024 | loss: 1.01908\n",
      "batch 013 / 024 | loss: 1.01283\n",
      "batch 014 / 024 | loss: 1.03218\n",
      "batch 015 / 024 | loss: 1.03194\n",
      "batch 016 / 024 | loss: 1.03311\n",
      "batch 017 / 024 | loss: 1.03875\n",
      "batch 018 / 024 | loss: 1.04890\n",
      "batch 019 / 024 | loss: 1.05001\n",
      "batch 020 / 024 | loss: 1.04162\n",
      "batch 021 / 024 | loss: 1.03397\n",
      "batch 022 / 024 | loss: 1.03149\n",
      "batch 023 / 024 | loss: 1.03192\n",
      "batch 024 / 024 | loss: 1.02236\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 141 sec | loss: 1.13240 | err: 0.46800\n",
      "batch 001 / 024 | loss: 0.86112\n",
      "batch 002 / 024 | loss: 0.92426\n",
      "batch 003 / 024 | loss: 0.93774\n",
      "batch 004 / 024 | loss: 0.91957\n",
      "batch 005 / 024 | loss: 0.93299\n",
      "batch 006 / 024 | loss: 0.94849\n",
      "batch 007 / 024 | loss: 0.94735\n",
      "batch 008 / 024 | loss: 0.93497\n",
      "batch 009 / 024 | loss: 0.94468\n",
      "batch 010 / 024 | loss: 0.96152\n",
      "batch 011 / 024 | loss: 0.97061\n",
      "batch 012 / 024 | loss: 0.96777\n",
      "batch 013 / 024 | loss: 0.96130\n",
      "batch 014 / 024 | loss: 0.95674\n",
      "batch 015 / 024 | loss: 0.98553\n",
      "batch 016 / 024 | loss: 0.97911\n",
      "batch 017 / 024 | loss: 0.97792\n",
      "batch 018 / 024 | loss: 0.97140\n",
      "batch 019 / 024 | loss: 0.97200\n",
      "batch 020 / 024 | loss: 0.96679\n",
      "batch 021 / 024 | loss: 0.96592\n",
      "batch 022 / 024 | loss: 0.96379\n",
      "batch 023 / 024 | loss: 0.96018\n",
      "batch 024 / 024 | loss: 0.95728\n",
      "----- epoch 002 / 010 | time: 134 sec | loss: 0.92939 | err: 0.47867\n",
      "batch 001 / 024 | loss: 0.80796\n",
      "batch 002 / 024 | loss: 0.78851\n",
      "batch 003 / 024 | loss: 0.80384\n",
      "batch 004 / 024 | loss: 0.82957\n",
      "batch 005 / 024 | loss: 0.87100\n",
      "batch 006 / 024 | loss: 0.87406\n",
      "batch 007 / 024 | loss: 0.89660\n",
      "batch 008 / 024 | loss: 0.88872\n",
      "batch 009 / 024 | loss: 0.89173\n",
      "batch 010 / 024 | loss: 0.89992\n",
      "batch 011 / 024 | loss: 0.89809\n",
      "batch 012 / 024 | loss: 0.89796\n",
      "batch 013 / 024 | loss: 0.90409\n",
      "batch 014 / 024 | loss: 0.90202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 015 / 024 | loss: 0.89372\n",
      "batch 016 / 024 | loss: 0.89838\n",
      "batch 017 / 024 | loss: 0.89655\n",
      "batch 018 / 024 | loss: 0.90205\n",
      "batch 019 / 024 | loss: 0.89215\n",
      "batch 020 / 024 | loss: 0.89324\n",
      "batch 021 / 024 | loss: 0.90155\n",
      "batch 022 / 024 | loss: 0.90887\n",
      "batch 023 / 024 | loss: 0.92119\n",
      "batch 024 / 024 | loss: 0.91753\n",
      "----- epoch 003 / 010 | time: 122 sec | loss: 0.96705 | err: 0.47867\n",
      "batch 001 / 024 | loss: 0.66036\n",
      "batch 002 / 024 | loss: 0.82384\n",
      "batch 003 / 024 | loss: 0.83632\n",
      "batch 004 / 024 | loss: 0.88396\n",
      "batch 005 / 024 | loss: 0.87612\n",
      "batch 006 / 024 | loss: 0.85240\n",
      "batch 007 / 024 | loss: 0.85258\n",
      "batch 008 / 024 | loss: 0.85644\n",
      "batch 009 / 024 | loss: 0.83198\n",
      "batch 010 / 024 | loss: 0.84953\n",
      "batch 011 / 024 | loss: 0.86008\n",
      "batch 012 / 024 | loss: 0.87001\n",
      "batch 013 / 024 | loss: 0.85916\n",
      "batch 014 / 024 | loss: 0.86635\n",
      "batch 015 / 024 | loss: 0.87222\n",
      "batch 016 / 024 | loss: 0.87882\n",
      "batch 017 / 024 | loss: 0.90052\n",
      "batch 018 / 024 | loss: 0.90375\n",
      "batch 019 / 024 | loss: 0.91760\n",
      "batch 020 / 024 | loss: 0.92172\n",
      "batch 021 / 024 | loss: 0.92961\n",
      "batch 022 / 024 | loss: 0.92306\n",
      "batch 023 / 024 | loss: 0.92002\n",
      "batch 024 / 024 | loss: 0.92372\n",
      "----- epoch 004 / 010 | time: 120 sec | loss: 0.88630 | err: 0.47600\n",
      "batch 001 / 024 | loss: 1.39456\n",
      "batch 002 / 024 | loss: 1.14236\n",
      "batch 003 / 024 | loss: 1.12320\n",
      "batch 004 / 024 | loss: 1.02610\n",
      "batch 005 / 024 | loss: 1.02585\n",
      "batch 006 / 024 | loss: 0.99390\n",
      "batch 007 / 024 | loss: 0.98508\n",
      "batch 008 / 024 | loss: 0.96371\n",
      "batch 009 / 024 | loss: 0.96859\n",
      "batch 010 / 024 | loss: 0.95629\n",
      "batch 011 / 024 | loss: 0.95043\n",
      "batch 012 / 024 | loss: 0.95546\n",
      "batch 013 / 024 | loss: 0.94172\n",
      "batch 014 / 024 | loss: 0.93237\n",
      "batch 015 / 024 | loss: 0.94777\n",
      "batch 016 / 024 | loss: 0.94865\n",
      "batch 017 / 024 | loss: 0.93796\n",
      "batch 018 / 024 | loss: 0.93404\n",
      "batch 019 / 024 | loss: 0.93307\n",
      "batch 020 / 024 | loss: 0.93544\n",
      "batch 021 / 024 | loss: 0.93782\n",
      "batch 022 / 024 | loss: 0.93871\n",
      "batch 023 / 024 | loss: 0.93426\n",
      "batch 024 / 024 | loss: 0.92861\n",
      "----- epoch 005 / 010 | time: 111 sec | loss: 0.99067 | err: 0.47600\n",
      "batch 001 / 024 | loss: 0.89685\n",
      "batch 002 / 024 | loss: 0.83186\n",
      "batch 003 / 024 | loss: 0.87308\n",
      "batch 004 / 024 | loss: 0.87643\n",
      "batch 005 / 024 | loss: 0.88786\n",
      "batch 006 / 024 | loss: 0.91187\n",
      "batch 007 / 024 | loss: 0.91177\n",
      "batch 008 / 024 | loss: 0.92588\n",
      "batch 009 / 024 | loss: 0.92082\n",
      "batch 010 / 024 | loss: 0.91155\n",
      "batch 011 / 024 | loss: 0.95550\n",
      "batch 012 / 024 | loss: 0.95956\n",
      "batch 013 / 024 | loss: 0.96350\n",
      "batch 014 / 024 | loss: 0.96011\n",
      "batch 015 / 024 | loss: 0.95111\n",
      "batch 016 / 024 | loss: 0.94869\n",
      "batch 017 / 024 | loss: 0.94603\n",
      "batch 018 / 024 | loss: 0.95423\n",
      "batch 019 / 024 | loss: 0.95254\n",
      "batch 020 / 024 | loss: 0.93311\n",
      "batch 021 / 024 | loss: 0.93256\n",
      "batch 022 / 024 | loss: 0.93329\n",
      "batch 023 / 024 | loss: 0.92856\n",
      "batch 024 / 024 | loss: 0.92629\n",
      "training time: 734.1001718044281 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.646167078746697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 79 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 80 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 81 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 1.07924\n",
      "batch 002 / 024 | loss: 1.23834\n",
      "batch 003 / 024 | loss: 1.12517\n",
      "batch 004 / 024 | loss: 1.07222\n",
      "batch 005 / 024 | loss: 1.07334\n",
      "batch 006 / 024 | loss: 1.06780\n",
      "batch 007 / 024 | loss: 1.06717\n",
      "batch 008 / 024 | loss: 1.05051\n",
      "batch 009 / 024 | loss: 1.05282\n",
      "batch 010 / 024 | loss: 1.04571\n",
      "batch 011 / 024 | loss: 1.02329\n",
      "batch 012 / 024 | loss: 1.02896\n",
      "batch 013 / 024 | loss: 1.02272\n",
      "batch 014 / 024 | loss: 1.04503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 015 / 024 | loss: 1.04456\n",
      "batch 016 / 024 | loss: 1.04567\n",
      "batch 017 / 024 | loss: 1.05135\n",
      "batch 018 / 024 | loss: 1.06157\n",
      "batch 019 / 024 | loss: 1.06244\n",
      "batch 020 / 024 | loss: 1.05360\n",
      "batch 021 / 024 | loss: 1.04556\n",
      "batch 022 / 024 | loss: 1.04270\n",
      "batch 023 / 024 | loss: 1.04340\n",
      "batch 024 / 024 | loss: 1.03358\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 124 sec | loss: 1.16031 | err: 0.46933\n",
      "batch 001 / 024 | loss: 0.86795\n",
      "batch 002 / 024 | loss: 0.93896\n",
      "batch 003 / 024 | loss: 0.95032\n",
      "batch 004 / 024 | loss: 0.92902\n",
      "batch 005 / 024 | loss: 0.94050\n",
      "batch 006 / 024 | loss: 0.95520\n",
      "batch 007 / 024 | loss: 0.95350\n",
      "batch 008 / 024 | loss: 0.94059\n",
      "batch 009 / 024 | loss: 0.95079\n",
      "batch 010 / 024 | loss: 0.97157\n",
      "batch 011 / 024 | loss: 0.98110\n",
      "batch 012 / 024 | loss: 0.97819\n",
      "batch 013 / 024 | loss: 0.97194\n",
      "batch 014 / 024 | loss: 0.96722\n",
      "batch 015 / 024 | loss: 1.00132\n",
      "batch 016 / 024 | loss: 0.99358\n",
      "batch 017 / 024 | loss: 0.99157\n",
      "batch 018 / 024 | loss: 0.98488\n",
      "batch 019 / 024 | loss: 0.98500\n",
      "batch 020 / 024 | loss: 0.97922\n",
      "batch 021 / 024 | loss: 0.97723\n",
      "batch 022 / 024 | loss: 0.97436\n",
      "batch 023 / 024 | loss: 0.97026\n",
      "batch 024 / 024 | loss: 0.96651\n",
      "----- epoch 002 / 010 | time: 161 sec | loss: 0.92480 | err: 0.47867\n",
      "batch 001 / 024 | loss: 0.80325\n",
      "batch 002 / 024 | loss: 0.78622\n",
      "batch 003 / 024 | loss: 0.80260\n",
      "batch 004 / 024 | loss: 0.82925\n",
      "batch 005 / 024 | loss: 0.87052\n",
      "batch 006 / 024 | loss: 0.87483\n",
      "batch 007 / 024 | loss: 0.89690\n",
      "batch 008 / 024 | loss: 0.88943\n",
      "batch 009 / 024 | loss: 0.89265\n",
      "batch 010 / 024 | loss: 0.90080\n",
      "batch 011 / 024 | loss: 0.89952\n",
      "batch 012 / 024 | loss: 0.89931\n",
      "batch 013 / 024 | loss: 0.90504\n",
      "batch 014 / 024 | loss: 0.90286\n",
      "batch 015 / 024 | loss: 0.89453\n",
      "batch 016 / 024 | loss: 0.89925\n",
      "batch 017 / 024 | loss: 0.89727\n",
      "batch 018 / 024 | loss: 0.90262\n",
      "batch 019 / 024 | loss: 0.89259\n",
      "batch 020 / 024 | loss: 0.89367\n",
      "batch 021 / 024 | loss: 0.90367\n",
      "batch 022 / 024 | loss: 0.91075\n",
      "batch 023 / 024 | loss: 0.92574\n",
      "batch 024 / 024 | loss: 0.92182\n",
      "----- epoch 003 / 010 | time: 135 sec | loss: 0.94909 | err: 0.47867\n",
      "batch 001 / 024 | loss: 0.66107\n",
      "batch 002 / 024 | loss: 0.82242\n",
      "batch 003 / 024 | loss: 0.83489\n",
      "batch 004 / 024 | loss: 0.88200\n",
      "batch 005 / 024 | loss: 0.87316\n",
      "batch 006 / 024 | loss: 0.84973\n",
      "batch 007 / 024 | loss: 0.85037\n",
      "batch 008 / 024 | loss: 0.85494\n",
      "batch 009 / 024 | loss: 0.83071\n",
      "batch 010 / 024 | loss: 0.84866\n",
      "batch 011 / 024 | loss: 0.85951\n",
      "batch 012 / 024 | loss: 0.86982\n",
      "batch 013 / 024 | loss: 0.85910\n",
      "batch 014 / 024 | loss: 0.86655\n",
      "batch 015 / 024 | loss: 0.87246\n",
      "batch 016 / 024 | loss: 0.87908\n",
      "batch 017 / 024 | loss: 0.90484\n",
      "batch 018 / 024 | loss: 0.90799\n",
      "batch 019 / 024 | loss: 0.92363\n",
      "batch 020 / 024 | loss: 0.92737\n",
      "batch 021 / 024 | loss: 0.93512\n",
      "batch 022 / 024 | loss: 0.92812\n",
      "batch 023 / 024 | loss: 0.92485\n",
      "batch 024 / 024 | loss: 0.92890\n",
      "----- epoch 004 / 010 | time: 133 sec | loss: 0.89073 | err: 0.47600\n",
      "batch 001 / 024 | loss: 1.47136\n",
      "batch 002 / 024 | loss: 1.17527\n",
      "batch 003 / 024 | loss: 1.14286\n",
      "batch 004 / 024 | loss: 1.03810\n",
      "batch 005 / 024 | loss: 1.03722\n",
      "batch 006 / 024 | loss: 1.00451\n",
      "batch 007 / 024 | loss: 0.99418\n",
      "batch 008 / 024 | loss: 0.97266\n",
      "batch 009 / 024 | loss: 0.97643\n",
      "batch 010 / 024 | loss: 0.96329\n",
      "batch 011 / 024 | loss: 0.95580\n",
      "batch 012 / 024 | loss: 0.96098\n",
      "batch 013 / 024 | loss: 0.94690\n",
      "batch 014 / 024 | loss: 0.93780\n",
      "batch 015 / 024 | loss: 0.95560\n",
      "batch 016 / 024 | loss: 0.95608\n",
      "batch 017 / 024 | loss: 0.94484\n",
      "batch 018 / 024 | loss: 0.94058\n",
      "batch 019 / 024 | loss: 0.93983\n",
      "batch 020 / 024 | loss: 0.94207\n",
      "batch 021 / 024 | loss: 0.94440\n",
      "batch 022 / 024 | loss: 0.94505\n",
      "batch 023 / 024 | loss: 0.94046\n",
      "batch 024 / 024 | loss: 0.93460\n",
      "----- epoch 005 / 010 | time: 133 sec | loss: 0.98054 | err: 0.47600\n",
      "batch 001 / 024 | loss: 0.90190\n",
      "batch 002 / 024 | loss: 0.83436\n",
      "batch 003 / 024 | loss: 0.87464\n",
      "batch 004 / 024 | loss: 0.87847\n",
      "batch 005 / 024 | loss: 0.89438\n",
      "batch 006 / 024 | loss: 0.91743\n",
      "batch 007 / 024 | loss: 0.91795\n",
      "batch 008 / 024 | loss: 0.93132\n",
      "batch 009 / 024 | loss: 0.92556\n",
      "batch 010 / 024 | loss: 0.91594\n",
      "batch 011 / 024 | loss: 0.96864\n",
      "batch 012 / 024 | loss: 0.97353\n",
      "batch 013 / 024 | loss: 0.97734\n",
      "batch 014 / 024 | loss: 0.97377\n",
      "batch 015 / 024 | loss: 0.96409\n",
      "batch 016 / 024 | loss: 0.96117\n",
      "batch 017 / 024 | loss: 0.95795\n",
      "batch 018 / 024 | loss: 0.96603\n",
      "batch 019 / 024 | loss: 0.96385\n",
      "batch 020 / 024 | loss: 0.94354\n",
      "batch 021 / 024 | loss: 0.94410\n",
      "batch 022 / 024 | loss: 0.94505\n",
      "batch 023 / 024 | loss: 0.94045\n",
      "batch 024 / 024 | loss: 0.93778\n",
      "training time: 818.4907894134521 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.7634113643539091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 82 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 83 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 84 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 1.13316\n",
      "batch 002 / 024 | loss: 1.30581\n",
      "batch 003 / 024 | loss: 1.17341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 004 / 024 | loss: 1.10924\n",
      "batch 005 / 024 | loss: 1.10373\n",
      "batch 006 / 024 | loss: 1.09500\n",
      "batch 007 / 024 | loss: 1.09166\n",
      "batch 008 / 024 | loss: 1.07301\n",
      "batch 009 / 024 | loss: 1.07300\n",
      "batch 010 / 024 | loss: 1.06415\n",
      "batch 011 / 024 | loss: 1.04016\n",
      "batch 012 / 024 | loss: 1.04509\n",
      "batch 013 / 024 | loss: 1.03787\n",
      "batch 014 / 024 | loss: 1.06306\n",
      "batch 015 / 024 | loss: 1.06193\n",
      "batch 016 / 024 | loss: 1.06253\n",
      "batch 017 / 024 | loss: 1.06799\n",
      "batch 018 / 024 | loss: 1.07825\n",
      "batch 019 / 024 | loss: 1.07894\n",
      "batch 020 / 024 | loss: 1.06959\n",
      "batch 021 / 024 | loss: 1.06136\n",
      "batch 022 / 024 | loss: 1.05827\n",
      "batch 023 / 024 | loss: 1.05848\n",
      "batch 024 / 024 | loss: 1.04816\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 158 sec | loss: 1.17648 | err: 0.47333\n",
      "batch 001 / 024 | loss: 0.87470\n",
      "batch 002 / 024 | loss: 0.94545\n",
      "batch 003 / 024 | loss: 0.95607\n",
      "batch 004 / 024 | loss: 0.93192\n",
      "batch 005 / 024 | loss: 0.94262\n",
      "batch 006 / 024 | loss: 0.95717\n",
      "batch 007 / 024 | loss: 0.95570\n",
      "batch 008 / 024 | loss: 0.94333\n",
      "batch 009 / 024 | loss: 0.95421\n",
      "batch 010 / 024 | loss: 0.97943\n",
      "batch 011 / 024 | loss: 0.98894\n",
      "batch 012 / 024 | loss: 0.98554\n",
      "batch 013 / 024 | loss: 0.97882\n",
      "batch 014 / 024 | loss: 0.97355\n",
      "batch 015 / 024 | loss: 1.01350\n",
      "batch 016 / 024 | loss: 1.00494\n",
      "batch 017 / 024 | loss: 1.00300\n",
      "batch 018 / 024 | loss: 0.99587\n",
      "batch 019 / 024 | loss: 0.99552\n",
      "batch 020 / 024 | loss: 0.98944\n",
      "batch 021 / 024 | loss: 0.98690\n",
      "batch 022 / 024 | loss: 0.98383\n",
      "batch 023 / 024 | loss: 0.97976\n",
      "batch 024 / 024 | loss: 0.97545\n",
      "----- epoch 002 / 010 | time: 133 sec | loss: 0.92366 | err: 0.48267\n",
      "batch 001 / 024 | loss: 0.80620\n",
      "batch 002 / 024 | loss: 0.78896\n",
      "batch 003 / 024 | loss: 0.80789\n",
      "batch 004 / 024 | loss: 0.83570\n",
      "batch 005 / 024 | loss: 0.87549\n",
      "batch 006 / 024 | loss: 0.88003\n",
      "batch 007 / 024 | loss: 0.90123\n",
      "batch 008 / 024 | loss: 0.89385\n",
      "batch 009 / 024 | loss: 0.89741\n",
      "batch 010 / 024 | loss: 0.90552\n",
      "batch 011 / 024 | loss: 0.90490\n",
      "batch 012 / 024 | loss: 0.90411\n",
      "batch 013 / 024 | loss: 0.91008\n",
      "batch 014 / 024 | loss: 0.90773\n",
      "batch 015 / 024 | loss: 0.89897\n",
      "batch 016 / 024 | loss: 0.90265\n",
      "batch 017 / 024 | loss: 0.90059\n",
      "batch 018 / 024 | loss: 0.90577\n",
      "batch 019 / 024 | loss: 0.89576\n",
      "batch 020 / 024 | loss: 0.89702\n",
      "batch 021 / 024 | loss: 0.90906\n",
      "batch 022 / 024 | loss: 0.91631\n",
      "batch 023 / 024 | loss: 0.93438\n",
      "batch 024 / 024 | loss: 0.93023\n",
      "----- epoch 003 / 010 | time: 132 sec | loss: 0.94555 | err: 0.47867\n",
      "batch 001 / 024 | loss: 0.66738\n",
      "batch 002 / 024 | loss: 0.82294\n",
      "batch 003 / 024 | loss: 0.83659\n",
      "batch 004 / 024 | loss: 0.88388\n",
      "batch 005 / 024 | loss: 0.87717\n",
      "batch 006 / 024 | loss: 0.85582\n",
      "batch 007 / 024 | loss: 0.85596\n",
      "batch 008 / 024 | loss: 0.85947\n",
      "batch 009 / 024 | loss: 0.83499\n",
      "batch 010 / 024 | loss: 0.85230\n",
      "batch 011 / 024 | loss: 0.86305\n",
      "batch 012 / 024 | loss: 0.87345\n",
      "batch 013 / 024 | loss: 0.86265\n",
      "batch 014 / 024 | loss: 0.87130\n",
      "batch 015 / 024 | loss: 0.87689\n",
      "batch 016 / 024 | loss: 0.88316\n",
      "batch 017 / 024 | loss: 0.91334\n",
      "batch 018 / 024 | loss: 0.91618\n",
      "batch 019 / 024 | loss: 0.93450\n",
      "batch 020 / 024 | loss: 0.93833\n",
      "batch 021 / 024 | loss: 0.94571\n",
      "batch 022 / 024 | loss: 0.93775\n",
      "batch 023 / 024 | loss: 0.93361\n",
      "batch 024 / 024 | loss: 0.93559\n",
      "----- epoch 004 / 010 | time: 130 sec | loss: 0.90051 | err: 0.47600\n",
      "batch 001 / 024 | loss: 1.56660\n",
      "batch 002 / 024 | loss: 1.22678\n",
      "batch 003 / 024 | loss: 1.18142\n",
      "batch 004 / 024 | loss: 1.06583\n",
      "batch 005 / 024 | loss: 1.05343\n",
      "batch 006 / 024 | loss: 1.01423\n",
      "batch 007 / 024 | loss: 1.00125\n",
      "batch 008 / 024 | loss: 0.97725\n",
      "batch 009 / 024 | loss: 0.98056\n",
      "batch 010 / 024 | loss: 0.96587\n",
      "batch 011 / 024 | loss: 0.95805\n",
      "batch 012 / 024 | loss: 0.96333\n",
      "batch 013 / 024 | loss: 0.94840\n",
      "batch 014 / 024 | loss: 0.93790\n",
      "batch 015 / 024 | loss: 0.95918\n",
      "batch 016 / 024 | loss: 0.95994\n",
      "batch 017 / 024 | loss: 0.94856\n",
      "batch 018 / 024 | loss: 0.94421\n",
      "batch 019 / 024 | loss: 0.94275\n",
      "batch 020 / 024 | loss: 0.94476\n",
      "batch 021 / 024 | loss: 0.94648\n",
      "batch 022 / 024 | loss: 0.94666\n",
      "batch 023 / 024 | loss: 0.94220\n",
      "batch 024 / 024 | loss: 0.93644\n",
      "----- epoch 005 / 010 | time: 129 sec | loss: 0.94867 | err: 0.47600\n",
      "batch 001 / 024 | loss: 0.90257\n",
      "batch 002 / 024 | loss: 0.83572\n",
      "batch 003 / 024 | loss: 0.87105\n",
      "batch 004 / 024 | loss: 0.87233\n",
      "batch 005 / 024 | loss: 0.87852\n",
      "batch 006 / 024 | loss: 0.90446\n",
      "batch 007 / 024 | loss: 0.90481\n",
      "batch 008 / 024 | loss: 0.91895\n",
      "batch 009 / 024 | loss: 0.91375\n",
      "batch 010 / 024 | loss: 0.90443\n",
      "batch 011 / 024 | loss: 0.96556\n",
      "batch 012 / 024 | loss: 0.96945\n",
      "batch 013 / 024 | loss: 0.97237\n",
      "batch 014 / 024 | loss: 0.96790\n",
      "batch 015 / 024 | loss: 0.95822\n",
      "batch 016 / 024 | loss: 0.95554\n",
      "batch 017 / 024 | loss: 0.95271\n",
      "batch 018 / 024 | loss: 0.96104\n",
      "batch 019 / 024 | loss: 0.95902\n",
      "batch 020 / 024 | loss: 0.93910\n",
      "batch 021 / 024 | loss: 0.93773\n",
      "batch 022 / 024 | loss: 0.93785\n",
      "batch 023 / 024 | loss: 0.93317\n",
      "batch 024 / 024 | loss: 0.93037\n",
      "training time: 829.5216495990753 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  0.9019291300867378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 85 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 86 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 87 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 1.19686\n",
      "batch 002 / 024 | loss: 1.38553\n",
      "batch 003 / 024 | loss: 1.23038\n",
      "batch 004 / 024 | loss: 1.15286\n",
      "batch 005 / 024 | loss: 1.13953\n",
      "batch 006 / 024 | loss: 1.12679\n",
      "batch 007 / 024 | loss: 1.12016\n",
      "batch 008 / 024 | loss: 1.09909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 009 / 024 | loss: 1.09646\n",
      "batch 010 / 024 | loss: 1.08563\n",
      "batch 011 / 024 | loss: 1.05993\n",
      "batch 012 / 024 | loss: 1.06383\n",
      "batch 013 / 024 | loss: 1.05538\n",
      "batch 014 / 024 | loss: 1.08392\n",
      "batch 015 / 024 | loss: 1.08198\n",
      "batch 016 / 024 | loss: 1.08188\n",
      "batch 017 / 024 | loss: 1.08696\n",
      "batch 018 / 024 | loss: 1.09726\n",
      "batch 019 / 024 | loss: 1.09768\n",
      "batch 020 / 024 | loss: 1.08772\n",
      "batch 021 / 024 | loss: 1.07921\n",
      "batch 022 / 024 | loss: 1.07592\n",
      "batch 023 / 024 | loss: 1.07549\n",
      "batch 024 / 024 | loss: 1.06426\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 199 sec | loss: 1.19850 | err: 0.47467\n",
      "batch 001 / 024 | loss: 0.88276\n",
      "batch 002 / 024 | loss: 0.94767\n",
      "batch 003 / 024 | loss: 0.95963\n",
      "batch 004 / 024 | loss: 0.93301\n",
      "batch 005 / 024 | loss: 0.94363\n",
      "batch 006 / 024 | loss: 0.95835\n",
      "batch 007 / 024 | loss: 0.95647\n",
      "batch 008 / 024 | loss: 0.94412\n",
      "batch 009 / 024 | loss: 0.95429\n",
      "batch 010 / 024 | loss: 0.98386\n",
      "batch 011 / 024 | loss: 0.99229\n",
      "batch 012 / 024 | loss: 0.98800\n",
      "batch 013 / 024 | loss: 0.98069\n",
      "batch 014 / 024 | loss: 0.97484\n",
      "batch 015 / 024 | loss: 1.02187\n",
      "batch 016 / 024 | loss: 1.01301\n",
      "batch 017 / 024 | loss: 1.01189\n",
      "batch 018 / 024 | loss: 1.00434\n",
      "batch 019 / 024 | loss: 1.00368\n",
      "batch 020 / 024 | loss: 0.99748\n",
      "batch 021 / 024 | loss: 0.99449\n",
      "batch 022 / 024 | loss: 0.99126\n",
      "batch 023 / 024 | loss: 0.98729\n",
      "batch 024 / 024 | loss: 0.98278\n",
      "----- epoch 002 / 010 | time: 140 sec | loss: 0.93180 | err: 0.48267\n",
      "batch 001 / 024 | loss: 0.81071\n",
      "batch 002 / 024 | loss: 0.78887\n",
      "batch 003 / 024 | loss: 0.80730\n",
      "batch 004 / 024 | loss: 0.83584\n",
      "batch 005 / 024 | loss: 0.87675\n",
      "batch 006 / 024 | loss: 0.88238\n",
      "batch 007 / 024 | loss: 0.90351\n",
      "batch 008 / 024 | loss: 0.89655\n",
      "batch 009 / 024 | loss: 0.90000\n",
      "batch 010 / 024 | loss: 0.90876\n",
      "batch 011 / 024 | loss: 0.90823\n",
      "batch 012 / 024 | loss: 0.90741\n",
      "batch 013 / 024 | loss: 0.91369\n",
      "batch 014 / 024 | loss: 0.91117\n",
      "batch 015 / 024 | loss: 0.90210\n",
      "batch 016 / 024 | loss: 0.90553\n",
      "batch 017 / 024 | loss: 0.90328\n",
      "batch 018 / 024 | loss: 0.90812\n",
      "batch 019 / 024 | loss: 0.89797\n",
      "batch 020 / 024 | loss: 0.89909\n",
      "batch 021 / 024 | loss: 0.91335\n",
      "batch 022 / 024 | loss: 0.92015\n",
      "batch 023 / 024 | loss: 0.94198\n",
      "batch 024 / 024 | loss: 0.93708\n",
      "----- epoch 003 / 010 | time: 139 sec | loss: 0.93862 | err: 0.47867\n",
      "batch 001 / 024 | loss: 0.66979\n",
      "batch 002 / 024 | loss: 0.82107\n",
      "batch 003 / 024 | loss: 0.83450\n",
      "batch 004 / 024 | loss: 0.88321\n",
      "batch 005 / 024 | loss: 0.87578\n",
      "batch 006 / 024 | loss: 0.85412\n",
      "batch 007 / 024 | loss: 0.85454\n",
      "batch 008 / 024 | loss: 0.85848\n",
      "batch 009 / 024 | loss: 0.83397\n",
      "batch 010 / 024 | loss: 0.85119\n",
      "batch 011 / 024 | loss: 0.86164\n",
      "batch 012 / 024 | loss: 0.87214\n",
      "batch 013 / 024 | loss: 0.86129\n",
      "batch 014 / 024 | loss: 0.87026\n",
      "batch 015 / 024 | loss: 0.87598\n",
      "batch 016 / 024 | loss: 0.88229\n",
      "batch 017 / 024 | loss: 0.91804\n",
      "batch 018 / 024 | loss: 0.92064\n",
      "batch 019 / 024 | loss: 0.94159\n",
      "batch 020 / 024 | loss: 0.94505\n",
      "batch 021 / 024 | loss: 0.95217\n",
      "batch 022 / 024 | loss: 0.94385\n",
      "batch 023 / 024 | loss: 0.93933\n",
      "batch 024 / 024 | loss: 0.94089\n",
      "----- epoch 004 / 010 | time: 148 sec | loss: 0.89863 | err: 0.47600\n",
      "batch 001 / 024 | loss: 1.67210\n",
      "batch 002 / 024 | loss: 1.28068\n",
      "batch 003 / 024 | loss: 1.21817\n",
      "batch 004 / 024 | loss: 1.09407\n",
      "batch 005 / 024 | loss: 1.07638\n",
      "batch 006 / 024 | loss: 1.03388\n",
      "batch 007 / 024 | loss: 1.01848\n",
      "batch 008 / 024 | loss: 0.99167\n",
      "batch 009 / 024 | loss: 0.99322\n",
      "batch 010 / 024 | loss: 0.97732\n",
      "batch 011 / 024 | loss: 0.96825\n",
      "batch 012 / 024 | loss: 0.97266\n",
      "batch 013 / 024 | loss: 0.95701\n",
      "batch 014 / 024 | loss: 0.94603\n",
      "batch 015 / 024 | loss: 0.97073\n",
      "batch 016 / 024 | loss: 0.97098\n",
      "batch 017 / 024 | loss: 0.95902\n",
      "batch 018 / 024 | loss: 0.95425\n",
      "batch 019 / 024 | loss: 0.95247\n",
      "batch 020 / 024 | loss: 0.95412\n",
      "batch 021 / 024 | loss: 0.95554\n",
      "batch 022 / 024 | loss: 0.95537\n",
      "batch 023 / 024 | loss: 0.95060\n",
      "batch 024 / 024 | loss: 0.94453\n",
      "----- epoch 005 / 010 | time: 119 sec | loss: 0.94952 | err: 0.47733\n",
      "batch 001 / 024 | loss: 0.90500\n",
      "batch 002 / 024 | loss: 0.83800\n",
      "batch 003 / 024 | loss: 0.87312\n",
      "batch 004 / 024 | loss: 0.87409\n",
      "batch 005 / 024 | loss: 0.88060\n",
      "batch 006 / 024 | loss: 0.90638\n",
      "batch 007 / 024 | loss: 0.90635\n",
      "batch 008 / 024 | loss: 0.91989\n",
      "batch 009 / 024 | loss: 0.91481\n",
      "batch 010 / 024 | loss: 0.90545\n",
      "batch 011 / 024 | loss: 0.97837\n",
      "batch 012 / 024 | loss: 0.98222\n",
      "batch 013 / 024 | loss: 0.98417\n",
      "batch 014 / 024 | loss: 0.97885\n",
      "batch 015 / 024 | loss: 0.96845\n",
      "batch 016 / 024 | loss: 0.96510\n",
      "batch 017 / 024 | loss: 0.96151\n",
      "batch 018 / 024 | loss: 0.96927\n",
      "batch 019 / 024 | loss: 0.96681\n",
      "batch 020 / 024 | loss: 0.94651\n",
      "batch 021 / 024 | loss: 0.94521\n",
      "batch 022 / 024 | loss: 0.94514\n",
      "batch 023 / 024 | loss: 0.94062\n",
      "batch 024 / 024 | loss: 0.93771\n",
      "training time: 871.9212465286255 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  1.0655803590079922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 88 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 89 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 90 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 1.27213\n",
      "batch 002 / 024 | loss: 1.47971\n",
      "batch 003 / 024 | loss: 1.29763\n",
      "batch 004 / 024 | loss: 1.20428\n",
      "batch 005 / 024 | loss: 1.18169\n",
      "batch 006 / 024 | loss: 1.16400\n",
      "batch 007 / 024 | loss: 1.15349\n",
      "batch 008 / 024 | loss: 1.12945\n",
      "batch 009 / 024 | loss: 1.12383\n",
      "batch 010 / 024 | loss: 1.11056\n",
      "batch 011 / 024 | loss: 1.08279\n",
      "batch 012 / 024 | loss: 1.08540\n",
      "batch 013 / 024 | loss: 1.07547\n",
      "batch 014 / 024 | loss: 1.10774\n",
      "batch 015 / 024 | loss: 1.10472\n",
      "batch 016 / 024 | loss: 1.10363\n",
      "batch 017 / 024 | loss: 1.10805\n",
      "batch 018 / 024 | loss: 1.11803\n",
      "batch 019 / 024 | loss: 1.11796\n",
      "batch 020 / 024 | loss: 1.10732\n",
      "batch 021 / 024 | loss: 1.09844\n",
      "batch 022 / 024 | loss: 1.09475\n",
      "batch 023 / 024 | loss: 1.09340\n",
      "batch 024 / 024 | loss: 1.08132\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 145 sec | loss: 1.21689 | err: 0.47467\n",
      "batch 001 / 024 | loss: 0.88881\n",
      "batch 002 / 024 | loss: 0.94606\n",
      "batch 003 / 024 | loss: 0.96080\n",
      "batch 004 / 024 | loss: 0.93342\n",
      "batch 005 / 024 | loss: 0.94431\n",
      "batch 006 / 024 | loss: 0.95936\n",
      "batch 007 / 024 | loss: 0.95686\n",
      "batch 008 / 024 | loss: 0.94410\n",
      "batch 009 / 024 | loss: 0.95320\n",
      "batch 010 / 024 | loss: 0.98882\n",
      "batch 011 / 024 | loss: 0.99559\n",
      "batch 012 / 024 | loss: 0.99041\n",
      "batch 013 / 024 | loss: 0.98283\n",
      "batch 014 / 024 | loss: 0.97697\n",
      "batch 015 / 024 | loss: 1.03236\n",
      "batch 016 / 024 | loss: 1.02335\n",
      "batch 017 / 024 | loss: 1.02234\n",
      "batch 018 / 024 | loss: 1.01496\n",
      "batch 019 / 024 | loss: 1.01443\n",
      "batch 020 / 024 | loss: 1.00883\n",
      "batch 021 / 024 | loss: 1.00634\n",
      "batch 022 / 024 | loss: 1.00379\n",
      "batch 023 / 024 | loss: 1.00097\n",
      "batch 024 / 024 | loss: 0.99556\n",
      "----- epoch 002 / 010 | time: 100 sec | loss: 1.02856 | err: 0.48267\n",
      "batch 001 / 024 | loss: 0.80554\n",
      "batch 002 / 024 | loss: 0.80568\n",
      "batch 003 / 024 | loss: 0.82221\n",
      "batch 004 / 024 | loss: 0.85559\n",
      "batch 005 / 024 | loss: 0.89594\n",
      "batch 006 / 024 | loss: 0.90263\n",
      "batch 007 / 024 | loss: 0.91896\n",
      "batch 008 / 024 | loss: 0.91181\n",
      "batch 009 / 024 | loss: 0.91447\n",
      "batch 010 / 024 | loss: 0.92251\n",
      "batch 011 / 024 | loss: 0.92391\n",
      "batch 012 / 024 | loss: 0.92271\n",
      "batch 013 / 024 | loss: 0.92811\n",
      "batch 014 / 024 | loss: 0.92525\n",
      "batch 015 / 024 | loss: 0.91587\n",
      "batch 016 / 024 | loss: 0.91887\n",
      "batch 017 / 024 | loss: 0.91653\n",
      "batch 018 / 024 | loss: 0.92060\n",
      "batch 019 / 024 | loss: 0.90945\n",
      "batch 020 / 024 | loss: 0.91001\n",
      "batch 021 / 024 | loss: 0.92710\n",
      "batch 022 / 024 | loss: 0.93617\n",
      "batch 023 / 024 | loss: 0.96251\n",
      "batch 024 / 024 | loss: 0.95674\n",
      "----- epoch 003 / 010 | time: 099 sec | loss: 0.94405 | err: 0.48000\n",
      "batch 001 / 024 | loss: 0.67125\n",
      "batch 002 / 024 | loss: 0.82949\n",
      "batch 003 / 024 | loss: 0.84669\n",
      "batch 004 / 024 | loss: 0.89765\n",
      "batch 005 / 024 | loss: 0.89341\n",
      "batch 006 / 024 | loss: 0.87638\n",
      "batch 007 / 024 | loss: 0.87859\n",
      "batch 008 / 024 | loss: 0.88270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 009 / 024 | loss: 0.85466\n",
      "batch 010 / 024 | loss: 0.87049\n",
      "batch 011 / 024 | loss: 0.87962\n",
      "batch 012 / 024 | loss: 0.89086\n",
      "batch 013 / 024 | loss: 0.87874\n",
      "batch 014 / 024 | loss: 0.88889\n",
      "batch 015 / 024 | loss: 0.89418\n",
      "batch 016 / 024 | loss: 0.89943\n",
      "batch 017 / 024 | loss: 0.94123\n",
      "batch 018 / 024 | loss: 0.94277\n",
      "batch 019 / 024 | loss: 0.96575\n",
      "batch 020 / 024 | loss: 0.96844\n",
      "batch 021 / 024 | loss: 0.97496\n",
      "batch 022 / 024 | loss: 0.96607\n",
      "batch 023 / 024 | loss: 0.96133\n",
      "batch 024 / 024 | loss: 0.96142\n",
      "----- epoch 004 / 010 | time: 097 sec | loss: 0.89909 | err: 0.47733\n",
      "batch 001 / 024 | loss: 1.80261\n",
      "batch 002 / 024 | loss: 1.33273\n",
      "batch 003 / 024 | loss: 1.24258\n",
      "batch 004 / 024 | loss: 1.09818\n",
      "batch 005 / 024 | loss: 1.06953\n",
      "batch 006 / 024 | loss: 1.02528\n",
      "batch 007 / 024 | loss: 1.01121\n",
      "batch 008 / 024 | loss: 0.99616\n",
      "batch 009 / 024 | loss: 0.99816\n",
      "batch 010 / 024 | loss: 0.98450\n",
      "batch 011 / 024 | loss: 0.97562\n",
      "batch 012 / 024 | loss: 0.97463\n",
      "batch 013 / 024 | loss: 0.95964\n",
      "batch 014 / 024 | loss: 0.95094\n",
      "batch 015 / 024 | loss: 0.97953\n",
      "batch 016 / 024 | loss: 0.97937\n",
      "batch 017 / 024 | loss: 0.96826\n",
      "batch 018 / 024 | loss: 0.96331\n",
      "batch 019 / 024 | loss: 0.95992\n",
      "batch 020 / 024 | loss: 0.96094\n",
      "batch 021 / 024 | loss: 0.96261\n",
      "batch 022 / 024 | loss: 0.96260\n",
      "batch 023 / 024 | loss: 0.95709\n",
      "batch 024 / 024 | loss: 0.95070\n",
      "----- epoch 005 / 010 | time: 098 sec | loss: 0.97005 | err: 0.47600\n",
      "batch 001 / 024 | loss: 0.89976\n",
      "batch 002 / 024 | loss: 0.82966\n",
      "batch 003 / 024 | loss: 0.86801\n",
      "batch 004 / 024 | loss: 0.87130\n",
      "batch 005 / 024 | loss: 0.88360\n",
      "batch 006 / 024 | loss: 0.90945\n",
      "batch 007 / 024 | loss: 0.91034\n",
      "batch 008 / 024 | loss: 0.92427\n",
      "batch 009 / 024 | loss: 0.91884\n",
      "batch 010 / 024 | loss: 0.91029\n",
      "batch 011 / 024 | loss: 0.99755\n",
      "batch 012 / 024 | loss: 0.99757\n",
      "batch 013 / 024 | loss: 0.99895\n",
      "batch 014 / 024 | loss: 0.99316\n",
      "batch 015 / 024 | loss: 0.98179\n",
      "batch 016 / 024 | loss: 0.97760\n",
      "batch 017 / 024 | loss: 0.97238\n",
      "batch 018 / 024 | loss: 0.97963\n",
      "batch 019 / 024 | loss: 0.97639\n",
      "batch 020 / 024 | loss: 0.95463\n",
      "batch 021 / 024 | loss: 0.95710\n",
      "batch 022 / 024 | loss: 0.95759\n",
      "batch 023 / 024 | loss: 0.95303\n",
      "batch 024 / 024 | loss: 0.95026\n",
      "training time: 638.195070028305 seconds\n",
      "---------- training strategically----------\n",
      "lambda:  1.2589254117941673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 91 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 92 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 93 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:213: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 024 | loss: 1.36105\n",
      "batch 002 / 024 | loss: 1.59098\n",
      "batch 003 / 024 | loss: 1.37703\n",
      "batch 004 / 024 | loss: 1.26490\n",
      "batch 005 / 024 | loss: 1.23142\n",
      "batch 006 / 024 | loss: 1.20806\n",
      "batch 007 / 024 | loss: 1.19293\n",
      "batch 008 / 024 | loss: 1.16538\n",
      "batch 009 / 024 | loss: 1.15635\n",
      "batch 010 / 024 | loss: 1.14034\n",
      "batch 011 / 024 | loss: 1.11015\n",
      "batch 012 / 024 | loss: 1.11114\n",
      "batch 013 / 024 | loss: 1.09953\n",
      "batch 014 / 024 | loss: 1.13614\n",
      "batch 015 / 024 | loss: 1.13183\n",
      "batch 016 / 024 | loss: 1.12968\n",
      "batch 017 / 024 | loss: 1.13332\n",
      "batch 018 / 024 | loss: 1.14301\n",
      "batch 019 / 024 | loss: 1.14244\n",
      "batch 020 / 024 | loss: 1.13100\n",
      "batch 021 / 024 | loss: 1.12165\n",
      "batch 022 / 024 | loss: 1.11760\n",
      "batch 023 / 024 | loss: 1.11533\n",
      "batch 024 / 024 | loss: 1.10171\n",
      "model saved!\n",
      "----- epoch 001 / 010 | time: 118 sec | loss: 1.26773 | err: 0.47600\n",
      "batch 001 / 024 | loss: 0.89890\n",
      "batch 002 / 024 | loss: 0.94570\n",
      "batch 003 / 024 | loss: 0.96370\n",
      "batch 004 / 024 | loss: 0.93457\n",
      "batch 005 / 024 | loss: 0.94589\n",
      "batch 006 / 024 | loss: 0.96085\n",
      "batch 007 / 024 | loss: 0.95746\n",
      "batch 008 / 024 | loss: 0.94404\n",
      "batch 009 / 024 | loss: 0.95247\n",
      "batch 010 / 024 | loss: 1.00218\n",
      "batch 011 / 024 | loss: 1.00691\n",
      "batch 012 / 024 | loss: 1.00034\n",
      "batch 013 / 024 | loss: 0.99153\n",
      "batch 014 / 024 | loss: 0.98490\n",
      "batch 015 / 024 | loss: 1.05005\n",
      "batch 016 / 024 | loss: 1.04027\n",
      "batch 017 / 024 | loss: 1.03970\n",
      "batch 018 / 024 | loss: 1.03201\n",
      "batch 019 / 024 | loss: 1.03044\n",
      "batch 020 / 024 | loss: 1.02489\n",
      "batch 021 / 024 | loss: 1.02249\n",
      "batch 022 / 024 | loss: 1.02003\n",
      "batch 023 / 024 | loss: 1.01765\n",
      "batch 024 / 024 | loss: 1.01197\n",
      "----- epoch 002 / 010 | time: 105 sec | loss: 1.20528 | err: 0.48267\n",
      "batch 001 / 024 | loss: 0.81161\n",
      "batch 002 / 024 | loss: 0.82188\n",
      "batch 003 / 024 | loss: 0.83302\n",
      "batch 004 / 024 | loss: 0.87111\n",
      "batch 005 / 024 | loss: 0.91404\n",
      "batch 006 / 024 | loss: 0.92477\n",
      "batch 007 / 024 | loss: 0.94297\n",
      "batch 008 / 024 | loss: 0.93690\n",
      "batch 009 / 024 | loss: 0.94115\n",
      "batch 010 / 024 | loss: 0.95005\n",
      "batch 011 / 024 | loss: 0.94998\n",
      "batch 012 / 024 | loss: 0.94856\n",
      "batch 013 / 024 | loss: 0.95472\n",
      "batch 014 / 024 | loss: 0.95205\n",
      "batch 015 / 024 | loss: 0.94126\n",
      "batch 016 / 024 | loss: 0.94453\n",
      "batch 017 / 024 | loss: 0.94205\n",
      "batch 018 / 024 | loss: 0.94557\n",
      "batch 019 / 024 | loss: 0.93390\n",
      "batch 020 / 024 | loss: 0.93342\n",
      "batch 021 / 024 | loss: 0.95168\n",
      "batch 022 / 024 | loss: 0.95672\n",
      "batch 023 / 024 | loss: 0.98720\n",
      "batch 024 / 024 | loss: 0.98012\n",
      "----- epoch 003 / 010 | time: 099 sec | loss: 0.92299 | err: 0.48000\n",
      "batch 001 / 024 | loss: 0.67957\n",
      "batch 002 / 024 | loss: 0.82826\n",
      "batch 003 / 024 | loss: 0.83796\n",
      "batch 004 / 024 | loss: 0.88974\n",
      "batch 005 / 024 | loss: 0.87808\n",
      "batch 006 / 024 | loss: 0.85558\n",
      "batch 007 / 024 | loss: 0.85655\n",
      "batch 008 / 024 | loss: 0.85968\n",
      "batch 009 / 024 | loss: 0.83449\n",
      "batch 010 / 024 | loss: 0.84958\n",
      "batch 011 / 024 | loss: 0.85884\n",
      "batch 012 / 024 | loss: 0.86919\n",
      "batch 013 / 024 | loss: 0.85849\n",
      "batch 014 / 024 | loss: 0.86738\n",
      "batch 015 / 024 | loss: 0.87348\n",
      "batch 016 / 024 | loss: 0.87996\n",
      "batch 017 / 024 | loss: 0.93027\n",
      "batch 018 / 024 | loss: 0.93263\n",
      "batch 019 / 024 | loss: 0.96032\n",
      "batch 020 / 024 | loss: 0.96275\n",
      "batch 021 / 024 | loss: 0.96965\n",
      "batch 022 / 024 | loss: 0.96001\n",
      "batch 023 / 024 | loss: 0.95428\n",
      "batch 024 / 024 | loss: 0.95602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barw1\\anaconda3\\envs\\ranked_sc\\lib\\site-packages\\cvxpy\\problems\\problem.py:1388: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- epoch 004 / 010 | time: 098 sec | loss: 0.92314 | err: 0.47600\n",
      "batch 001 / 024 | loss: 1.94561\n",
      "batch 002 / 024 | loss: 1.40967\n",
      "batch 003 / 024 | loss: 1.30240\n",
      "batch 004 / 024 | loss: 1.15077\n",
      "batch 005 / 024 | loss: 1.12382\n",
      "batch 006 / 024 | loss: 1.07377\n",
      "batch 007 / 024 | loss: 1.05230\n",
      "batch 008 / 024 | loss: 1.02097\n",
      "batch 009 / 024 | loss: 1.01964\n",
      "batch 010 / 024 | loss: 1.00173\n",
      "batch 011 / 024 | loss: 0.98970\n",
      "batch 012 / 024 | loss: 0.99269\n",
      "batch 013 / 024 | loss: 0.97551\n",
      "batch 014 / 024 | loss: 0.96380\n",
      "batch 015 / 024 | loss: 0.99651\n",
      "batch 016 / 024 | loss: 0.99497\n",
      "batch 017 / 024 | loss: 0.98170\n",
      "batch 018 / 024 | loss: 0.97583\n",
      "batch 019 / 024 | loss: 0.97333\n",
      "batch 020 / 024 | loss: 0.97384\n",
      "batch 021 / 024 | loss: 0.97486\n",
      "batch 022 / 024 | loss: 0.97401\n",
      "batch 023 / 024 | loss: 0.96800\n",
      "batch 024 / 024 | loss: 0.96122\n",
      "----- epoch 005 / 010 | time: 097 sec | loss: 0.96068 | err: 0.47600\n",
      "batch 001 / 024 | loss: 0.90168\n",
      "batch 002 / 024 | loss: 0.83545\n",
      "batch 003 / 024 | loss: 0.87283\n",
      "batch 004 / 024 | loss: 0.87786\n",
      "batch 005 / 024 | loss: 0.88671\n",
      "batch 006 / 024 | loss: 0.91267\n",
      "batch 007 / 024 | loss: 0.91279\n",
      "batch 008 / 024 | loss: 0.92632\n",
      "batch 009 / 024 | loss: 0.92130\n",
      "batch 010 / 024 | loss: 0.91236\n",
      "batch 011 / 024 | loss: 1.01594\n",
      "batch 012 / 024 | loss: 1.01567\n",
      "batch 013 / 024 | loss: 1.01567\n",
      "batch 014 / 024 | loss: 1.00839\n",
      "batch 015 / 024 | loss: 0.99583\n",
      "batch 016 / 024 | loss: 0.99066\n",
      "batch 017 / 024 | loss: 0.98476\n",
      "batch 018 / 024 | loss: 0.99155\n",
      "batch 019 / 024 | loss: 0.98765\n",
      "batch 020 / 024 | loss: 0.96526\n",
      "batch 021 / 024 | loss: 0.96507\n",
      "batch 022 / 024 | loss: 0.96456\n",
      "batch 023 / 024 | loss: 0.95917\n",
      "batch 024 / 024 | loss: 0.95675\n",
      "training time: 616.4828016757965 seconds\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "x_dim = XDIM\n",
    "\n",
    "# non-strategic classification\n",
    "print(\"---------- training non-strategically----------\")\n",
    "non_strategic_model = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, strategic=False)\n",
    "\n",
    "fit_res_non_strategic = non_strategic_model.fit(X, Y, Xval, Yval, Xtest, Ytest,\n",
    "                                opt=torch.optim.Adam, opt_kwargs={\"lr\": 5*(1e-2)},\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=True, calc_train_errors=False)\n",
    "\n",
    "lambda_range = torch.logspace(start=-2, end=0.1, steps=30)\n",
    "print(lambda_range)\n",
    "for lamb in lambda_range:\n",
    "\n",
    "    # strategic classification\n",
    "    print(\"---------- training strategically----------\")\n",
    "    print(\"lambda: \", lamb.item())\n",
    "    strategic_model = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, strategic=True, lamb=lamb)\n",
    "\n",
    "    fit_res_strategic = strategic_model.fit(X, Y, Xval, Yval, Xtest, Ytest,\n",
    "                                    opt=torch.optim.Adam, opt_kwargs={\"lr\": 5*(1e-2)},\n",
    "                                    batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=True, calc_train_errors=False,\n",
    "                                    comment=\"burden_\" + str(lamb.item()))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
